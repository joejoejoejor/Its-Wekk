{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Webscraping and Cleaning of Airtraffic Data**\n",
    "\n",
    "First import the necessary Datasets for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import polars as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to download the content of one website**\n",
    "\n",
    "\n",
    "page_to_scrape = requests.get('https://www.dfld.de/Mess/StatAirportTag.php?R=601&D=12.07.2024')\n",
    "soup = BeautifulSoup(page_to_scrape.text, 'html.parser')\n",
    "\n",
    "entries = soup.findAll(\"tr\", attrs={\"class\": \"5af0f0 trennerlinie\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first solution\n",
    "\n",
    "\n",
    "# Step 1: Fetch the HTML content of the webpage\n",
    "url = \"https://www.dfld.de/Mess/StatAirportTag.php?R=601&D=12.07.2024\"  # Replace with the actual URL of the webpage\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "# Step 2: Parse the HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 3: Locate the table\n",
    "table = soup.find(\"table\", class_=\"table_lines\")  # Update with the actual class or ID of the table\n",
    "\n",
    "# Step 4: Extract table headers\n",
    "headers = [header.text.strip() for header in table.find_all(\"th\")]\n",
    "\n",
    "# Step 5: Extract table rows\n",
    "rows = []\n",
    "for row in table.find_all(\"tr\"):\n",
    "    cells = row.find_all(\"td\")\n",
    "    if cells:  # Skip rows without <td>\n",
    "        rows.append([cell.text.strip() for cell in cells])\n",
    "\n",
    "# Step 6: Create a pandas DataFrame\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# Step 7: Export to CSV\n",
    "df.to_csv(r\"C:\\Users\\maxd2\\OneDrive - Universitaet St.Gallen\\Desktop\\DSF\\output.csv\", index=False)\n",
    "print(\"Table successfully scraped and saved to output.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now change the different Website domains to get the code for all the days since the 01-01-2022**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for date: 01.01.2022\n",
      "Scraping data for date: 02.01.2022\n",
      "Scraping data for date: 03.01.2022\n",
      "Scraping data for date: 04.01.2022\n",
      "Scraping data for date: 05.01.2022\n",
      "Scraping data for date: 06.01.2022\n",
      "No table found for date: 06.01.2022\n",
      "Scraping data for date: 07.01.2022\n",
      "Scraping data for date: 08.01.2022\n",
      "Scraping data for date: 09.01.2022\n",
      "Scraping data for date: 10.01.2022\n",
      "Scraping data for date: 11.01.2022\n",
      "Scraping data for date: 12.01.2022\n",
      "Scraping data for date: 13.01.2022\n",
      "Scraping data for date: 14.01.2022\n",
      "Scraping data for date: 15.01.2022\n",
      "Scraping data for date: 16.01.2022\n",
      "Scraping data for date: 17.01.2022\n",
      "Scraping data for date: 18.01.2022\n",
      "Scraping data for date: 19.01.2022\n",
      "Scraping data for date: 20.01.2022\n",
      "Scraping data for date: 21.01.2022\n",
      "Scraping data for date: 22.01.2022\n",
      "Scraping data for date: 23.01.2022\n",
      "Scraping data for date: 24.01.2022\n",
      "Scraping data for date: 25.01.2022\n",
      "Scraping data for date: 26.01.2022\n",
      "Scraping data for date: 27.01.2022\n",
      "Scraping data for date: 28.01.2022\n",
      "Scraping data for date: 29.01.2022\n",
      "Scraping data for date: 30.01.2022\n",
      "No table found for date: 30.01.2022\n",
      "Scraping data for date: 31.01.2022\n",
      "Scraping data for date: 01.02.2022\n",
      "Scraping data for date: 02.02.2022\n",
      "Scraping data for date: 03.02.2022\n",
      "Scraping data for date: 04.02.2022\n",
      "Scraping data for date: 05.02.2022\n",
      "Scraping data for date: 06.02.2022\n",
      "Scraping data for date: 07.02.2022\n",
      "Scraping data for date: 08.02.2022\n",
      "Scraping data for date: 09.02.2022\n",
      "Scraping data for date: 10.02.2022\n",
      "Scraping data for date: 11.02.2022\n",
      "Scraping data for date: 12.02.2022\n",
      "Scraping data for date: 13.02.2022\n",
      "Scraping data for date: 14.02.2022\n",
      "Scraping data for date: 15.02.2022\n",
      "Scraping data for date: 16.02.2022\n",
      "Scraping data for date: 17.02.2022\n",
      "Scraping data for date: 18.02.2022\n",
      "Scraping data for date: 19.02.2022\n",
      "Scraping data for date: 20.02.2022\n",
      "Scraping data for date: 21.02.2022\n",
      "Scraping data for date: 22.02.2022\n",
      "Scraping data for date: 23.02.2022\n",
      "Scraping data for date: 24.02.2022\n",
      "Scraping data for date: 25.02.2022\n",
      "No table found for date: 25.02.2022\n",
      "Scraping data for date: 26.02.2022\n",
      "No table found for date: 26.02.2022\n",
      "Scraping data for date: 27.02.2022\n",
      "No table found for date: 27.02.2022\n",
      "Scraping data for date: 28.02.2022\n",
      "Scraping data for date: 01.03.2022\n",
      "Scraping data for date: 02.03.2022\n",
      "Scraping data for date: 03.03.2022\n",
      "Scraping data for date: 04.03.2022\n",
      "Scraping data for date: 05.03.2022\n",
      "Scraping data for date: 06.03.2022\n",
      "Scraping data for date: 07.03.2022\n",
      "Scraping data for date: 08.03.2022\n",
      "Scraping data for date: 09.03.2022\n",
      "Scraping data for date: 10.03.2022\n",
      "Scraping data for date: 11.03.2022\n",
      "Scraping data for date: 12.03.2022\n",
      "Scraping data for date: 13.03.2022\n",
      "Scraping data for date: 14.03.2022\n",
      "Scraping data for date: 15.03.2022\n",
      "Scraping data for date: 16.03.2022\n",
      "Scraping data for date: 17.03.2022\n",
      "Scraping data for date: 18.03.2022\n",
      "Scraping data for date: 19.03.2022\n",
      "Scraping data for date: 20.03.2022\n",
      "Scraping data for date: 21.03.2022\n",
      "Scraping data for date: 22.03.2022\n",
      "Scraping data for date: 23.03.2022\n",
      "Scraping data for date: 24.03.2022\n",
      "Scraping data for date: 25.03.2022\n",
      "Scraping data for date: 26.03.2022\n",
      "Scraping data for date: 27.03.2022\n",
      "Scraping data for date: 28.03.2022\n",
      "Scraping data for date: 29.03.2022\n",
      "Scraping data for date: 30.03.2022\n",
      "Scraping data for date: 31.03.2022\n",
      "Scraping data for date: 01.04.2022\n",
      "Scraping data for date: 02.04.2022\n",
      "Scraping data for date: 03.04.2022\n",
      "Scraping data for date: 04.04.2022\n",
      "Scraping data for date: 05.04.2022\n",
      "Scraping data for date: 06.04.2022\n",
      "Scraping data for date: 07.04.2022\n",
      "Scraping data for date: 08.04.2022\n",
      "Scraping data for date: 09.04.2022\n",
      "Scraping data for date: 10.04.2022\n",
      "Scraping data for date: 11.04.2022\n",
      "Scraping data for date: 12.04.2022\n",
      "Scraping data for date: 13.04.2022\n",
      "Scraping data for date: 14.04.2022\n",
      "Scraping data for date: 15.04.2022\n",
      "No table found for date: 15.04.2022\n",
      "Scraping data for date: 16.04.2022\n",
      "Scraping data for date: 17.04.2022\n",
      "Scraping data for date: 18.04.2022\n",
      "Scraping data for date: 19.04.2022\n",
      "Scraping data for date: 20.04.2022\n",
      "Scraping data for date: 21.04.2022\n",
      "Scraping data for date: 22.04.2022\n",
      "Scraping data for date: 23.04.2022\n",
      "Scraping data for date: 24.04.2022\n",
      "Scraping data for date: 25.04.2022\n",
      "Scraping data for date: 26.04.2022\n",
      "Scraping data for date: 27.04.2022\n",
      "Scraping data for date: 28.04.2022\n",
      "Scraping data for date: 29.04.2022\n",
      "Scraping data for date: 30.04.2022\n",
      "Scraping data for date: 01.05.2022\n",
      "Scraping data for date: 02.05.2022\n",
      "Scraping data for date: 03.05.2022\n",
      "Scraping data for date: 04.05.2022\n",
      "Scraping data for date: 05.05.2022\n",
      "Scraping data for date: 06.05.2022\n",
      "Scraping data for date: 07.05.2022\n",
      "Scraping data for date: 08.05.2022\n",
      "Scraping data for date: 09.05.2022\n",
      "Scraping data for date: 10.05.2022\n",
      "Scraping data for date: 11.05.2022\n",
      "Scraping data for date: 12.05.2022\n",
      "Scraping data for date: 13.05.2022\n",
      "Scraping data for date: 14.05.2022\n",
      "Scraping data for date: 15.05.2022\n",
      "Scraping data for date: 16.05.2022\n",
      "Scraping data for date: 17.05.2022\n",
      "Scraping data for date: 18.05.2022\n",
      "Scraping data for date: 19.05.2022\n",
      "Scraping data for date: 20.05.2022\n",
      "Scraping data for date: 21.05.2022\n",
      "Scraping data for date: 22.05.2022\n",
      "Scraping data for date: 23.05.2022\n",
      "Scraping data for date: 24.05.2022\n",
      "Scraping data for date: 25.05.2022\n",
      "Scraping data for date: 26.05.2022\n",
      "Scraping data for date: 27.05.2022\n",
      "Scraping data for date: 28.05.2022\n",
      "Scraping data for date: 29.05.2022\n",
      "Scraping data for date: 30.05.2022\n",
      "Scraping data for date: 31.05.2022\n",
      "Scraping data for date: 01.06.2022\n",
      "Scraping data for date: 02.06.2022\n",
      "Scraping data for date: 03.06.2022\n",
      "Scraping data for date: 04.06.2022\n",
      "No table found for date: 04.06.2022\n",
      "Scraping data for date: 05.06.2022\n",
      "Scraping data for date: 06.06.2022\n",
      "Scraping data for date: 07.06.2022\n",
      "Scraping data for date: 08.06.2022\n",
      "Scraping data for date: 09.06.2022\n",
      "Scraping data for date: 10.06.2022\n",
      "Scraping data for date: 11.06.2022\n",
      "Scraping data for date: 12.06.2022\n",
      "Scraping data for date: 13.06.2022\n",
      "Scraping data for date: 14.06.2022\n",
      "Scraping data for date: 15.06.2022\n",
      "Scraping data for date: 16.06.2022\n",
      "Scraping data for date: 17.06.2022\n",
      "Scraping data for date: 18.06.2022\n",
      "Scraping data for date: 19.06.2022\n",
      "Scraping data for date: 20.06.2022\n",
      "Scraping data for date: 21.06.2022\n",
      "Scraping data for date: 22.06.2022\n",
      "Scraping data for date: 23.06.2022\n",
      "Scraping data for date: 24.06.2022\n",
      "Scraping data for date: 25.06.2022\n",
      "Scraping data for date: 26.06.2022\n",
      "Scraping data for date: 27.06.2022\n",
      "Scraping data for date: 28.06.2022\n",
      "Scraping data for date: 29.06.2022\n",
      "Scraping data for date: 30.06.2022\n",
      "Scraping data for date: 01.07.2022\n",
      "Scraping data for date: 02.07.2022\n",
      "Scraping data for date: 03.07.2022\n",
      "Scraping data for date: 04.07.2022\n",
      "Scraping data for date: 05.07.2022\n",
      "Scraping data for date: 06.07.2022\n",
      "Scraping data for date: 07.07.2022\n",
      "Scraping data for date: 08.07.2022\n",
      "Scraping data for date: 09.07.2022\n",
      "Scraping data for date: 10.07.2022\n",
      "Scraping data for date: 11.07.2022\n",
      "Scraping data for date: 12.07.2022\n",
      "Scraping data for date: 13.07.2022\n",
      "Scraping data for date: 14.07.2022\n",
      "Scraping data for date: 15.07.2022\n",
      "Scraping data for date: 16.07.2022\n",
      "Scraping data for date: 17.07.2022\n",
      "Scraping data for date: 18.07.2022\n",
      "Scraping data for date: 19.07.2022\n",
      "Scraping data for date: 20.07.2022\n",
      "Scraping data for date: 21.07.2022\n",
      "Scraping data for date: 22.07.2022\n",
      "Scraping data for date: 23.07.2022\n",
      "Scraping data for date: 24.07.2022\n",
      "Scraping data for date: 25.07.2022\n",
      "Scraping data for date: 26.07.2022\n",
      "Scraping data for date: 27.07.2022\n",
      "Scraping data for date: 28.07.2022\n",
      "Scraping data for date: 29.07.2022\n",
      "Scraping data for date: 30.07.2022\n",
      "Scraping data for date: 31.07.2022\n",
      "Scraping data for date: 01.08.2022\n",
      "Scraping data for date: 02.08.2022\n",
      "Scraping data for date: 03.08.2022\n",
      "No table found for date: 03.08.2022\n",
      "Scraping data for date: 04.08.2022\n",
      "No table found for date: 04.08.2022\n",
      "Scraping data for date: 05.08.2022\n",
      "Scraping data for date: 06.08.2022\n",
      "Scraping data for date: 07.08.2022\n",
      "Scraping data for date: 08.08.2022\n",
      "Scraping data for date: 09.08.2022\n",
      "Scraping data for date: 10.08.2022\n",
      "Scraping data for date: 11.08.2022\n",
      "Scraping data for date: 12.08.2022\n",
      "Scraping data for date: 13.08.2022\n",
      "Scraping data for date: 14.08.2022\n",
      "Scraping data for date: 15.08.2022\n",
      "Scraping data for date: 16.08.2022\n",
      "Scraping data for date: 17.08.2022\n",
      "Scraping data for date: 18.08.2022\n",
      "Scraping data for date: 19.08.2022\n",
      "Scraping data for date: 20.08.2022\n",
      "Scraping data for date: 21.08.2022\n",
      "Scraping data for date: 22.08.2022\n",
      "Scraping data for date: 23.08.2022\n",
      "Scraping data for date: 24.08.2022\n",
      "Scraping data for date: 25.08.2022\n",
      "Scraping data for date: 26.08.2022\n",
      "Scraping data for date: 27.08.2022\n",
      "Scraping data for date: 28.08.2022\n",
      "Scraping data for date: 29.08.2022\n",
      "Scraping data for date: 30.08.2022\n",
      "Scraping data for date: 31.08.2022\n",
      "Scraping data for date: 01.09.2022\n",
      "Scraping data for date: 02.09.2022\n",
      "Scraping data for date: 03.09.2022\n",
      "No table found for date: 03.09.2022\n",
      "Scraping data for date: 04.09.2022\n",
      "No table found for date: 04.09.2022\n",
      "Scraping data for date: 05.09.2022\n",
      "No table found for date: 05.09.2022\n",
      "Scraping data for date: 06.09.2022\n",
      "No table found for date: 06.09.2022\n",
      "Scraping data for date: 07.09.2022\n",
      "Scraping data for date: 08.09.2022\n",
      "Scraping data for date: 09.09.2022\n",
      "Scraping data for date: 10.09.2022\n",
      "Scraping data for date: 11.09.2022\n",
      "Scraping data for date: 12.09.2022\n",
      "Scraping data for date: 13.09.2022\n",
      "Scraping data for date: 14.09.2022\n",
      "Scraping data for date: 15.09.2022\n",
      "Scraping data for date: 16.09.2022\n",
      "Scraping data for date: 17.09.2022\n",
      "Scraping data for date: 18.09.2022\n",
      "Scraping data for date: 19.09.2022\n",
      "Scraping data for date: 20.09.2022\n",
      "Scraping data for date: 21.09.2022\n",
      "Scraping data for date: 22.09.2022\n",
      "Scraping data for date: 23.09.2022\n",
      "Scraping data for date: 24.09.2022\n",
      "Scraping data for date: 25.09.2022\n",
      "Scraping data for date: 26.09.2022\n",
      "Scraping data for date: 27.09.2022\n",
      "Scraping data for date: 28.09.2022\n",
      "Scraping data for date: 29.09.2022\n",
      "Scraping data for date: 30.09.2022\n",
      "Scraping data for date: 01.10.2022\n",
      "Scraping data for date: 02.10.2022\n",
      "Scraping data for date: 03.10.2022\n",
      "Scraping data for date: 04.10.2022\n",
      "Scraping data for date: 05.10.2022\n",
      "Scraping data for date: 06.10.2022\n",
      "No table found for date: 06.10.2022\n",
      "Scraping data for date: 07.10.2022\n",
      "No table found for date: 07.10.2022\n",
      "Scraping data for date: 08.10.2022\n",
      "No table found for date: 08.10.2022\n",
      "Scraping data for date: 09.10.2022\n",
      "No table found for date: 09.10.2022\n",
      "Scraping data for date: 10.10.2022\n",
      "No table found for date: 10.10.2022\n",
      "Scraping data for date: 11.10.2022\n",
      "Scraping data for date: 12.10.2022\n",
      "Scraping data for date: 13.10.2022\n",
      "Scraping data for date: 14.10.2022\n",
      "Scraping data for date: 15.10.2022\n",
      "Scraping data for date: 16.10.2022\n",
      "Scraping data for date: 17.10.2022\n",
      "Scraping data for date: 18.10.2022\n",
      "Scraping data for date: 19.10.2022\n",
      "Scraping data for date: 20.10.2022\n",
      "Scraping data for date: 21.10.2022\n",
      "Scraping data for date: 22.10.2022\n",
      "Scraping data for date: 23.10.2022\n",
      "Scraping data for date: 24.10.2022\n",
      "Scraping data for date: 25.10.2022\n",
      "Scraping data for date: 26.10.2022\n",
      "Scraping data for date: 27.10.2022\n",
      "Scraping data for date: 28.10.2022\n",
      "Scraping data for date: 29.10.2022\n",
      "Scraping data for date: 30.10.2022\n",
      "Scraping data for date: 31.10.2022\n",
      "No table found for date: 31.10.2022\n",
      "Scraping data for date: 01.11.2022\n",
      "Scraping data for date: 02.11.2022\n",
      "Scraping data for date: 03.11.2022\n",
      "Scraping data for date: 04.11.2022\n",
      "Scraping data for date: 05.11.2022\n",
      "Scraping data for date: 06.11.2022\n",
      "Scraping data for date: 07.11.2022\n",
      "Scraping data for date: 08.11.2022\n",
      "Scraping data for date: 09.11.2022\n",
      "Scraping data for date: 10.11.2022\n",
      "Scraping data for date: 11.11.2022\n",
      "Scraping data for date: 12.11.2022\n",
      "Scraping data for date: 13.11.2022\n",
      "Scraping data for date: 14.11.2022\n",
      "Scraping data for date: 15.11.2022\n",
      "Scraping data for date: 16.11.2022\n",
      "Scraping data for date: 17.11.2022\n",
      "Scraping data for date: 18.11.2022\n",
      "Scraping data for date: 19.11.2022\n",
      "Scraping data for date: 20.11.2022\n",
      "Scraping data for date: 21.11.2022\n",
      "Scraping data for date: 22.11.2022\n",
      "Scraping data for date: 23.11.2022\n",
      "Scraping data for date: 24.11.2022\n",
      "Scraping data for date: 25.11.2022\n",
      "Scraping data for date: 26.11.2022\n",
      "Scraping data for date: 27.11.2022\n",
      "Scraping data for date: 28.11.2022\n",
      "Scraping data for date: 29.11.2022\n",
      "Scraping data for date: 30.11.2022\n",
      "Scraping data for date: 01.12.2022\n",
      "Scraping data for date: 02.12.2022\n",
      "Scraping data for date: 03.12.2022\n",
      "Scraping data for date: 04.12.2022\n",
      "Scraping data for date: 05.12.2022\n",
      "Scraping data for date: 06.12.2022\n",
      "Scraping data for date: 07.12.2022\n",
      "Scraping data for date: 08.12.2022\n",
      "Scraping data for date: 09.12.2022\n",
      "Scraping data for date: 10.12.2022\n",
      "Scraping data for date: 11.12.2022\n",
      "Scraping data for date: 12.12.2022\n",
      "Scraping data for date: 13.12.2022\n",
      "Scraping data for date: 14.12.2022\n",
      "Scraping data for date: 15.12.2022\n",
      "Scraping data for date: 16.12.2022\n",
      "Scraping data for date: 17.12.2022\n",
      "Scraping data for date: 18.12.2022\n",
      "Scraping data for date: 19.12.2022\n",
      "Scraping data for date: 20.12.2022\n",
      "No table found for date: 20.12.2022\n",
      "Scraping data for date: 21.12.2022\n",
      "Scraping data for date: 22.12.2022\n",
      "Scraping data for date: 23.12.2022\n",
      "Scraping data for date: 24.12.2022\n",
      "Scraping data for date: 25.12.2022\n",
      "Scraping data for date: 26.12.2022\n",
      "Scraping data for date: 27.12.2022\n",
      "Scraping data for date: 28.12.2022\n",
      "Scraping data for date: 29.12.2022\n",
      "Scraping data for date: 30.12.2022\n",
      "Scraping data for date: 31.12.2022\n",
      "Scraping data for date: 01.01.2023\n",
      "Scraping data for date: 02.01.2023\n",
      "Scraping data for date: 03.01.2023\n",
      "Scraping data for date: 04.01.2023\n",
      "Scraping data for date: 05.01.2023\n",
      "Scraping data for date: 06.01.2023\n",
      "Scraping data for date: 07.01.2023\n",
      "Scraping data for date: 08.01.2023\n",
      "Scraping data for date: 09.01.2023\n",
      "Scraping data for date: 10.01.2023\n",
      "Scraping data for date: 11.01.2023\n",
      "No table found for date: 11.01.2023\n",
      "Scraping data for date: 12.01.2023\n",
      "No table found for date: 12.01.2023\n",
      "Scraping data for date: 13.01.2023\n",
      "No table found for date: 13.01.2023\n",
      "Scraping data for date: 14.01.2023\n",
      "No table found for date: 14.01.2023\n",
      "Scraping data for date: 15.01.2023\n",
      "No table found for date: 15.01.2023\n",
      "Scraping data for date: 16.01.2023\n",
      "No table found for date: 16.01.2023\n",
      "Scraping data for date: 17.01.2023\n",
      "Scraping data for date: 18.01.2023\n",
      "Scraping data for date: 19.01.2023\n",
      "Scraping data for date: 20.01.2023\n",
      "Scraping data for date: 21.01.2023\n",
      "Scraping data for date: 22.01.2023\n",
      "Scraping data for date: 23.01.2023\n",
      "Scraping data for date: 24.01.2023\n",
      "Scraping data for date: 25.01.2023\n",
      "Scraping data for date: 26.01.2023\n",
      "Scraping data for date: 27.01.2023\n",
      "Scraping data for date: 28.01.2023\n",
      "Scraping data for date: 29.01.2023\n",
      "Scraping data for date: 30.01.2023\n",
      "Scraping data for date: 31.01.2023\n",
      "Scraping data for date: 01.02.2023\n",
      "Scraping data for date: 02.02.2023\n",
      "Scraping data for date: 03.02.2023\n",
      "Scraping data for date: 04.02.2023\n",
      "Scraping data for date: 05.02.2023\n",
      "Scraping data for date: 06.02.2023\n",
      "Scraping data for date: 07.02.2023\n",
      "Scraping data for date: 08.02.2023\n",
      "Scraping data for date: 09.02.2023\n",
      "Scraping data for date: 10.02.2023\n",
      "Scraping data for date: 11.02.2023\n",
      "Scraping data for date: 12.02.2023\n",
      "Scraping data for date: 13.02.2023\n",
      "Scraping data for date: 14.02.2023\n",
      "Scraping data for date: 15.02.2023\n",
      "Scraping data for date: 16.02.2023\n",
      "Scraping data for date: 17.02.2023\n",
      "Scraping data for date: 18.02.2023\n",
      "Scraping data for date: 19.02.2023\n",
      "Scraping data for date: 20.02.2023\n",
      "Scraping data for date: 21.02.2023\n",
      "Scraping data for date: 22.02.2023\n",
      "Scraping data for date: 23.02.2023\n",
      "Scraping data for date: 24.02.2023\n",
      "Scraping data for date: 25.02.2023\n",
      "Scraping data for date: 26.02.2023\n",
      "Scraping data for date: 27.02.2023\n",
      "Scraping data for date: 28.02.2023\n",
      "Scraping data for date: 01.03.2023\n",
      "Scraping data for date: 02.03.2023\n",
      "Scraping data for date: 03.03.2023\n",
      "Scraping data for date: 04.03.2023\n",
      "Scraping data for date: 05.03.2023\n",
      "Scraping data for date: 06.03.2023\n",
      "Scraping data for date: 07.03.2023\n",
      "Scraping data for date: 08.03.2023\n",
      "Scraping data for date: 09.03.2023\n",
      "Scraping data for date: 10.03.2023\n",
      "Scraping data for date: 11.03.2023\n",
      "Scraping data for date: 12.03.2023\n",
      "Scraping data for date: 13.03.2023\n",
      "Scraping data for date: 14.03.2023\n",
      "Scraping data for date: 15.03.2023\n",
      "Scraping data for date: 16.03.2023\n",
      "Scraping data for date: 17.03.2023\n",
      "Scraping data for date: 18.03.2023\n",
      "Scraping data for date: 19.03.2023\n",
      "Scraping data for date: 20.03.2023\n",
      "Scraping data for date: 21.03.2023\n",
      "Scraping data for date: 22.03.2023\n",
      "Scraping data for date: 23.03.2023\n",
      "Scraping data for date: 24.03.2023\n",
      "Scraping data for date: 25.03.2023\n",
      "Scraping data for date: 26.03.2023\n",
      "Scraping data for date: 27.03.2023\n",
      "Scraping data for date: 28.03.2023\n",
      "Scraping data for date: 29.03.2023\n",
      "No table found for date: 29.03.2023\n",
      "Scraping data for date: 30.03.2023\n",
      "Scraping data for date: 31.03.2023\n",
      "Scraping data for date: 01.04.2023\n",
      "Scraping data for date: 02.04.2023\n",
      "Scraping data for date: 03.04.2023\n",
      "Scraping data for date: 04.04.2023\n",
      "Scraping data for date: 05.04.2023\n",
      "Scraping data for date: 06.04.2023\n",
      "Scraping data for date: 07.04.2023\n",
      "Scraping data for date: 08.04.2023\n",
      "Scraping data for date: 09.04.2023\n",
      "Scraping data for date: 10.04.2023\n",
      "Scraping data for date: 11.04.2023\n",
      "Scraping data for date: 12.04.2023\n",
      "Scraping data for date: 13.04.2023\n",
      "Scraping data for date: 14.04.2023\n",
      "Scraping data for date: 15.04.2023\n",
      "Scraping data for date: 16.04.2023\n",
      "Scraping data for date: 17.04.2023\n",
      "Scraping data for date: 18.04.2023\n",
      "Scraping data for date: 19.04.2023\n",
      "Scraping data for date: 20.04.2023\n",
      "Scraping data for date: 21.04.2023\n",
      "Scraping data for date: 22.04.2023\n",
      "Scraping data for date: 23.04.2023\n",
      "Scraping data for date: 24.04.2023\n",
      "Scraping data for date: 25.04.2023\n",
      "Scraping data for date: 26.04.2023\n",
      "Scraping data for date: 27.04.2023\n",
      "Scraping data for date: 28.04.2023\n",
      "Scraping data for date: 29.04.2023\n",
      "Scraping data for date: 30.04.2023\n",
      "Scraping data for date: 01.05.2023\n",
      "Scraping data for date: 02.05.2023\n",
      "Scraping data for date: 03.05.2023\n",
      "Scraping data for date: 04.05.2023\n",
      "Scraping data for date: 05.05.2023\n",
      "Scraping data for date: 06.05.2023\n",
      "Scraping data for date: 07.05.2023\n",
      "Scraping data for date: 08.05.2023\n",
      "Scraping data for date: 09.05.2023\n",
      "Scraping data for date: 10.05.2023\n",
      "Scraping data for date: 11.05.2023\n",
      "Scraping data for date: 12.05.2023\n",
      "Scraping data for date: 13.05.2023\n",
      "Scraping data for date: 14.05.2023\n",
      "Scraping data for date: 15.05.2023\n",
      "Scraping data for date: 16.05.2023\n",
      "Scraping data for date: 17.05.2023\n",
      "Scraping data for date: 18.05.2023\n",
      "No table found for date: 18.05.2023\n",
      "Scraping data for date: 19.05.2023\n",
      "Scraping data for date: 20.05.2023\n",
      "Scraping data for date: 21.05.2023\n",
      "Scraping data for date: 22.05.2023\n",
      "Scraping data for date: 23.05.2023\n",
      "Scraping data for date: 24.05.2023\n",
      "Scraping data for date: 25.05.2023\n",
      "Scraping data for date: 26.05.2023\n",
      "Scraping data for date: 27.05.2023\n",
      "Scraping data for date: 28.05.2023\n",
      "Scraping data for date: 29.05.2023\n",
      "Scraping data for date: 30.05.2023\n",
      "Scraping data for date: 31.05.2023\n",
      "Scraping data for date: 01.06.2023\n",
      "Scraping data for date: 02.06.2023\n",
      "Scraping data for date: 03.06.2023\n",
      "Scraping data for date: 04.06.2023\n",
      "Scraping data for date: 05.06.2023\n",
      "Scraping data for date: 06.06.2023\n",
      "Scraping data for date: 07.06.2023\n",
      "Scraping data for date: 08.06.2023\n",
      "Scraping data for date: 09.06.2023\n",
      "Scraping data for date: 10.06.2023\n",
      "Scraping data for date: 11.06.2023\n",
      "Scraping data for date: 12.06.2023\n",
      "Scraping data for date: 13.06.2023\n",
      "Scraping data for date: 14.06.2023\n",
      "Scraping data for date: 15.06.2023\n",
      "Scraping data for date: 16.06.2023\n",
      "Scraping data for date: 17.06.2023\n",
      "Scraping data for date: 18.06.2023\n",
      "Scraping data for date: 19.06.2023\n",
      "Scraping data for date: 20.06.2023\n",
      "Scraping data for date: 21.06.2023\n",
      "Scraping data for date: 22.06.2023\n",
      "Scraping data for date: 23.06.2023\n",
      "Scraping data for date: 24.06.2023\n",
      "Scraping data for date: 25.06.2023\n",
      "Scraping data for date: 26.06.2023\n",
      "Scraping data for date: 27.06.2023\n",
      "Scraping data for date: 28.06.2023\n",
      "Scraping data for date: 29.06.2023\n",
      "Scraping data for date: 30.06.2023\n",
      "Scraping data for date: 01.07.2023\n",
      "Scraping data for date: 02.07.2023\n",
      "Scraping data for date: 03.07.2023\n",
      "Scraping data for date: 04.07.2023\n",
      "Scraping data for date: 05.07.2023\n",
      "Scraping data for date: 06.07.2023\n",
      "Scraping data for date: 07.07.2023\n",
      "Scraping data for date: 08.07.2023\n",
      "Scraping data for date: 09.07.2023\n",
      "Scraping data for date: 10.07.2023\n",
      "Scraping data for date: 11.07.2023\n",
      "Scraping data for date: 12.07.2023\n",
      "Scraping data for date: 13.07.2023\n",
      "Scraping data for date: 14.07.2023\n",
      "Scraping data for date: 15.07.2023\n",
      "Scraping data for date: 16.07.2023\n",
      "Scraping data for date: 17.07.2023\n",
      "Scraping data for date: 18.07.2023\n",
      "Scraping data for date: 19.07.2023\n",
      "Scraping data for date: 20.07.2023\n",
      "Scraping data for date: 21.07.2023\n",
      "Scraping data for date: 22.07.2023\n",
      "Scraping data for date: 23.07.2023\n",
      "Scraping data for date: 24.07.2023\n",
      "Scraping data for date: 25.07.2023\n",
      "Scraping data for date: 26.07.2023\n",
      "Scraping data for date: 27.07.2023\n",
      "Scraping data for date: 28.07.2023\n",
      "Scraping data for date: 29.07.2023\n",
      "Scraping data for date: 30.07.2023\n",
      "Scraping data for date: 31.07.2023\n",
      "Scraping data for date: 01.08.2023\n",
      "Scraping data for date: 02.08.2023\n",
      "Scraping data for date: 03.08.2023\n",
      "Scraping data for date: 04.08.2023\n",
      "Scraping data for date: 05.08.2023\n",
      "Scraping data for date: 06.08.2023\n",
      "Scraping data for date: 07.08.2023\n",
      "Scraping data for date: 08.08.2023\n",
      "Scraping data for date: 09.08.2023\n",
      "Scraping data for date: 10.08.2023\n",
      "Scraping data for date: 11.08.2023\n",
      "Scraping data for date: 12.08.2023\n",
      "Scraping data for date: 13.08.2023\n",
      "Scraping data for date: 14.08.2023\n",
      "Scraping data for date: 15.08.2023\n",
      "Scraping data for date: 16.08.2023\n",
      "Scraping data for date: 17.08.2023\n",
      "Scraping data for date: 18.08.2023\n",
      "Scraping data for date: 19.08.2023\n",
      "Scraping data for date: 20.08.2023\n",
      "Scraping data for date: 21.08.2023\n",
      "Scraping data for date: 22.08.2023\n",
      "Scraping data for date: 23.08.2023\n",
      "Scraping data for date: 24.08.2023\n",
      "Scraping data for date: 25.08.2023\n",
      "Scraping data for date: 26.08.2023\n",
      "Scraping data for date: 27.08.2023\n",
      "Scraping data for date: 28.08.2023\n",
      "Scraping data for date: 29.08.2023\n",
      "Scraping data for date: 30.08.2023\n",
      "Scraping data for date: 31.08.2023\n",
      "Scraping data for date: 01.09.2023\n",
      "Scraping data for date: 02.09.2023\n",
      "Scraping data for date: 03.09.2023\n",
      "Scraping data for date: 04.09.2023\n",
      "Scraping data for date: 05.09.2023\n",
      "Scraping data for date: 06.09.2023\n",
      "Scraping data for date: 07.09.2023\n",
      "Scraping data for date: 08.09.2023\n",
      "Scraping data for date: 09.09.2023\n",
      "Scraping data for date: 10.09.2023\n",
      "Scraping data for date: 11.09.2023\n",
      "Scraping data for date: 12.09.2023\n",
      "Scraping data for date: 13.09.2023\n",
      "Scraping data for date: 14.09.2023\n",
      "Scraping data for date: 15.09.2023\n",
      "Scraping data for date: 16.09.2023\n",
      "Scraping data for date: 17.09.2023\n",
      "Scraping data for date: 18.09.2023\n",
      "No table found for date: 18.09.2023\n",
      "Scraping data for date: 19.09.2023\n",
      "No table found for date: 19.09.2023\n",
      "Scraping data for date: 20.09.2023\n",
      "No table found for date: 20.09.2023\n",
      "Scraping data for date: 21.09.2023\n",
      "Scraping data for date: 22.09.2023\n",
      "Scraping data for date: 23.09.2023\n",
      "Scraping data for date: 24.09.2023\n",
      "Scraping data for date: 25.09.2023\n",
      "Scraping data for date: 26.09.2023\n",
      "Scraping data for date: 27.09.2023\n",
      "Scraping data for date: 28.09.2023\n",
      "Scraping data for date: 29.09.2023\n",
      "Scraping data for date: 30.09.2023\n",
      "Scraping data for date: 01.10.2023\n",
      "Scraping data for date: 02.10.2023\n",
      "Scraping data for date: 03.10.2023\n",
      "Scraping data for date: 04.10.2023\n",
      "Scraping data for date: 05.10.2023\n",
      "Scraping data for date: 06.10.2023\n",
      "Scraping data for date: 07.10.2023\n",
      "Scraping data for date: 08.10.2023\n",
      "Scraping data for date: 09.10.2023\n",
      "Scraping data for date: 10.10.2023\n",
      "Scraping data for date: 11.10.2023\n",
      "Scraping data for date: 12.10.2023\n",
      "Scraping data for date: 13.10.2023\n",
      "Scraping data for date: 14.10.2023\n",
      "No table found for date: 14.10.2023\n",
      "Scraping data for date: 15.10.2023\n",
      "Scraping data for date: 16.10.2023\n",
      "Scraping data for date: 17.10.2023\n",
      "Scraping data for date: 18.10.2023\n",
      "Scraping data for date: 19.10.2023\n",
      "Scraping data for date: 20.10.2023\n",
      "No table found for date: 20.10.2023\n",
      "Scraping data for date: 21.10.2023\n",
      "Scraping data for date: 22.10.2023\n",
      "Scraping data for date: 23.10.2023\n",
      "Scraping data for date: 24.10.2023\n",
      "Scraping data for date: 25.10.2023\n",
      "No table found for date: 25.10.2023\n",
      "Scraping data for date: 26.10.2023\n",
      "No table found for date: 26.10.2023\n",
      "Scraping data for date: 27.10.2023\n",
      "Scraping data for date: 28.10.2023\n",
      "Scraping data for date: 29.10.2023\n",
      "Scraping data for date: 30.10.2023\n",
      "Scraping data for date: 31.10.2023\n",
      "Scraping data for date: 01.11.2023\n",
      "Scraping data for date: 02.11.2023\n",
      "Scraping data for date: 03.11.2023\n",
      "Scraping data for date: 04.11.2023\n",
      "Scraping data for date: 05.11.2023\n",
      "Scraping data for date: 06.11.2023\n",
      "Scraping data for date: 07.11.2023\n",
      "Scraping data for date: 08.11.2023\n",
      "Scraping data for date: 09.11.2023\n",
      "Scraping data for date: 10.11.2023\n",
      "Scraping data for date: 11.11.2023\n",
      "Scraping data for date: 12.11.2023\n",
      "Scraping data for date: 13.11.2023\n",
      "Scraping data for date: 14.11.2023\n",
      "Scraping data for date: 15.11.2023\n",
      "Scraping data for date: 16.11.2023\n",
      "Scraping data for date: 17.11.2023\n",
      "Scraping data for date: 18.11.2023\n",
      "Scraping data for date: 19.11.2023\n",
      "Scraping data for date: 20.11.2023\n",
      "Scraping data for date: 21.11.2023\n",
      "Scraping data for date: 22.11.2023\n",
      "Scraping data for date: 23.11.2023\n",
      "Scraping data for date: 24.11.2023\n",
      "Scraping data for date: 25.11.2023\n",
      "Scraping data for date: 26.11.2023\n",
      "Scraping data for date: 27.11.2023\n",
      "Scraping data for date: 28.11.2023\n",
      "Scraping data for date: 29.11.2023\n",
      "Scraping data for date: 30.11.2023\n",
      "Scraping data for date: 01.12.2023\n",
      "Scraping data for date: 02.12.2023\n",
      "Scraping data for date: 03.12.2023\n",
      "Scraping data for date: 04.12.2023\n",
      "Scraping data for date: 05.12.2023\n",
      "Scraping data for date: 06.12.2023\n",
      "Scraping data for date: 07.12.2023\n",
      "Scraping data for date: 08.12.2023\n",
      "Scraping data for date: 09.12.2023\n",
      "Scraping data for date: 10.12.2023\n",
      "Scraping data for date: 11.12.2023\n",
      "Scraping data for date: 12.12.2023\n",
      "Scraping data for date: 13.12.2023\n",
      "Scraping data for date: 14.12.2023\n",
      "Scraping data for date: 15.12.2023\n",
      "Scraping data for date: 16.12.2023\n",
      "Scraping data for date: 17.12.2023\n",
      "Scraping data for date: 18.12.2023\n",
      "Scraping data for date: 19.12.2023\n",
      "Scraping data for date: 20.12.2023\n",
      "Scraping data for date: 21.12.2023\n",
      "Scraping data for date: 22.12.2023\n",
      "Scraping data for date: 23.12.2023\n",
      "Scraping data for date: 24.12.2023\n",
      "Scraping data for date: 25.12.2023\n",
      "Scraping data for date: 26.12.2023\n",
      "Scraping data for date: 27.12.2023\n",
      "Scraping data for date: 28.12.2023\n",
      "Scraping data for date: 29.12.2023\n",
      "Scraping data for date: 30.12.2023\n",
      "Scraping data for date: 31.12.2023\n",
      "Scraping data for date: 01.01.2024\n",
      "Scraping data for date: 02.01.2024\n",
      "Scraping data for date: 03.01.2024\n",
      "Scraping data for date: 04.01.2024\n",
      "Scraping data for date: 05.01.2024\n",
      "Scraping data for date: 06.01.2024\n",
      "Scraping data for date: 07.01.2024\n",
      "Scraping data for date: 08.01.2024\n",
      "Scraping data for date: 09.01.2024\n",
      "Scraping data for date: 10.01.2024\n",
      "Scraping data for date: 11.01.2024\n",
      "Scraping data for date: 12.01.2024\n",
      "Scraping data for date: 13.01.2024\n",
      "Scraping data for date: 14.01.2024\n",
      "Scraping data for date: 15.01.2024\n",
      "Scraping data for date: 16.01.2024\n",
      "Scraping data for date: 17.01.2024\n",
      "Scraping data for date: 18.01.2024\n",
      "Scraping data for date: 19.01.2024\n",
      "Scraping data for date: 20.01.2024\n",
      "Scraping data for date: 21.01.2024\n",
      "Scraping data for date: 22.01.2024\n",
      "Scraping data for date: 23.01.2024\n",
      "Scraping data for date: 24.01.2024\n",
      "Scraping data for date: 25.01.2024\n",
      "Scraping data for date: 26.01.2024\n",
      "Scraping data for date: 27.01.2024\n",
      "Scraping data for date: 28.01.2024\n",
      "Scraping data for date: 29.01.2024\n",
      "Scraping data for date: 30.01.2024\n",
      "Scraping data for date: 31.01.2024\n",
      "Scraping data for date: 01.02.2024\n",
      "Scraping data for date: 02.02.2024\n",
      "Scraping data for date: 03.02.2024\n",
      "Scraping data for date: 04.02.2024\n",
      "Scraping data for date: 05.02.2024\n",
      "Scraping data for date: 06.02.2024\n",
      "Scraping data for date: 07.02.2024\n",
      "Scraping data for date: 08.02.2024\n",
      "Scraping data for date: 09.02.2024\n",
      "Scraping data for date: 10.02.2024\n",
      "Scraping data for date: 11.02.2024\n",
      "Scraping data for date: 12.02.2024\n",
      "Scraping data for date: 13.02.2024\n",
      "Scraping data for date: 14.02.2024\n",
      "Scraping data for date: 15.02.2024\n",
      "Scraping data for date: 16.02.2024\n",
      "Scraping data for date: 17.02.2024\n",
      "Scraping data for date: 18.02.2024\n",
      "Scraping data for date: 19.02.2024\n",
      "Scraping data for date: 20.02.2024\n",
      "Scraping data for date: 21.02.2024\n",
      "Scraping data for date: 22.02.2024\n",
      "Scraping data for date: 23.02.2024\n",
      "Scraping data for date: 24.02.2024\n",
      "Scraping data for date: 25.02.2024\n",
      "Scraping data for date: 26.02.2024\n",
      "Scraping data for date: 27.02.2024\n",
      "Scraping data for date: 28.02.2024\n",
      "Scraping data for date: 29.02.2024\n",
      "Scraping data for date: 01.03.2024\n",
      "Scraping data for date: 02.03.2024\n",
      "Scraping data for date: 03.03.2024\n",
      "Scraping data for date: 04.03.2024\n",
      "Scraping data for date: 05.03.2024\n",
      "Scraping data for date: 06.03.2024\n",
      "Scraping data for date: 07.03.2024\n",
      "Scraping data for date: 08.03.2024\n",
      "Scraping data for date: 09.03.2024\n",
      "Scraping data for date: 10.03.2024\n",
      "Scraping data for date: 11.03.2024\n",
      "No table found for date: 11.03.2024\n",
      "Scraping data for date: 12.03.2024\n",
      "Scraping data for date: 13.03.2024\n",
      "Scraping data for date: 14.03.2024\n",
      "Scraping data for date: 15.03.2024\n",
      "Scraping data for date: 16.03.2024\n",
      "Scraping data for date: 17.03.2024\n",
      "Scraping data for date: 18.03.2024\n",
      "Scraping data for date: 19.03.2024\n",
      "Scraping data for date: 20.03.2024\n",
      "Scraping data for date: 21.03.2024\n",
      "Scraping data for date: 22.03.2024\n",
      "Scraping data for date: 23.03.2024\n",
      "Scraping data for date: 24.03.2024\n",
      "Scraping data for date: 25.03.2024\n",
      "Scraping data for date: 26.03.2024\n",
      "Scraping data for date: 27.03.2024\n",
      "Scraping data for date: 28.03.2024\n",
      "Scraping data for date: 29.03.2024\n",
      "Scraping data for date: 30.03.2024\n",
      "Scraping data for date: 31.03.2024\n",
      "Scraping data for date: 01.04.2024\n",
      "Scraping data for date: 02.04.2024\n",
      "Scraping data for date: 03.04.2024\n",
      "Scraping data for date: 04.04.2024\n",
      "Scraping data for date: 05.04.2024\n",
      "Scraping data for date: 06.04.2024\n",
      "Scraping data for date: 07.04.2024\n",
      "Scraping data for date: 08.04.2024\n",
      "Scraping data for date: 09.04.2024\n",
      "Scraping data for date: 10.04.2024\n",
      "Scraping data for date: 11.04.2024\n",
      "Scraping data for date: 12.04.2024\n",
      "Scraping data for date: 13.04.2024\n",
      "Scraping data for date: 14.04.2024\n",
      "Scraping data for date: 15.04.2024\n",
      "Scraping data for date: 16.04.2024\n",
      "Scraping data for date: 17.04.2024\n",
      "Scraping data for date: 18.04.2024\n",
      "Scraping data for date: 19.04.2024\n",
      "Scraping data for date: 20.04.2024\n",
      "Scraping data for date: 21.04.2024\n",
      "Scraping data for date: 22.04.2024\n",
      "Scraping data for date: 23.04.2024\n",
      "Scraping data for date: 24.04.2024\n",
      "Scraping data for date: 25.04.2024\n",
      "Scraping data for date: 26.04.2024\n",
      "Scraping data for date: 27.04.2024\n",
      "Scraping data for date: 28.04.2024\n",
      "Scraping data for date: 29.04.2024\n",
      "Scraping data for date: 30.04.2024\n",
      "No table found for date: 30.04.2024\n",
      "Scraping data for date: 01.05.2024\n",
      "Scraping data for date: 02.05.2024\n",
      "Scraping data for date: 03.05.2024\n",
      "Scraping data for date: 04.05.2024\n",
      "Scraping data for date: 05.05.2024\n",
      "Scraping data for date: 06.05.2024\n",
      "Scraping data for date: 07.05.2024\n",
      "Scraping data for date: 08.05.2024\n",
      "Scraping data for date: 09.05.2024\n",
      "Scraping data for date: 10.05.2024\n",
      "Scraping data for date: 11.05.2024\n",
      "Scraping data for date: 12.05.2024\n",
      "Scraping data for date: 13.05.2024\n",
      "Scraping data for date: 14.05.2024\n",
      "Scraping data for date: 15.05.2024\n",
      "Scraping data for date: 16.05.2024\n",
      "Scraping data for date: 17.05.2024\n",
      "Scraping data for date: 18.05.2024\n",
      "Scraping data for date: 19.05.2024\n",
      "Scraping data for date: 20.05.2024\n",
      "Scraping data for date: 21.05.2024\n",
      "Scraping data for date: 22.05.2024\n",
      "Scraping data for date: 23.05.2024\n",
      "Scraping data for date: 24.05.2024\n",
      "Scraping data for date: 25.05.2024\n",
      "Scraping data for date: 26.05.2024\n",
      "Scraping data for date: 27.05.2024\n",
      "Scraping data for date: 28.05.2024\n",
      "Scraping data for date: 29.05.2024\n",
      "Scraping data for date: 30.05.2024\n",
      "Scraping data for date: 31.05.2024\n",
      "Scraping data for date: 01.06.2024\n",
      "Scraping data for date: 02.06.2024\n",
      "Scraping data for date: 03.06.2024\n",
      "Scraping data for date: 04.06.2024\n",
      "Scraping data for date: 05.06.2024\n",
      "Scraping data for date: 06.06.2024\n",
      "Scraping data for date: 07.06.2024\n",
      "Scraping data for date: 08.06.2024\n",
      "Scraping data for date: 09.06.2024\n",
      "Scraping data for date: 10.06.2024\n",
      "Scraping data for date: 11.06.2024\n",
      "Scraping data for date: 12.06.2024\n",
      "Scraping data for date: 13.06.2024\n",
      "Scraping data for date: 14.06.2024\n",
      "Scraping data for date: 15.06.2024\n",
      "Scraping data for date: 16.06.2024\n",
      "Scraping data for date: 17.06.2024\n",
      "Scraping data for date: 18.06.2024\n",
      "Scraping data for date: 19.06.2024\n",
      "Scraping data for date: 20.06.2024\n",
      "Scraping data for date: 21.06.2024\n",
      "Scraping data for date: 22.06.2024\n",
      "Scraping data for date: 23.06.2024\n",
      "Scraping data for date: 24.06.2024\n",
      "Scraping data for date: 25.06.2024\n",
      "Scraping data for date: 26.06.2024\n",
      "Scraping data for date: 27.06.2024\n",
      "Scraping data for date: 28.06.2024\n",
      "Scraping data for date: 29.06.2024\n",
      "No table found for date: 29.06.2024\n",
      "Scraping data for date: 30.06.2024\n",
      "No table found for date: 30.06.2024\n",
      "Scraping data for date: 01.07.2024\n",
      "No table found for date: 01.07.2024\n",
      "Scraping data for date: 02.07.2024\n",
      "No table found for date: 02.07.2024\n",
      "Scraping data for date: 03.07.2024\n",
      "No table found for date: 03.07.2024\n",
      "Scraping data for date: 04.07.2024\n",
      "No table found for date: 04.07.2024\n",
      "Scraping data for date: 05.07.2024\n",
      "Scraping data for date: 06.07.2024\n",
      "Scraping data for date: 07.07.2024\n",
      "Scraping data for date: 08.07.2024\n",
      "Scraping data for date: 09.07.2024\n",
      "Scraping data for date: 10.07.2024\n",
      "Scraping data for date: 11.07.2024\n",
      "Scraping data for date: 12.07.2024\n",
      "Scraping data for date: 13.07.2024\n",
      "Scraping data for date: 14.07.2024\n",
      "Scraping data for date: 15.07.2024\n",
      "Scraping data for date: 16.07.2024\n",
      "Scraping data for date: 17.07.2024\n",
      "Scraping data for date: 18.07.2024\n",
      "Scraping data for date: 19.07.2024\n",
      "Scraping data for date: 20.07.2024\n",
      "Scraping data for date: 21.07.2024\n",
      "Scraping data for date: 22.07.2024\n",
      "Scraping data for date: 23.07.2024\n",
      "Scraping data for date: 24.07.2024\n",
      "Scraping data for date: 25.07.2024\n",
      "Scraping data for date: 26.07.2024\n",
      "Scraping data for date: 27.07.2024\n",
      "Scraping data for date: 28.07.2024\n",
      "Scraping data for date: 29.07.2024\n",
      "Scraping data for date: 30.07.2024\n",
      "Scraping data for date: 31.07.2024\n",
      "Scraping data for date: 01.08.2024\n",
      "Scraping data for date: 02.08.2024\n",
      "Scraping data for date: 03.08.2024\n",
      "Scraping data for date: 04.08.2024\n",
      "Scraping data for date: 05.08.2024\n",
      "Scraping data for date: 06.08.2024\n",
      "Scraping data for date: 07.08.2024\n",
      "Scraping data for date: 08.08.2024\n",
      "Scraping data for date: 09.08.2024\n",
      "Scraping data for date: 10.08.2024\n",
      "Scraping data for date: 11.08.2024\n",
      "Scraping data for date: 12.08.2024\n",
      "Scraping data for date: 13.08.2024\n",
      "Scraping data for date: 14.08.2024\n",
      "Scraping data for date: 15.08.2024\n",
      "Scraping data for date: 16.08.2024\n",
      "Scraping data for date: 17.08.2024\n",
      "Scraping data for date: 18.08.2024\n",
      "Scraping data for date: 19.08.2024\n",
      "Scraping data for date: 20.08.2024\n",
      "Scraping data for date: 21.08.2024\n",
      "Scraping data for date: 22.08.2024\n",
      "Scraping data for date: 23.08.2024\n",
      "No table found for date: 23.08.2024\n",
      "Scraping data for date: 24.08.2024\n",
      "Scraping data for date: 25.08.2024\n",
      "Scraping data for date: 26.08.2024\n",
      "Scraping data for date: 27.08.2024\n",
      "Scraping data for date: 28.08.2024\n",
      "Scraping data for date: 29.08.2024\n",
      "Scraping data for date: 30.08.2024\n",
      "Scraping data for date: 31.08.2024\n",
      "Scraping data for date: 01.09.2024\n",
      "Scraping data for date: 02.09.2024\n",
      "Scraping data for date: 03.09.2024\n",
      "Scraping data for date: 04.09.2024\n",
      "Scraping data for date: 05.09.2024\n",
      "Scraping data for date: 06.09.2024\n",
      "Scraping data for date: 07.09.2024\n",
      "Scraping data for date: 08.09.2024\n",
      "Scraping data for date: 09.09.2024\n",
      "Scraping data for date: 10.09.2024\n",
      "Scraping data for date: 11.09.2024\n",
      "Scraping data for date: 12.09.2024\n",
      "Scraping data for date: 13.09.2024\n",
      "Scraping data for date: 14.09.2024\n",
      "Scraping data for date: 15.09.2024\n",
      "Scraping data for date: 16.09.2024\n",
      "Scraping data for date: 17.09.2024\n",
      "Scraping data for date: 18.09.2024\n",
      "Scraping data for date: 19.09.2024\n",
      "Scraping data for date: 20.09.2024\n",
      "Scraping data for date: 21.09.2024\n",
      "Scraping data for date: 22.09.2024\n",
      "Scraping data for date: 23.09.2024\n",
      "Scraping data for date: 24.09.2024\n",
      "Scraping data for date: 25.09.2024\n",
      "Scraping data for date: 26.09.2024\n",
      "Scraping data for date: 27.09.2024\n",
      "Scraping data for date: 28.09.2024\n",
      "Scraping data for date: 29.09.2024\n",
      "Scraping data for date: 30.09.2024\n",
      "Scraping data for date: 01.10.2024\n",
      "Scraping data for date: 02.10.2024\n",
      "Scraping data for date: 03.10.2024\n",
      "Scraping data for date: 04.10.2024\n",
      "Scraping data for date: 05.10.2024\n",
      "Scraping data for date: 06.10.2024\n",
      "Scraping data for date: 07.10.2024\n",
      "Scraping data for date: 08.10.2024\n",
      "Scraping data for date: 09.10.2024\n",
      "Scraping data for date: 10.10.2024\n",
      "Scraping data for date: 11.10.2024\n",
      "Scraping data for date: 12.10.2024\n",
      "Scraping data for date: 13.10.2024\n",
      "Scraping data for date: 14.10.2024\n",
      "Scraping data for date: 15.10.2024\n",
      "Scraping data for date: 16.10.2024\n",
      "Scraping data for date: 17.10.2024\n",
      "Scraping data for date: 18.10.2024\n",
      "Scraping data for date: 19.10.2024\n",
      "Scraping data for date: 20.10.2024\n",
      "Scraping data for date: 21.10.2024\n",
      "Scraping data for date: 22.10.2024\n",
      "Scraping data for date: 23.10.2024\n",
      "Scraping data for date: 24.10.2024\n",
      "Scraping data for date: 25.10.2024\n",
      "Scraping data for date: 26.10.2024\n",
      "Scraping data for date: 27.10.2024\n",
      "Scraping data for date: 28.10.2024\n",
      "Scraping data for date: 29.10.2024\n",
      "Scraping data for date: 30.10.2024\n",
      "Scraping data for date: 31.10.2024\n",
      "Scraping data for date: 01.11.2024\n",
      "Scraping data for date: 02.11.2024\n",
      "Scraping data for date: 03.11.2024\n",
      "Scraping data for date: 04.11.2024\n",
      "Scraping data for date: 05.11.2024\n",
      "Scraping data for date: 06.11.2024\n",
      "Scraping data for date: 07.11.2024\n",
      "Scraping data for date: 08.11.2024\n",
      "Scraping data for date: 09.11.2024\n",
      "Scraping data for date: 10.11.2024\n",
      "Scraping data for date: 11.11.2024\n",
      "Scraping data for date: 12.11.2024\n",
      "Scraping data for date: 13.11.2024\n",
      "Scraping data for date: 14.11.2024\n",
      "Scraping data for date: 15.11.2024\n",
      "Scraping data for date: 16.11.2024\n",
      "Scraping data for date: 17.11.2024\n",
      "Scraping data for date: 18.11.2024\n",
      "Scraping data for date: 19.11.2024\n",
      "Scraping data for date: 20.11.2024\n",
      "Scraping data for date: 21.11.2024\n",
      "Scraping data for date: 22.11.2024\n",
      "Data successfully saved to airport_statistics.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Funktion zur Erstellung einer Liste von Datumswerten\n",
    "def generate_dates(start_date, end_date):\n",
    "    \"\"\"Erstellt eine Liste von Datumswerten zwischen zwei Daten.\"\"\"\n",
    "    date_list = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_list.append(current_date.strftime(\"%d.%m.%Y\"))\n",
    "        current_date += timedelta(days=1)\n",
    "    return date_list\n",
    "\n",
    "# Step 2: Scraping-Funktion für eine einzelne Tabelle\n",
    "def scrape_table_for_date(date):\n",
    "    \"\"\"Scrapt die Tabelle für ein bestimmtes Datum.\"\"\"\n",
    "    # URL mit dem dynamischen Datum\n",
    "    url = f\"https://www.dfld.de/Mess/StatAirportTag.php?R=601&D={date}\"\n",
    "    print(f\"Scraping data for date: {date}\")\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Sicherstellen, dass die Anfrage erfolgreich war\n",
    "    \n",
    "    # HTML-Inhalt mit BeautifulSoup parsen\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Tabelle finden\n",
    "    table = soup.find(\"table\", class_=\"table_lines\")  # Ersetzen, falls sich die Tabelle ändert\n",
    "    if not table:\n",
    "        print(f\"No table found for date: {date}\")\n",
    "        return None  # Kein Inhalt für dieses Datum\n",
    "    \n",
    "    # Tabellenkopf scrapen\n",
    "    headers = [header.text.strip() for header in table.find_all(\"th\")]\n",
    "    if not headers:\n",
    "        print(f\"No headers found for date: {date}\")\n",
    "        return None\n",
    "    \n",
    "    # Tabelleninhalte scrapen\n",
    "    rows = []\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cells = row.find_all(\"td\")\n",
    "    # Keep rows even if they have fewer cells\n",
    "        if cells:\n",
    "            # Fill missing cells with None\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            while len(row_data) < len(headers):\n",
    "                row_data.append(None)  # Or use \"\" if you prefer empty strings\n",
    "            rows.append(row_data)\n",
    "\n",
    "    \n",
    "    # DataFrame erstellen\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"No valid data found for date: {date}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Daten für mehrere Tage sammeln\n",
    "def scrape_multiple_days(start_date, end_date, output_file):\n",
    "    \"\"\"Scrapt Tabellen für ein Datumsspektrum und speichert sie in einer CSV-Datei.\"\"\"\n",
    "    all_data = []  # Liste für die gesammelten Daten\n",
    "    \n",
    "    # Liste von Datumswerten erstellen\n",
    "    dates = generate_dates(start_date, end_date)\n",
    "    \n",
    "    # Für jedes Datum scrapen\n",
    "    for date in dates:\n",
    "        df = scrape_table_for_date(date)\n",
    "        if df is not None:\n",
    "            # Spalte für Datum hinzufügen\n",
    "            df[\"Datum\"] = date\n",
    "            all_data.append(df)\n",
    "    \n",
    "    # Alle Daten zusammenfügen und in eine CSV-Datei speichern\n",
    "    if all_data:\n",
    "        final_df = pd.concat(all_data, ignore_index=True)\n",
    "        final_df.to_csv(output_file, index=False)\n",
    "        print(f\"Data successfully saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No data was collected.\")\n",
    "\n",
    "# Step 4: Start- und Enddatum definieren\n",
    "start_date = datetime.strptime(\"01.01.2022\", \"%d.%m.%Y\")  # Startdatum\n",
    "end_date = datetime.strptime(\"22.11.2024\", \"%d.%m.%Y\")    # Enddatum\n",
    "\n",
    "# Daten scrapen und exportieren\n",
    "output_file = \"airport_statistics.csv\"\n",
    "scrape_multiple_days(start_date, end_date, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning Part** \n",
    "Steps:\n",
    "1. Import Data and see what I am working with\n",
    "2. get the flights allocated to the hours\n",
    "3. fill up the days, where no flights were recorded, because its unlikely \n",
    "4. ensure the date is still in the right format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zeit</th>\n",
       "      <th>Dep/Arr</th>\n",
       "      <th>Fluggesellschaft</th>\n",
       "      <th>Dep-Arr</th>\n",
       "      <th>Registrierung Callsign / Flugnummer</th>\n",
       "      <th>Flugzeug</th>\n",
       "      <th>Klasse</th>\n",
       "      <th>Piste</th>\n",
       "      <th>Datum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:13:33</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HB-ZQGRGA02</td>\n",
       "      <td>EC45</td>\n",
       "      <td>L</td>\n",
       "      <td>33.0</td>\n",
       "      <td>01.01.2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06:22:39</td>\n",
       "      <td></td>\n",
       "      <td>DS (CH)easyJet Switzerland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EZS5632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>01.01.2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06:26:36</td>\n",
       "      <td></td>\n",
       "      <td>LH (DE)Deutsche Lufthansa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D-ACNGDLH6TN</td>\n",
       "      <td>CRJ9</td>\n",
       "      <td>M</td>\n",
       "      <td>33.0</td>\n",
       "      <td>01.01.2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06:29:23</td>\n",
       "      <td></td>\n",
       "      <td>KL (NL)KLM Royal Dutch Airlines</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PH-EXRKLM84X</td>\n",
       "      <td>E75L</td>\n",
       "      <td>M</td>\n",
       "      <td>33.0</td>\n",
       "      <td>01.01.2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06:37:38</td>\n",
       "      <td></td>\n",
       "      <td>8C (CN)Shanxi Airlines</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CXI7265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>01.01.2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Zeit Dep/Arr                 Fluggesellschaft  Dep-Arr  \\\n",
       "0  00:13:33                                     NaN      NaN   \n",
       "1  06:22:39              DS (CH)easyJet Switzerland      NaN   \n",
       "2  06:26:36               LH (DE)Deutsche Lufthansa      NaN   \n",
       "3  06:29:23         KL (NL)KLM Royal Dutch Airlines      NaN   \n",
       "4  06:37:38                  8C (CN)Shanxi Airlines      NaN   \n",
       "\n",
       "  Registrierung Callsign / Flugnummer Flugzeug Klasse  Piste       Datum  \n",
       "0                         HB-ZQGRGA02     EC45      L   33.0  01.01.2022  \n",
       "1                             EZS5632      NaN    NaN   33.0  01.01.2022  \n",
       "2                        D-ACNGDLH6TN     CRJ9      M   33.0  01.01.2022  \n",
       "3                        PH-EXRKLM84X     E75L      M   33.0  01.01.2022  \n",
       "4                             CXI7265      NaN    NaN   15.0  01.01.2022  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_statistics = pd.read_csv(\"./airport_statistics.csv\")\n",
    "airport_statistics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly Traffic Data (missing hours filled with weekday averages):\n",
      "                          Datum  Hour  Traffic   Weekday\n",
      "0     2022-01-01 00:00:00+00:00     0      1.0  Saturday\n",
      "1     2022-01-01 00:00:00+00:00     1      0.0  Saturday\n",
      "2     2022-01-01 00:00:00+00:00     2      0.0  Saturday\n",
      "3     2022-01-01 00:00:00+00:00     3      0.0  Saturday\n",
      "4     2022-01-01 00:00:00+00:00     4      0.0  Saturday\n",
      "...                         ...   ...      ...       ...\n",
      "25363 2024-11-22 00:00:00+00:00    19      9.0    Friday\n",
      "25364 2024-11-22 00:00:00+00:00    20     11.0    Friday\n",
      "25365 2024-11-22 00:00:00+00:00    21     12.0    Friday\n",
      "25366 2024-11-22 00:00:00+00:00    22      9.0    Friday\n",
      "25367 2024-11-22 00:00:00+00:00    23      5.0    Friday\n",
      "\n",
      "[25368 rows x 4 columns]\n",
      "\n",
      "Daily Traffic Data (missing days filled with hourly averages of weekday):\n",
      "                         Datum  Traffic\n",
      "0    2022-01-01 00:00:00+00:00    168.0\n",
      "1    2022-01-02 00:00:00+00:00    191.0\n",
      "2    2022-01-03 00:00:00+00:00    171.0\n",
      "3    2022-01-04 00:00:00+00:00    141.0\n",
      "4    2022-01-05 00:00:00+00:00     46.0\n",
      "...                        ...      ...\n",
      "1052 2024-11-18 00:00:00+00:00    208.0\n",
      "1053 2024-11-19 00:00:00+00:00    144.0\n",
      "1054 2024-11-20 00:00:00+00:00    198.0\n",
      "1055 2024-11-21 00:00:00+00:00    117.0\n",
      "1056 2024-11-22 00:00:00+00:00    200.0\n",
      "\n",
      "[1057 rows x 2 columns]\n",
      "214335.8476821192\n"
     ]
    }
   ],
   "source": [
    "# Assuming airport_statistics is a pandas DataFrame\n",
    "\n",
    "# Filter out invalid 'Zeit' values before conversion\n",
    "airport_statistics = airport_statistics[~airport_statistics[\"Zeit\"].str.contains(\"Summe\")]\n",
    "\n",
    "# Convert the 'Datum' column to a proper datetime format\n",
    "airport_statistics[\"Datum\"] = pd.to_datetime(airport_statistics[\"Datum\"], format=\"%d.%m.%Y\", utc=True)\n",
    "\n",
    "# Convert the 'Zeit' column to a time format\n",
    "airport_statistics[\"Zeit\"] = pd.to_datetime(airport_statistics[\"Zeit\"], format=\"%H:%M:%S\", utc=True).dt.time\n",
    "\n",
    "# Create a new column for the hour extracted from 'Zeit'\n",
    "airport_statistics[\"Hour\"] = pd.to_datetime(airport_statistics[\"Zeit\"].astype(str), format=\"%H:%M:%S\").dt.hour\n",
    "\n",
    "# Group by 'Datum' and 'Hour' to calculate the hourly count of traffic\n",
    "hourly_sum = airport_statistics.groupby([\"Datum\", \"Hour\"]).size().reset_index(name=\"Traffic\")\n",
    "\n",
    "# Ensure all hours (0-23) are present for each day\n",
    "all_dates = pd.date_range(hourly_sum[\"Datum\"].min(), hourly_sum[\"Datum\"].max(), freq=\"D\")\n",
    "all_hours = pd.DataFrame({\"Hour\": range(24)})\n",
    "all_combinations = pd.MultiIndex.from_product([all_dates, all_hours[\"Hour\"]], names=[\"Datum\", \"Hour\"])\n",
    "\n",
    "hourly_sum = hourly_sum.set_index([\"Datum\", \"Hour\"]).reindex(all_combinations, fill_value=0).reset_index()\n",
    "\n",
    "# Add the weekday for each row\n",
    "hourly_sum[\"Weekday\"] = hourly_sum[\"Datum\"].dt.day_name()\n",
    "\n",
    "# Calculate hourly averages for each weekday\n",
    "hourly_weekday_averages = hourly_sum.groupby([\"Weekday\", \"Hour\"])[\"Traffic\"].mean().reset_index()\n",
    "\n",
    "# Ensure the 'Traffic' column is of a compatible dtype (float)\n",
    "hourly_sum[\"Traffic\"] = hourly_sum[\"Traffic\"].astype(float)\n",
    "\n",
    "# Fill in missing days with hourly averages for the same weekday\n",
    "missing_days = hourly_sum.groupby(\"Datum\")[\"Traffic\"].sum() == 0  # Identify missing days\n",
    "for missing_date in hourly_sum[\"Datum\"].unique()[missing_days]:\n",
    "    weekday = missing_date.day_name()\n",
    "    avg_values = hourly_weekday_averages[hourly_weekday_averages[\"Weekday\"] == weekday]\n",
    "    for _, row in avg_values.iterrows():\n",
    "        hourly_sum.loc[\n",
    "            (hourly_sum[\"Datum\"] == missing_date) & (hourly_sum[\"Hour\"] == row[\"Hour\"]), \"Traffic\"\n",
    "        ] = row[\"Traffic\"]\n",
    "\n",
    "# Group by 'Datum' to calculate the daily count of traffic\n",
    "daily_sum = hourly_sum.groupby(\"Datum\").agg(Traffic=(\"Traffic\", \"sum\")).reset_index()\n",
    "\n",
    "\n",
    "total_sum = daily_sum[\"Traffic\"].sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"Hourly Traffic Data (missing hours filled with weekday averages):\")\n",
    "print(hourly_sum)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nDaily Traffic Data (missing days filled with hourly averages of weekday):\")\n",
    "print(daily_sum)\n",
    "\n",
    "print(total_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code doesnt want to keep the hour somehow, so its entered afterwards via adding hour column to the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'Datum' and 'Hour' into a single datetime column\n",
    "hourly_sum[\"Datum\"] = hourly_sum.apply(\n",
    "    lambda row: pd.Timestamp(row[\"Datum\"]) + pd.Timedelta(hours=row[\"Hour\"]), axis=1\n",
    ")\n",
    "\n",
    "# Ensure the new column is in the desired datetime format\n",
    "hourly_sum[\"Datum\"] = pd.to_datetime(hourly_sum[\"Datum\"], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_statistics.head()\n",
    "hourly_sum.head(25)\n",
    "hourly_sum.drop(columns=[\"Weekday\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cleaned Data to the cleaned Data Folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hourly_sum.to_csv(\"../Final_Data/Cleaned/Airport_traffic_hourly_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some old code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming airport_statistics is a pandas DataFrame\n",
    "\n",
    "# Ensure the 'Zeit' column is of type string\n",
    "airport_statistics[\"Zeit\"] = airport_statistics[\"Zeit\"].astype(str)\n",
    "\n",
    "# Filter out invalid 'Zeit' values (e.g., containing \"Summe\")\n",
    "airport_statistics = airport_statistics[~airport_statistics[\"Zeit\"].str.contains(\"Summe\", na=False)]\n",
    "\n",
    "# Convert the 'Datum' column to a proper datetime format\n",
    "airport_statistics[\"Datum\"] = pd.to_datetime(airport_statistics[\"Datum\"], format=\"%d.%m.%Y\", utc=True)\n",
    "\n",
    "# Convert the 'Zeit' column to a time format\n",
    "airport_statistics[\"Zeit\"] = pd.to_datetime(airport_statistics[\"Zeit\"], format=\"%H:%M:%S\", errors='coerce').dt.time\n",
    "\n",
    "# Drop rows where 'Zeit' could not be converted to a valid time\n",
    "airport_statistics = airport_statistics.dropna(subset=[\"Zeit\"])\n",
    "\n",
    "# Combine 'Datum' and 'Zeit' into a single datetime column\n",
    "airport_statistics[\"Datum\"] = airport_statistics.apply(\n",
    "    lambda row: pd.Timestamp.combine(row[\"Datum\"].date(), row[\"Zeit\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Group by 'Datum' to calculate the hourly count of traffic\n",
    "hourly_sum = airport_statistics.groupby(\"Datum\").size().reset_index(name=\"Traffic\")\n",
    "\n",
    "# Ensure all hours (0-23) are present for each day\n",
    "all_hours = pd.date_range(\n",
    "    hourly_sum[\"Datum\"].min().floor(\"D\"),\n",
    "    hourly_sum[\"Datum\"].max().ceil(\"D\"),\n",
    "    freq=\"H\"\n",
    ")\n",
    "hourly_sum = hourly_sum.set_index(\"Datum\").reindex(all_hours, fill_value=0).reset_index()\n",
    "hourly_sum.rename(columns={\"index\": \"Datum\"}, inplace=True)\n",
    "\n",
    "# Calculate hourly averages for each hour (optional, if needed)\n",
    "hourly_sum[\"Hour\"] = hourly_sum[\"Datum\"].dt.hour\n",
    "hourly_hourly_averages = hourly_sum.groupby(\"Hour\")[\"Traffic\"].mean().reset_index()\n",
    "\n",
    "# Fill in missing hourly traffic values with hourly averages\n",
    "hourly_sum[\"Traffic\"] = hourly_sum[\"Traffic\"].where(\n",
    "    hourly_sum[\"Traffic\"] != 0,\n",
    "    hourly_sum[\"Hour\"].map(hourly_hourly_averages.set_index(\"Hour\")[\"Traffic\"])\n",
    ")\n",
    "\n",
    "# Group by 'Datum' to calculate the daily count of traffic\n",
    "daily_sum = hourly_sum.groupby(hourly_sum[\"Datum\"].dt.floor(\"D\")).agg(Traffic=(\"Traffic\", \"sum\")).reset_index()\n",
    "daily_sum.rename(columns={\"Datum\": \"Day\"}, inplace=True)\n",
    "\n",
    "# Calculate the total traffic\n",
    "total_sum = daily_sum[\"Traffic\"].sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"Hourly Traffic Data (missing hours filled with hourly averages):\")\n",
    "print(hourly_sum)\n",
    "\n",
    "print(\"\\nDaily Traffic Data (summarized by day):\")\n",
    "print(daily_sum)\n",
    "\n",
    "print(f\"\\nTotal Traffic: {total_sum}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assuming 'airport_statistics' is the DataFrame with 'Datum' (datetime) and 'Flugbewegungen_per_hour' (traffic)\n",
    "\n",
    "# If 'Datum' is in datetime format, we can directly extract the date part\n",
    "Airport_total['Datum'] = pd.to_datetime(Airport_total['Datum'])  # Ensure 'Datum' is in datetime format\n",
    "\n",
    "# Extract the date part only (ignoring the time)\n",
    "Airport_total['Date'] = Airport_total['Datum'].dt.date\n",
    "\n",
    "# Group by the 'Date' and calculate the sum of 'Flugbewegungen_per_hour'\n",
    "daily_sum2 = Airport_total.groupby('Date')['Flugbewegungen_per_hour'].sum().reset_index()\n",
    "\n",
    "\n",
    "# Display the result\n",
    "print(\"Daily Sum of 'Flugbewegungen_per_hour':\")\n",
    "print(daily_sum2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "flug_uncleaned = pd.read_csv(r\"C:\\Users\\maxd2\\OneDrive - Universitaet St.Gallen\\Dokumente\\GitHub\\Its-Wekk\\4 - Data\\Final_Data\\EuroPort_Flugbewegungen.csv\", sep=\";\")\n",
    "flug_uncleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flug_uncleaned.sort_values(by=['Datum'], inplace=True)\n",
    "flug_uncleaned = flug_uncleaned[flug_uncleaned['Datum'] >= '2022-01-01']\n",
    "flug_uncleaned.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the column names in both DataFrames\n",
    "print(\"Columns in daily_sum:\")\n",
    "print(daily_sum.columns)\n",
    "\n",
    "print(\"\\nColumns in daily_sum2:\")\n",
    "print(daily_sum2.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename columns in daily_sum2 to match daily_sum for consistency\n",
    "daily_sum2 = daily_sum2.rename(columns={'Date': 'Datum', 'Flugbewegungen_per_hour': 'Traffic'})\n",
    "\n",
    "# Now both dataframes have 'Datum' as the date column and 'Traffic' as the traffic column\n",
    "\n",
    "# Loop through each row in daily_sum and compare with daily_sum2\n",
    "for i in range(len(daily_sum)):\n",
    "    # Ensure the date columns match\n",
    "    if daily_sum.loc[i, \"Datum\"] == daily_sum2.loc[i, \"Datum\"]:\n",
    "        if daily_sum.loc[i, \"Traffic\"] != daily_sum2.loc[i, \"Traffic\"]:\n",
    "            # Calculate the difference\n",
    "            x = daily_sum2.loc[i, \"Traffic\"] - daily_sum.loc[i, \"Traffic\"]\n",
    "            print(f\"Difference for {daily_sum.loc[i, 'Datum']}: {x}\")\n",
    "    else:\n",
    "        # Skip if 'Datum' does not match\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round floats to integers\n",
    "daily_sum['Traffic'] = daily_sum['Traffic'].round().astype(int)\n",
    "daily_sum2['Flugbewegungen_per_hour'] = daily_sum2['Flugbewegungen_per_hour'].round().astype(int)\n",
    "\n",
    "# Convert 'Date' to datetime in daily_sum2\n",
    "daily_sum2['Date'] = pd.to_datetime(daily_sum2['Date'])\n",
    "\n",
    "# Merge, calculate difference, and filter\n",
    "merged_df = pd.merge(daily_sum, daily_sum2, left_on='Datum', right_on='Date', how='inner')\n",
    "merged_df['Difference'] = merged_df['Traffic'] - merged_df['Flugbewegungen_per_hour']\n",
    "differences = merged_df[merged_df['Difference'] != 0]\n",
    "\n",
    "# Display differences\n",
    "print(differences[['Datum', 'Traffic', 'Flugbewegungen_per_hour', 'Difference']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the total of 'Traffic' column in daily_sum\n",
    "total_traffic = daily_sum['Traffic'].sum()\n",
    "\n",
    "# Calculate the total of 'Flugbewegungen_per_hour' column in daily_sum2\n",
    "total_flugbewegungen_per_hour = daily_sum2['Flugbewegungen_per_hour'].sum()\n",
    "\n",
    "print(f\"Total Traffic: {total_traffic}\")\n",
    "print(f\"Total Flugbewegungen_per_hour: {total_flugbewegungen_per_hour}\")\n",
    "\n",
    "for idx,row in differences.iterrows():\n",
    "    if abs(row[\"Flugbewegungen_per_hour\"]) == abs(row[\"Difference\"]):\n",
    "        print(row[\"Flugbewegungen_per_hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
