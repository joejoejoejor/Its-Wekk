{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use a nicer style for plots\n",
    "plt.style.use(\"seaborn-v0_8-muted\")\n",
    "\n",
    "# Import the regression tree from scikit-learn and a plotting helper\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# Import our train_test_split helper\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset into target and features and split them into test train Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = pd.read_csv(\"/Users/rubenstark/Documents/GitHub/Its-Wekk/4 - Data/2 - Ruben/Final_Target_Data_Combined_resid_Trend\")\n",
    "\n",
    "#features Dataset muss noch angepasst werden\n",
    "features_data = pd.read_csv(\"/Users/rubenstark/Documents/GitHub/Its-Wekk/4 - Data/2 - Ruben/Working_DataFrame.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konvertiere die Datumsspalte in einen datetime-Index (falls nicht bereits)\n",
    "target_data['Datum'] = pd.to_datetime(target_data['Datum'])\n",
    "features_data['Datum'] = pd.to_datetime(features_data['Datum'])\n",
    "\n",
    "# Definiere das Cut-Off-Datum\n",
    "cutoff_date = pd.Timestamp('2024-10-20 21:00:00+00:00')\n",
    "\n",
    "# Filtere das Dataset auf Einträge bis einschließlich des Cut-Off-Datums\n",
    "target_data_cutted = target_data[target_data['Datum'] <= cutoff_date]\n",
    "features_data_cutted = features_data[features_data['Datum'] <= cutoff_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 17201\n",
      "Validation Size: 2458\n",
      "Test Size: 4915\n"
     ]
    }
   ],
   "source": [
    "# Split our data intro features and targets\n",
    "# Teile das Dataset in Features und Zielvariable\n",
    "y = target_data_cutted[\"PM10_Combined_Trend_Residual\"]  # Zielvariable\n",
    "X = features_data_cutted.drop(columns=[\"Datum\"])  # Alle Spalten außer der Zielvariable\n",
    "\n",
    "X.head(10)\n",
    "\n",
    "# Daten splitten\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=72)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=72)  # 10% von Gesamt\n",
    "\n",
    "print(\"Train Size:\", len(X_train))\n",
    "print(\"Validation Size:\", len(X_val))\n",
    "print(\"Test Size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 64.60048714505179\n",
      "Fold 2: MSE = 65.26163477989127\n",
      "Fold 3: MSE = 62.40212093212197\n",
      "Fold 4: MSE = 61.04229798932156\n",
      "Fold 5: MSE = 58.06193083868737\n",
      "Durchschnittlicher MSE über alle Folds: 62.27369433701479\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Expanding Cross-Validation (5 Splits)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "results = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict and calculate MSE\n",
    "    y_pred = model.predict(X_val_fold)\n",
    "    mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results.append(mse)\n",
    "    print(f\"Fold {fold + 1}: MSE = {mse}\")\n",
    "\n",
    "# Average MSE across all folds\n",
    "average_mse = np.mean(results)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ccp_alpha (and other parameters) to optimize the Decision Tree for example when it comes to overfitting\n",
    "\n",
    "[`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 62.862562216827534\n",
      "Fold 2: MSE = 63.09566186655212\n",
      "Fold 3: MSE = 58.091970327110744\n",
      "Fold 4: MSE = 55.76480915646729\n",
      "Fold 5: MSE = 52.66354122276783\n",
      "Durchschnittlicher MSE über alle Folds: 58.49570895794511\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_ccp = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Modell trainieren\n",
    "    tree_ccp = DecisionTreeRegressor(ccp_alpha=0.01)\n",
    "    tree_ccp.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred_ccp = tree_ccp.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    mse_ccp = mean_squared_error(y_val_fold, y_pred_ccp)\n",
    "    results_ccp.append(mse_ccp)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {mse_ccp}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_ccp = np.mean(results_ccp)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_ccp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation to find best alpha \n",
    "\n",
    " Geht nicht mit so vielen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools for model selection\n",
    "from sklearn.model_selection import cross_validate, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimales ccp_alpha: 0.10481131341546852\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Definiere die Werte für ccp_alpha (Cost Complexity Pruning)\n",
    "alphas = np.logspace(-4, 0, 50)  # Werte zwischen 10^-4 und 10^0\n",
    "\n",
    "# Initialisiere Cross-Validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=72)\n",
    "\n",
    "# Speicher für Ergebnisse\n",
    "scores = []  # Durchschnittliche MSE für jedes ccp_alpha\n",
    "scores_std = []  # Standardabweichung der Scores für Stabilitätsanalyse\n",
    "\n",
    "# Cross-Validation für jedes ccp_alpha\n",
    "for alpha in alphas:\n",
    "    # Decision Tree mit aktuellem ccp_alpha-Wert\n",
    "    tree_cv = DecisionTreeRegressor(ccp_alpha=alpha, random_state=72)\n",
    "    \n",
    "    # Negative MSE, da cross_val_score maximiert; wir wollen minimieren\n",
    "    mse_scores = cross_val_score(tree_cv, X_train, y_train, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "    \n",
    "    # Durchschnittlichen MSE speichern (negativ, daher multiplizieren mit -1)\n",
    "    scores.append(-mse_scores.mean())\n",
    "    scores_std.append(mse_scores.std())\n",
    "\n",
    "# Optimiere ccp_alpha: Der Wert mit dem niedrigsten MSE\n",
    "optimal_alpha = alphas[np.argmin(scores)]\n",
    "\n",
    "print(f\"Optimales ccp_alpha: {optimal_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 55.5456298158434\n",
      "Fold 2: MSE = 47.179887765873374\n",
      "Fold 3: MSE = 44.651038715518865\n",
      "Fold 4: MSE = 42.46225589862255\n",
      "Fold 5: MSE = 34.31281454209581\n",
      "Durchschnittlicher MSE über alle Folds: 44.8303253475908\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_ccp = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Modell trainieren\n",
    "    tree_ccp = DecisionTreeRegressor(ccp_alpha=optimal_alpha)\n",
    "    tree_ccp.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred_ccp = tree_ccp.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    mse_ccp = mean_squared_error(y_val_fold, y_pred_ccp)\n",
    "    results_ccp.append(mse_ccp)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {mse_ccp}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_ccp = np.mean(results_ccp)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_ccp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging (Bootstrap Aggregating) ist eine Technik, um die Stabilität und Genauigkeit von Machine-Learning-Algorithmen zu verbessern, insbesondere bei Modellen wie Entscheidungsbäumen, die anfällig für hohe Varianz sind. Es basiert auf dem Bootstrapping-Prinzip, bei dem mehrere Trainingssets durch Zufallsstichproben mit Zurücklegen erzeugt werden.\n",
    "\n",
    "Jeder Baum wird auf einem dieser zufälligen Datensets trainiert, und die Vorhersagen der B Modelle werden durch Mittelung kombiniert. Mathematisch reduziert Bagging die Varianz der Modelle, weil unabhängige Fehler über die Modelle hinweg geglättet werden. So wird die Vorhersage insgesamt stabiler und robuster gegen Variationen in den Trainingsdaten.\n",
    "\n",
    "Das Ziel ist, Vorhersagefehler durch Mittelung der Outputs der individuellen Modelle zu minimieren, was insgesamt zu einer besseren Modellleistung führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE:  4.14416781416351\n",
      "Test MSE :  43.869268188376886\n",
      "Fold 1: MSE = 33.214242439500325\n",
      "Fold 2: MSE = 32.6710615934234\n",
      "Fold 3: MSE = 30.241463693003745\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m y_val_fold \u001b[38;5;241m=\u001b[39m y_train_init\u001b[38;5;241m.\u001b[39miloc[val_index]\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Modell trainieren\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mbagged_trees\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Vorhersagen machen\u001b[39;00m\n\u001b[1;32m     52\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m bagged_trees\u001b[38;5;241m.\u001b[39mpredict(X_val_fold)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:66\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     69\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     72\u001b[0m ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_bagging.py:402\u001b[0m, in \u001b[0;36mBaseBagging.fit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    399\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    400\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_bagging.py:545\u001b[0m, in \u001b[0;36mBaseBagging._fit\u001b[0;34m(self, X, y, max_samples, max_depth, check_input, **fit_params)\u001b[0m\n\u001b[1;32m    542\u001b[0m seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39mn_more_estimators)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds \u001b[38;5;241m=\u001b[39m seeds\n\u001b[0;32m--> 545\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_estimators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_n_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# Reduce\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    564\u001b[0m     itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_results)\n\u001b[1;32m    565\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_bagging.py:187\u001b[0m, in \u001b[0;36m_parallel_build_estimators\u001b[0;34m(n_estimators, ensemble, X, y, seeds, total_n_estimators, verbose, check_input, fit_params)\u001b[0m\n\u001b[1;32m    185\u001b[0m     fit_params_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m curr_sample_weight\n\u001b[1;32m    186\u001b[0m     X_ \u001b[38;5;241m=\u001b[39m X[:, features] \u001b[38;5;28;01mif\u001b[39;00m requires_feature_indexing \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 187\u001b[0m     \u001b[43mestimator_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# cannot use sample_weight, so use indexing\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     y_ \u001b[38;5;241m=\u001b[39m _safe_indexing(y, indices)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/tree/_classes.py:1377\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \n\u001b[1;32m   1351\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1377\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import the regression tree from scikit-learn and a plotting helper\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# Import our train_test_split helper\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import the mean_squared_error function under the alias mse\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "# Import the resampling helper\n",
    "from sklearn.utils import resample\n",
    "# Import the sklearn implementation of bagging\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a bagged tree estimator with B=100 trees\n",
    "bagged_trees = BaggingRegressor(DecisionTreeRegressor(), n_estimators=100)\n",
    "\n",
    "# Fit the bagged estimator and compute the MSE on the training set\n",
    "bagged_trees.fit(X_train, y_train)\n",
    "\n",
    "# Compute the predictions on the training and test sets\n",
    "y_pred_train_bag = bagged_trees.predict(X_train)\n",
    "y_pred_test_bag = bagged_trees.predict(X_test)\n",
    "\n",
    "print(\"Train MSE: \", mean_squared_error(y_train, y_pred_train_bag))\n",
    "print(\"Test MSE : \", mean_squared_error(y_test, y_pred_test_bag))\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_bag = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Modell trainieren\n",
    "    bagged_trees.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred = bagged_trees.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results_bag.append(mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_bag = np.mean(results_bag)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_bag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest\n",
    "\n",
    "Random Forests erweitern Bagging, indem sie jedem Baum eine zusätzliche Zufallskomponente hinzufügen. Jeder Baum wird mit einem bootstrap-Sample der Trainingsdaten trainiert, wobei nur ein zufälliger Teil der Features für die Konstruktion des Baums verwendet wird. Dadurch unterscheidet sich Random Forests von klassischem Bagging, bei dem alle Features verfügbar sind.\n",
    "\n",
    "Die zufällige Auswahl der Features reduziert die Korrelation zwischen den Bäumen und verbessert die Generalisierung des Modells. Üblicherweise wird die Anzahl der verwendeten Features  m  so gewählt, dass  m \\approx \\sqrt{p} , wobei  p  die Gesamtzahl der Features ist. Wenn  m = p  gesetzt wird, ist Random Forest gleichbedeutend mit einem Bagging-Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE:  4.515092698542732\n",
      "Test MSE :  46.555903436203614\n",
      "Fold 1: MSE = 32.61206161809249\n",
      "Fold 2: MSE = 33.32146476267613\n",
      "Fold 3: MSE = 32.6669091954285\n",
      "Fold 4: MSE = 33.44130434933117\n",
      "Fold 5: MSE = 29.35676751830648\n",
      "Durchschnittlicher MSE über alle Folds: 32.27970148876695\n"
     ]
    }
   ],
   "source": [
    "# Import the random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize the random forest regressor\n",
    "rf = RandomForestRegressor(n_estimators=100, max_features=\"sqrt\")\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Compute the predictions on the training and test sets\n",
    "y_pred_train_rf = rf.predict(X_train)\n",
    "y_pred_test_rf = rf.predict(X_test)\n",
    "\n",
    "# Print the mean squared error for training and test sets\n",
    "print(\"Train MSE: \", mean_squared_error(y_train, y_pred_train_rf))\n",
    "print(\"Test MSE : \", mean_squared_error(y_test, y_pred_test_rf))\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_rf = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    X_train_fold, X_val_fold = X_train_init.iloc[train_index], X_train_init.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_init.iloc[train_index], y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Modell trainieren\n",
    "    rf.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred = rf.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    fold_mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results_rf.append(fold_mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {fold_mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_rf = np.mean(results_rf)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Unterschied zwischen der Verwendung eines Integers oder eines Floats bei der Angabe von max_features in einem Random Forest Modell (wie in Scikit-learn) liegt in der Bedeutung des Parameters und wie die Anzahl der maximal zu betrachtenden Features berechnet wird:\n",
    "\n",
    "1. Wenn max_features ein Integer ist:\n",
    "\n",
    "\t•\tDer Wert gibt die exakte Anzahl der maximal zu betrachtenden Features an, die bei der Teilung eines Knotens in jedem Decision Tree berücksichtigt werden sollen.\n",
    "\t•\tBeispiel: max_features=3 bedeutet, dass 3 Features aus dem gesamten Feature-Set zufällig ausgewählt werden, um die beste Teilung zu bestimmen.\n",
    "\n",
    "2. Wenn max_features ein Float ist:\n",
    "\n",
    "\t•\tDer Wert gibt einen Prozentsatz der verfügbaren Features an, die verwendet werden sollen. Der Float-Wert muss zwischen 0.0 und 1.0 liegen.\n",
    "\t•\tBeispiel: max_features=0.5 bedeutet, dass 50 % der Features (aufgerundet) zufällig ausgewählt werden, um die beste Teilung zu bestimmen.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagged Variable mit 1h lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6l/nfq1629j4jg8fynztn5kxlph0000gn/T/ipykernel_3969/378574758.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_data_cutted['Datum'] = pd.to_datetime(target_data_cutted['Datum']).copy()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datum</th>\n",
       "      <th>PM10_Combined_Trend_Residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-31 23:00:00+00:00</td>\n",
       "      <td>75.197962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 00:00:00+00:00</td>\n",
       "      <td>51.472071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 01:00:00+00:00</td>\n",
       "      <td>32.710483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 02:00:00+00:00</td>\n",
       "      <td>24.801767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 03:00:00+00:00</td>\n",
       "      <td>9.683660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Datum  PM10_Combined_Trend_Residual\n",
       "0 2021-12-31 23:00:00+00:00                     75.197962\n",
       "1 2022-01-01 00:00:00+00:00                     51.472071\n",
       "2 2022-01-01 01:00:00+00:00                     32.710483\n",
       "3 2022-01-01 02:00:00+00:00                     24.801767\n",
       "4 2022-01-01 03:00:00+00:00                      9.683660"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure 'Datum' column is in datetime format\n",
    "target_data_cutted['Datum'] = pd.to_datetime(target_data_cutted['Datum']).copy()\n",
    "\n",
    "# Create a copy of target_data to apply the offset\n",
    "lagged_target_variable_1h = target_data.copy()\n",
    "\n",
    "\n",
    "# Offset von -1 Stunde anwenden\n",
    "lagged_target_variable_1h['Datum'] = target_data['Datum'] + pd.Timedelta(hours=-1)\n",
    "\n",
    "lagged_target_variable_1h.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datum</th>\n",
       "      <th>Basel Temperature [2 m elevation corrected]</th>\n",
       "      <th>Basel Precipitation Total</th>\n",
       "      <th>Basel Wind Speed [10 m]</th>\n",
       "      <th>Basel Wind Direction [10 m]</th>\n",
       "      <th>Stromverbrauch</th>\n",
       "      <th>350n_sumLief</th>\n",
       "      <th>350v_sumLW</th>\n",
       "      <th>352v_sumPW</th>\n",
       "      <th>352v_sumLief</th>\n",
       "      <th>...</th>\n",
       "      <th>660v_sumPW</th>\n",
       "      <th>660v_sumLW</th>\n",
       "      <th>660n_sumPW</th>\n",
       "      <th>660n_sumLW</th>\n",
       "      <th>84111104n_sumLief</th>\n",
       "      <th>84111104v_sumLief</th>\n",
       "      <th>84111108v_sumLief</th>\n",
       "      <th>Gasverbrauch</th>\n",
       "      <th>Traffic</th>\n",
       "      <th>PM10_Combined_Trend_Residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01 00:00:00+00:00</td>\n",
       "      <td>-0.926467</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.694066</td>\n",
       "      <td>0.115891</td>\n",
       "      <td>-1.116259</td>\n",
       "      <td>-0.969412</td>\n",
       "      <td>-0.731284</td>\n",
       "      <td>-0.253232</td>\n",
       "      <td>-0.810792</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.540182</td>\n",
       "      <td>-1.300587</td>\n",
       "      <td>-1.679601</td>\n",
       "      <td>-1.288546</td>\n",
       "      <td>2.164977</td>\n",
       "      <td>1.404508</td>\n",
       "      <td>-0.413276</td>\n",
       "      <td>0.294494</td>\n",
       "      <td>-1.142151</td>\n",
       "      <td>51.472071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 01:00:00+00:00</td>\n",
       "      <td>-1.041967</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.918024</td>\n",
       "      <td>0.047361</td>\n",
       "      <td>-1.368317</td>\n",
       "      <td>-0.910171</td>\n",
       "      <td>-0.809324</td>\n",
       "      <td>-0.370929</td>\n",
       "      <td>-0.810792</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.577611</td>\n",
       "      <td>-1.300587</td>\n",
       "      <td>-1.679601</td>\n",
       "      <td>-1.288546</td>\n",
       "      <td>1.515830</td>\n",
       "      <td>0.733423</td>\n",
       "      <td>-0.767192</td>\n",
       "      <td>0.357518</td>\n",
       "      <td>-1.295479</td>\n",
       "      <td>32.710483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 02:00:00+00:00</td>\n",
       "      <td>-1.162435</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.857675</td>\n",
       "      <td>-0.143112</td>\n",
       "      <td>-1.523703</td>\n",
       "      <td>-0.969412</td>\n",
       "      <td>-0.731284</td>\n",
       "      <td>-0.841720</td>\n",
       "      <td>-0.909298</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.568254</td>\n",
       "      <td>-1.300587</td>\n",
       "      <td>-1.591872</td>\n",
       "      <td>-1.288546</td>\n",
       "      <td>0.798352</td>\n",
       "      <td>0.565652</td>\n",
       "      <td>-0.413276</td>\n",
       "      <td>0.385414</td>\n",
       "      <td>-1.295479</td>\n",
       "      <td>24.801767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 03:00:00+00:00</td>\n",
       "      <td>-1.135112</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.831118</td>\n",
       "      <td>-0.006355</td>\n",
       "      <td>-1.546518</td>\n",
       "      <td>-0.880550</td>\n",
       "      <td>-0.809324</td>\n",
       "      <td>-1.004687</td>\n",
       "      <td>-0.712287</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.399824</td>\n",
       "      <td>-1.198189</td>\n",
       "      <td>-1.143479</td>\n",
       "      <td>-1.076564</td>\n",
       "      <td>0.832517</td>\n",
       "      <td>0.263664</td>\n",
       "      <td>-0.413276</td>\n",
       "      <td>0.617131</td>\n",
       "      <td>-1.295479</td>\n",
       "      <td>9.683660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 04:00:00+00:00</td>\n",
       "      <td>-1.163677</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.687750</td>\n",
       "      <td>-0.148316</td>\n",
       "      <td>-1.432685</td>\n",
       "      <td>-0.910171</td>\n",
       "      <td>-0.887364</td>\n",
       "      <td>-1.059009</td>\n",
       "      <td>-0.958550</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.960036</td>\n",
       "      <td>-0.890998</td>\n",
       "      <td>-0.714582</td>\n",
       "      <td>-0.864583</td>\n",
       "      <td>0.695855</td>\n",
       "      <td>0.565652</td>\n",
       "      <td>-0.649220</td>\n",
       "      <td>1.109860</td>\n",
       "      <td>-1.295479</td>\n",
       "      <td>5.787813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Datum  Basel Temperature [2 m elevation corrected]  \\\n",
       "0 2022-01-01 00:00:00+00:00                                    -0.926467   \n",
       "1 2022-01-01 01:00:00+00:00                                    -1.041967   \n",
       "2 2022-01-01 02:00:00+00:00                                    -1.162435   \n",
       "3 2022-01-01 03:00:00+00:00                                    -1.135112   \n",
       "4 2022-01-01 04:00:00+00:00                                    -1.163677   \n",
       "\n",
       "   Basel Precipitation Total  Basel Wind Speed [10 m]  \\\n",
       "0                  -0.314805                -0.694066   \n",
       "1                  -0.314805                -0.918024   \n",
       "2                  -0.314805                -0.857675   \n",
       "3                  -0.314805                -0.831118   \n",
       "4                  -0.314805                -0.687750   \n",
       "\n",
       "   Basel Wind Direction [10 m]  Stromverbrauch  350n_sumLief  350v_sumLW  \\\n",
       "0                     0.115891       -1.116259     -0.969412   -0.731284   \n",
       "1                     0.047361       -1.368317     -0.910171   -0.809324   \n",
       "2                    -0.143112       -1.523703     -0.969412   -0.731284   \n",
       "3                    -0.006355       -1.546518     -0.880550   -0.809324   \n",
       "4                    -0.148316       -1.432685     -0.910171   -0.887364   \n",
       "\n",
       "   352v_sumPW  352v_sumLief  ...  660v_sumPW  660v_sumLW  660n_sumPW  \\\n",
       "0   -0.253232     -0.810792  ...   -1.540182   -1.300587   -1.679601   \n",
       "1   -0.370929     -0.810792  ...   -1.577611   -1.300587   -1.679601   \n",
       "2   -0.841720     -0.909298  ...   -1.568254   -1.300587   -1.591872   \n",
       "3   -1.004687     -0.712287  ...   -1.399824   -1.198189   -1.143479   \n",
       "4   -1.059009     -0.958550  ...   -0.960036   -0.890998   -0.714582   \n",
       "\n",
       "   660n_sumLW  84111104n_sumLief  84111104v_sumLief  84111108v_sumLief  \\\n",
       "0   -1.288546           2.164977           1.404508          -0.413276   \n",
       "1   -1.288546           1.515830           0.733423          -0.767192   \n",
       "2   -1.288546           0.798352           0.565652          -0.413276   \n",
       "3   -1.076564           0.832517           0.263664          -0.413276   \n",
       "4   -0.864583           0.695855           0.565652          -0.649220   \n",
       "\n",
       "   Gasverbrauch   Traffic  PM10_Combined_Trend_Residual  \n",
       "0      0.294494 -1.142151                     51.472071  \n",
       "1      0.357518 -1.295479                     32.710483  \n",
       "2      0.385414 -1.295479                     24.801767  \n",
       "3      0.617131 -1.295479                      9.683660  \n",
       "4      1.109860 -1.295479                      5.787813  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure 'Datum' column in features_data is in datetime format\n",
    "features_data_cutted['Datum'] = pd.to_datetime(features_data['Datum'])\n",
    "\n",
    "# Merge the dataframes\n",
    "features_data_lagged = pd.merge(features_data_cutted, lagged_target_variable_1h, on=\"Datum\", how=\"left\")\n",
    "\n",
    "features_data_lagged.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models mit Lagged Value trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 17201\n",
      "Validation Size: 2458\n",
      "Test Size: 4915\n"
     ]
    }
   ],
   "source": [
    "# Split our data intro features and targets\n",
    "# Teile das Dataset in Features und Zielvariable\n",
    "y = target_data_cutted[\"PM10_Combined_Trend_Residual\"]  # Zielvariable\n",
    "L = features_data_lagged.drop(columns=[\"Datum\"])  # Alle Spalten außer der Zielvariable\n",
    "\n",
    "X.head(10)\n",
    "\n",
    "# Split into training and test sets\n",
    "#L_train, L_test, y_train, y_test = train_test_split(L, y, random_state=72)\n",
    "\n",
    "# Daten splitten\n",
    "L_train_val, L_test, y_train_val, y_test = train_test_split(L, y, test_size=0.2, random_state=72)\n",
    "L_train, L_val, y_train, y_val = train_test_split(L_train_val, y_train_val, test_size=0.125, random_state=72)  # 10% von Gesamt\n",
    "\n",
    "print(\"Train Size:\", len(L_train))\n",
    "print(\"Validation Size:\", len(L_val))\n",
    "print(\"Test Size:\", len(L_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normaler Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 13.537028539915747\n",
      "Fold 2: MSE = 18.60029993147356\n",
      "Fold 3: MSE = 23.966496569692804\n",
      "Fold 4: MSE = 20.06800177232026\n",
      "Fold 5: MSE = 20.05240215174017\n",
      "Durchschnittlicher MSE über alle Folds: 19.24484579302851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Expanding Cross-Validation (5 Splits)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "L_results = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "L_train_init = L_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(L_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    L_train_fold = L_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    L_val_fold = L_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    L_model = DecisionTreeRegressor()\n",
    "    L_model.fit(L_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict and calculate MSE\n",
    "    L_y_pred = L_model.predict(L_val_fold)\n",
    "    L_mse = mean_squared_error(y_val_fold, L_y_pred)\n",
    "    L_results.append(L_mse)\n",
    "    print(f\"Fold {fold + 1}: MSE = {L_mse}\")\n",
    "\n",
    "# Average MSE across all folds\n",
    "L_average_mse = np.mean(L_results)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {L_average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree mit Optimierung ccp_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 32.23701133051591\n",
      "Fold 2: MSE = 28.836340135052897\n",
      "Fold 3: MSE = 31.77497464437444\n",
      "Fold 4: MSE = 30.954521747456724\n",
      "Fold 5: MSE = 26.823752606536026\n",
      "Durchschnittlicher MSE über alle Folds: 30.1253200927872\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "L_results_ccp = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(L_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    L_train_fold = L_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    L_val_fold = L_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    L_tree_ccp = DecisionTreeRegressor(ccp_alpha=0.01)\n",
    "    L_tree_ccp.fit(L_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    L_y_pred_ccp = L_tree_ccp.predict(L_test)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    L_mse_ccp = mean_squared_error(y_test, L_y_pred_ccp)\n",
    "    L_results_ccp.append(L_mse_ccp)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {L_mse_ccp}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "L_average_mse_ccp = np.mean(L_results_ccp)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {L_average_mse_ccp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 25.39964964410839\n",
      "Fold 2: MSE = 25.552739894438897\n",
      "Fold 3: MSE = 25.799822895678957\n",
      "Fold 4: MSE = 25.208308313002473\n",
      "Fold 5: MSE = 25.356321936708795\n",
      "Durchschnittlicher MSE über alle Folds: 25.463368536787502\n"
     ]
    }
   ],
   "source": [
    "# Import the regression tree from scikit-learn and a plotting helper\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# Import our train_test_split helper\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import the mean_squared_error function under the alias mse\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "# Import the resampling helper\n",
    "from sklearn.utils import resample\n",
    "# Import the sklearn implementation of bagging\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "B = 100\n",
    "\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "L_results_bag = []\n",
    "\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(L_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    L_train_fold = L_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    L_val_fold = L_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    L_bagged_trees = BaggingRegressor(DecisionTreeRegressor(), n_estimators=B, random_state=42)\n",
    "    L_bagged_trees.fit(L_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    L_y_pred = L_bagged_trees.predict(L_test)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    L_mse = mean_squared_error(y_test, L_y_pred)\n",
    "    L_results_bag.append(L_mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {L_mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "L_average_mse_bag = np.mean(L_results_bag)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {L_average_mse_bag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 30.71223336537229\n",
      "Fold 2: MSE = 29.44769652119523\n",
      "Fold 3: MSE = 28.65957232456554\n",
      "Fold 4: MSE = 27.945608916354974\n",
      "Fold 5: MSE = 27.5356900056587\n",
      "Durchschnittlicher MSE über alle Folds: 28.860160226629347\n"
     ]
    }
   ],
   "source": [
    "# Import the random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "L_results_rf = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(L_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    L_train_fold = L_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    L_val_fold = L_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    L_rf = RandomForestRegressor(n_estimators=B, max_features=\"sqrt\", random_state=42)\n",
    "    L_rf.fit(L_train_fold, y_train_fold)\n",
    "\n",
    "    # Vorhersagen machen\n",
    "    L_y_pred = L_rf.predict(L_test)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    L_fold_mse = mean_squared_error(y_test, L_y_pred)\n",
    "    L_results_rf.append(L_fold_mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {L_fold_mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "L_average_mse_rf = np.mean(L_results_rf)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {L_average_mse_rf}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
