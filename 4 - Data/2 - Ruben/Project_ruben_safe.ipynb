{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use a nicer style for plots\n",
    "plt.style.use(\"seaborn-v0_8-muted\")\n",
    "\n",
    "# Import the regression tree from scikit-learn and a plotting helper\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# Import our train_test_split helper\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/Users/rubenstark/Documents/GitHub/Its-Wekk/4 - Data/4 - Johannes/merged_df.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Dataset into target and features and split them into test train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data intro features and targets\n",
    "# Teile das Dataset in Features und Zielvariable\n",
    "y = dataset[\"target_column\"]  # Zielvariable\n",
    "X = dataset.drop(columns=[\"target_column\"])  # Alle Spalten außer der Zielvariable\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Tree and fit it to the train Data\n",
    "\n",
    "\n",
    "\n",
    "## Aufbau des Entscheidungsbaums\n",
    "1. **Feature-Space-Aufteilung**: Der gesamte Merkmalsraum \\( X \\) wird in \\( J \\) Regionen \\( R_1, R_2, \\dots, R_J \\) unterteilt.\n",
    "2. **Vorhersage in den Regionen**: Für jede Region wird der Durchschnitt der Zielvariablen in dieser Region als Vorhersage verwendet.\n",
    "\n",
    "Beispiel: Ein Entscheidungsbaum mit zwei Regionen \\( R_1 \\) und \\( R_2 \\) könnte Vorhersagen von 5 und 12 liefern, je nachdem, in welche Region ein neues Beispiel fällt.\n",
    "\n",
    "---\n",
    "\n",
    "## Aufteilung des Merkmalsraums\n",
    "- **Vorgehen**: Der Merkmalsraum wird mit einem rekursiven, \"top-down\"-Ansatz (greedy) aufgeteilt:\n",
    "  - **Schritt 1**: Für jedes Merkmal \\( X_k \\) werden mögliche Splitpunkte \\( s \\) getestet, die den Raum in zwei Regionen teilen:\n",
    "    \\[\n",
    "    R_1 = \\{x \\mid x_k \\leq s\\}, \\quad R_2 = \\{x \\mid x_k > s\\}\n",
    "    \\]\n",
    "  - **Schritt 2**: Die Aufteilung wird basierend auf einer Loss-Funktion bewertet (z. B. Mean Squared Error).\n",
    "  - **Schritt 3**: Der Splitpunkt \\( s \\) und das Merkmal \\( X_k \\), die die Loss-Funktion minimieren, werden ausgewählt.\n",
    "\n",
    "- **Rekursion**: Dieser Prozess wird wiederholt, indem jede Region weiter unterteilt wird, bis ein Abbruchkriterium erreicht wird (z. B. maximale Baumtiefe oder minimale Anzahl an Datenpunkten in einer Region).\n",
    "\n",
    "---\n",
    "\n",
    "## Ziel\n",
    "Der Baum wird so aufgebaut, dass er die Zielvariable \\( Y \\) in den Regionen optimal vorhersagt, indem er die Varianz minimiert und die Struktur des zugrunde liegenden Datensatzes berücksichtigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor is just another sklearn estimator, we can use it\n",
    "# like we would any other model\n",
    "Decision_tree = DecisionTreeRegressor()\n",
    "Decision_tree.fit(X_train, y_train) #Habebn wir das jetzt irgendwo festgehalten/definiert? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision_tree.tree_.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "plot_tree(Decision_tree, label=\"root\", filled=True, max_depth=1, \n",
    "          feature_names=features, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ccp_alpha (and other parameters) to optimize the Decision Tree for example when it comes to overfitting\n",
    "\n",
    "[`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion='squared_error', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, mintree = DecisionTreeRegressor(_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0, monotonic_cst=None)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crossvalidation to find best alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools for model selection\n",
    "from sklearn.model_selection import cross_validate, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify our CV strategy\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=72)\n",
    "\n",
    "# Specify the values of cost-complexity parameter to try\n",
    "alphas = np.logspace(-4, 0, 200) #erzeugt werte von 10^-4 bis 10^0)\n",
    "\n",
    "scores = [] # Store the cross-validation scores\n",
    "scores_std = [] # Store the standard deviations of the scores\n",
    "\n",
    "# Perform cross-validation\n",
    "for alpha in alphas:\n",
    "    tree_cv = DecisionTreeRegressor(ccp_alpha=alpha)\n",
    "    # Cross validate the tree, \n",
    "    cv_results = cross_validate(tree_cv, X_train, y_train, cv=cv, \n",
    "                                scoring=\"neg_mean_squared_error\")\n",
    "    scores.append(-cv_results[\"test_score\"].mean())\n",
    "    scores_std.append(cv_results[\"test_score\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging\n",
    "\n",
    "Bagging (Bootstrap Aggregating) ist eine Technik, um die Stabilität und Genauigkeit von Machine-Learning-Algorithmen zu verbessern, insbesondere bei Modellen wie Entscheidungsbäumen, die anfällig für hohe Varianz sind. Es basiert auf dem Bootstrapping-Prinzip, bei dem mehrere Trainingssets durch Zufallsstichproben mit Zurücklegen erzeugt werden.\n",
    "\n",
    "Jeder Baum wird auf einem dieser zufälligen Datensets trainiert, und die Vorhersagen der B Modelle werden durch Mittelung kombiniert. Mathematisch reduziert Bagging die Varianz der Modelle, weil unabhängige Fehler über die Modelle hinweg geglättet werden. So wird die Vorhersage insgesamt stabiler und robuster gegen Variationen in den Trainingsdaten.\n",
    "\n",
    "Das Ziel ist, Vorhersagefehler durch Mittelung der Outputs der individuellen Modelle zu minimieren, was insgesamt zu einer besseren Modellleistung führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sklearn implementation of bagging\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# Create a bagged tree estimator with B=100 trees\n",
    "bagged_trees = BaggingRegressor(DecisionTreeRegressor(), n_estimators=B)\n",
    "\n",
    "# Fit the bagged estimator and compute the MSE on the training set\n",
    "bagged_trees.fit(X_train, y_train)\n",
    "\n",
    "# Compute the predictions on the training and test sets\n",
    "y_pred_train_bag = bagged_trees.predict(X_train)\n",
    "y_pred_test_bag = bagged_trees.predict(X_test)\n",
    "\n",
    "print(\"Train MSE: \", mse(y_train, y_pred_train_bag))\n",
    "print(\"Test MSE : \", mse(y_test, y_pred_test_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(n_samples), size=n_samples, replace=True)\n",
    "    X_bootstrap = X2[indices]\n",
    "    Y_bootstrap = Y[indices]\n",
    "    predicted_sales[i] = make_prediction(X_bootstrap, Y_bootstrap, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forrest\n",
    "\n",
    "Random Forests erweitern Bagging, indem sie jedem Baum eine zusätzliche Zufallskomponente hinzufügen. Jeder Baum wird mit einem bootstrap-Sample der Trainingsdaten trainiert, wobei nur ein zufälliger Teil der Features für die Konstruktion des Baums verwendet wird. Dadurch unterscheidet sich Random Forests von klassischem Bagging, bei dem alle Features verfügbar sind.\n",
    "\n",
    "Die zufällige Auswahl der Features reduziert die Korrelation zwischen den Bäumen und verbessert die Generalisierung des Modells. Üblicherweise wird die Anzahl der verwendeten Features  m  so gewählt, dass  m \\approx \\sqrt{p} , wobei  p  die Gesamtzahl der Features ist. Wenn  m = p  gesetzt wird, ist Random Forest gleichbedeutend mit einem Bagging-Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=B, max_features=\"sqrt\")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Compute the predictions on the training and test sets\n",
    "y_pred_train_rf = rf.predict(X_train)\n",
    "y_pred_test_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Train MSE: \", mse(y_train, y_pred_train_rf))\n",
    "print(\"Test MSE : \", mse(y_test, y_pred_test_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Unterschied zwischen der Verwendung eines Integers oder eines Floats bei der Angabe von max_features in einem Random Forest Modell (wie in Scikit-learn) liegt in der Bedeutung des Parameters und wie die Anzahl der maximal zu betrachtenden Features berechnet wird:\n",
    "\n",
    "1. Wenn max_features ein Integer ist:\n",
    "\n",
    "\t•\tDer Wert gibt die exakte Anzahl der maximal zu betrachtenden Features an, die bei der Teilung eines Knotens in jedem Decision Tree berücksichtigt werden sollen.\n",
    "\t•\tBeispiel: max_features=3 bedeutet, dass 3 Features aus dem gesamten Feature-Set zufällig ausgewählt werden, um die beste Teilung zu bestimmen.\n",
    "\n",
    "2. Wenn max_features ein Float ist:\n",
    "\n",
    "\t•\tDer Wert gibt einen Prozentsatz der verfügbaren Features an, die verwendet werden sollen. Der Float-Wert muss zwischen 0.0 und 1.0 liegen.\n",
    "\t•\tBeispiel: max_features=0.5 bedeutet, dass 50 % der Features (aufgerundet) zufällig ausgewählt werden, um die beste Teilung zu bestimmen.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = pd.read_csv(\"/Users/rubenstark/Documents/GitHub/Its-Wekk/4 - Data/Final_Data/Cleaned/10Targcleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Sicherstellen, dass 'Datum' ein datetime-Format hat\n",
    "target_data['Datum'] = pd.to_datetime(target_data['Datum'])\n",
    "target_data.set_index('Datum', inplace=True)\n",
    "\n",
    "# Auswahl der Zeitreihe (PM10 Mittelwerte)\n",
    "series = target_data['PM10 (Stundenmittelwerte)']\n",
    "\n",
    "# 1. Globalen Trend bestimmen und entfernen\n",
    "result_trend = seasonal_decompose(series, model='additive', period=len(series))  # Gesamter Trend\n",
    "trend = result_trend.trend\n",
    "series_detrended = series - trend  # Serie ohne Trend\n",
    "\n",
    "# 2. Wöchentliche Saisonalität entfernen\n",
    "result_weekly = seasonal_decompose(series_detrended, model='additive', period=168)  # Wochen-Saisonalität\n",
    "weekly_seasonal = result_weekly.seasonal\n",
    "series_no_weekly = series_detrended - weekly_seasonal  # Serie ohne wöchentliche Saisonalität\n",
    "\n",
    "# 3. Tägliche Saisonalität entfernen\n",
    "result_daily = seasonal_decompose(series_no_weekly, model='additive', period=24)  # Tages-Saisonalität\n",
    "daily_seasonal = result_daily.seasonal\n",
    "series_residual = series_no_weekly - daily_seasonal  # Serie ohne tägliche Saisonalität\n",
    "\n",
    "# Neues DataFrame mit den Residuen (ohne Trend, Wochen- und Tages-Saisonalität)\n",
    "residual_data = series_residual.reset_index()\n",
    "residual_data.columns = ['Datum', 'PM10_Residual']\n",
    "\n",
    "# NaN-Werte entfernen (falls Trend/Saisonalität die ersten/letzten Werte beeinflussen)\n",
    "residual_data.dropna(inplace=True)\n",
    "\n",
    "# Residuen-Dataset anzeigen\n",
    "print(residual_data.head())\n",
    "\n",
    "# Optional: Als CSV speichern\n",
    "#residual_data.to_csv(\"PM10_Residuals_NoTrend_NoSeasonality.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
