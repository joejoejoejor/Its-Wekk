{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use a nicer style for plots\n",
    "plt.style.use(\"seaborn-v0_8-muted\")\n",
    "\n",
    "# Import the regression tree from scikit-learn and a plotting helper\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# Import our train_test_split helper\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset into target and features and split them into test train Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = pd.read_csv(\"/Users/rubenstark/Documents/GitHub/Its-Wekk/4 - Data/2 - Ruben/Final_Target_Data_Combined_resid_Trend\")\n",
    "\n",
    "#features Dataset muss noch angepasst werden\n",
    "features_data = pd.read_csv(\"/Users/rubenstark/Documents/GitHub/Its-Wekk/Fucking Final Data/50MostImpFeatures_DF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konvertiere die Datumsspalte in einen datetime-Index (falls nicht bereits)\n",
    "target_data['Datum'] = pd.to_datetime(target_data['Datum'])\n",
    "\n",
    "# Definiere das Cut-Off-Datum\n",
    "cutoff_date = pd.Timestamp('2024-10-20 21:00:00+00:00')\n",
    "\n",
    "# Filtere das Dataset auf Einträge bis einschließlich des Cut-Off-Datums\n",
    "target_data_cutted = target_data[target_data['Datum'] <= cutoff_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 17201\n",
      "Validation Size: 2458\n",
      "Test Size: 4915\n"
     ]
    }
   ],
   "source": [
    "# Split our data intro features and targets\n",
    "# Teile das Dataset in Features und Zielvariable\n",
    "y = target_data_cutted[\"PM10_Combined_Trend_Residual\"]  # Zielvariable\n",
    "X = features_data.drop(columns=[\"Datum\"])  # Alle Spalten außer der Zielvariable\n",
    "\n",
    "X.head(10)\n",
    "\n",
    "# Daten splitten\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=72)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=72)  # 10% von Gesamt\n",
    "\n",
    "print(\"Train Size:\", len(X_train))\n",
    "print(\"Validation Size:\", len(X_val))\n",
    "print(\"Test Size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 65.4100379760354\n",
      "Fold 2: MSE = 66.41982425812651\n",
      "Fold 3: MSE = 63.324575397222276\n",
      "Fold 4: MSE = 58.6554521494229\n",
      "Fold 5: MSE = 55.60325914387411\n",
      "Durchschnittlicher MSE über alle Folds: 61.88262978493624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Expanding Cross-Validation (5 Splits)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "results = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict and calculate MSE\n",
    "    y_pred = model.predict(X_val_fold)\n",
    "    mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results.append(mse)\n",
    "    print(f\"Fold {fold + 1}: MSE = {mse}\")\n",
    "\n",
    "# Average MSE across all folds\n",
    "average_mse = np.mean(results)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ccp_alpha (and other parameters) to optimize the Decision Tree for example when it comes to overfitting\n",
    "\n",
    "[`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 64.70226249533177\n",
      "Fold 2: MSE = 63.185604029945665\n",
      "Fold 3: MSE = 58.699155515878665\n",
      "Fold 4: MSE = 54.641620801639704\n",
      "Fold 5: MSE = 48.90482827337562\n",
      "Durchschnittlicher MSE über alle Folds: 58.02669422323429\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_ccp = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Modell trainieren\n",
    "    tree_ccp = DecisionTreeRegressor(ccp_alpha=0.01)\n",
    "    tree_ccp.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred_ccp = tree_ccp.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    mse_ccp = mean_squared_error(y_val_fold, y_pred_ccp)\n",
    "    results_ccp.append(mse_ccp)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {mse_ccp}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_ccp = np.mean(results_ccp)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_ccp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation to find best alpha \n",
    "\n",
    " Geht nicht mit so vielen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools for model selection\n",
    "from sklearn.model_selection import cross_validate, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimales ccp_alpha: 0.10481131341546852\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Definiere die Werte für ccp_alpha (Cost Complexity Pruning)\n",
    "alphas = np.logspace(-4, 0, 50)  # Werte zwischen 10^-4 und 10^0\n",
    "\n",
    "# Initialisiere Cross-Validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=72)\n",
    "\n",
    "# Speicher für Ergebnisse\n",
    "scores = []  # Durchschnittliche MSE für jedes ccp_alpha\n",
    "scores_std = []  # Standardabweichung der Scores für Stabilitätsanalyse\n",
    "\n",
    "# Cross-Validation für jedes ccp_alpha\n",
    "for alpha in alphas:\n",
    "    # Decision Tree mit aktuellem ccp_alpha-Wert\n",
    "    tree_cv = DecisionTreeRegressor(ccp_alpha=alpha, random_state=72)\n",
    "    \n",
    "    # Negative MSE, da cross_val_score maximiert; wir wollen minimieren\n",
    "    mse_scores = cross_val_score(tree_cv, X_train, y_train, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "    \n",
    "    # Durchschnittlichen MSE speichern (negativ, daher multiplizieren mit -1)\n",
    "    scores.append(-mse_scores.mean())\n",
    "    scores_std.append(mse_scores.std())\n",
    "\n",
    "# Optimiere ccp_alpha: Der Wert mit dem niedrigsten MSE\n",
    "optimal_alpha = alphas[np.argmin(scores)]\n",
    "\n",
    "print(f\"Optimales ccp_alpha: {optimal_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 55.5456298158434\n",
      "Fold 2: MSE = 47.179887765873374\n",
      "Fold 3: MSE = 44.651038715518865\n",
      "Fold 4: MSE = 42.46225589862255\n",
      "Fold 5: MSE = 34.31281454209581\n",
      "Durchschnittlicher MSE über alle Folds: 44.8303253475908\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_ccp = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Modell trainieren\n",
    "    tree_ccp = DecisionTreeRegressor(ccp_alpha=optimal_alpha)\n",
    "    tree_ccp.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred_ccp = tree_ccp.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    mse_ccp = mean_squared_error(y_val_fold, y_pred_ccp)\n",
    "    results_ccp.append(mse_ccp)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {mse_ccp}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_ccp = np.mean(results_ccp)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_ccp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging (Bootstrap Aggregating) ist eine Technik, um die Stabilität und Genauigkeit von Machine-Learning-Algorithmen zu verbessern, insbesondere bei Modellen wie Entscheidungsbäumen, die anfällig für hohe Varianz sind. Es basiert auf dem Bootstrapping-Prinzip, bei dem mehrere Trainingssets durch Zufallsstichproben mit Zurücklegen erzeugt werden.\n",
    "\n",
    "Jeder Baum wird auf einem dieser zufälligen Datensets trainiert, und die Vorhersagen der B Modelle werden durch Mittelung kombiniert. Mathematisch reduziert Bagging die Varianz der Modelle, weil unabhängige Fehler über die Modelle hinweg geglättet werden. So wird die Vorhersage insgesamt stabiler und robuster gegen Variationen in den Trainingsdaten.\n",
    "\n",
    "Das Ziel ist, Vorhersagefehler durch Mittelung der Outputs der individuellen Modelle zu minimieren, was insgesamt zu einer besseren Modellleistung führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE:  4.263854031225405\n",
      "Test MSE :  45.717410268505475\n",
      "Fold 1: MSE = 33.63343065789898\n",
      "Fold 2: MSE = 32.90733996857499\n",
      "Fold 3: MSE = 31.39674993946731\n",
      "Fold 4: MSE = 32.85855816153008\n",
      "Fold 5: MSE = 27.303353119061953\n",
      "Durchschnittlicher MSE über alle Folds: 31.619886369306663\n"
     ]
    }
   ],
   "source": [
    "# Import the regression tree from scikit-learn and a plotting helper\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# Import our train_test_split helper\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import the mean_squared_error function under the alias mse\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "# Import the resampling helper\n",
    "from sklearn.utils import resample\n",
    "# Import the sklearn implementation of bagging\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_bag = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "\n",
    "    # Create a bagged tree estimator with B=100 trees\n",
    "    bagged_trees = BaggingRegressor(DecisionTreeRegressor(), n_estimators=100\n",
    "    \n",
    "    # Modell trainieren\n",
    "    bagged_trees.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred = bagged_trees.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results_bag.append(mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_bag = np.mean(results_bag)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_bag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest\n",
    "\n",
    "Random Forests erweitern Bagging, indem sie jedem Baum eine zusätzliche Zufallskomponente hinzufügen. Jeder Baum wird mit einem bootstrap-Sample der Trainingsdaten trainiert, wobei nur ein zufälliger Teil der Features für die Konstruktion des Baums verwendet wird. Dadurch unterscheidet sich Random Forests von klassischem Bagging, bei dem alle Features verfügbar sind.\n",
    "\n",
    "Die zufällige Auswahl der Features reduziert die Korrelation zwischen den Bäumen und verbessert die Generalisierung des Modells. Üblicherweise wird die Anzahl der verwendeten Features  m  so gewählt, dass  m \\approx \\sqrt{p} , wobei  p  die Gesamtzahl der Features ist. Wenn  m = p  gesetzt wird, ist Random Forest gleichbedeutend mit einem Bagging-Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE:  4.515092698542732\n",
      "Test MSE :  46.555903436203614\n",
      "Fold 1: MSE = 32.61206161809249\n",
      "Fold 2: MSE = 33.32146476267613\n",
      "Fold 3: MSE = 32.6669091954285\n",
      "Fold 4: MSE = 33.44130434933117\n",
      "Fold 5: MSE = 29.35676751830648\n",
      "Durchschnittlicher MSE über alle Folds: 32.27970148876695\n"
     ]
    }
   ],
   "source": [
    "# Import the random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_rf = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    X_train_fold, X_val_fold = X_train_init.iloc[train_index], X_train_init.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_init.iloc[train_index], y_train_init.iloc[val_index]\n",
    "\n",
    "    # Initialize the random forest regressor\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_features=\"sqrt\")\n",
    "    \n",
    "    # Modell trainieren\n",
    "    rf.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred = rf.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    fold_mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results_rf.append(fold_mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {fold_mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_rf = np.mean(results_rf)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Unterschied zwischen der Verwendung eines Integers oder eines Floats bei der Angabe von max_features in einem Random Forest Modell (wie in Scikit-learn) liegt in der Bedeutung des Parameters und wie die Anzahl der maximal zu betrachtenden Features berechnet wird:\n",
    "\n",
    "1. Wenn max_features ein Integer ist:\n",
    "\n",
    "\t•\tDer Wert gibt die exakte Anzahl der maximal zu betrachtenden Features an, die bei der Teilung eines Knotens in jedem Decision Tree berücksichtigt werden sollen.\n",
    "\t•\tBeispiel: max_features=3 bedeutet, dass 3 Features aus dem gesamten Feature-Set zufällig ausgewählt werden, um die beste Teilung zu bestimmen.\n",
    "\n",
    "2. Wenn max_features ein Float ist:\n",
    "\n",
    "\t•\tDer Wert gibt einen Prozentsatz der verfügbaren Features an, die verwendet werden sollen. Der Float-Wert muss zwischen 0.0 und 1.0 liegen.\n",
    "\t•\tBeispiel: max_features=0.5 bedeutet, dass 50 % der Features (aufgerundet) zufällig ausgewählt werden, um die beste Teilung zu bestimmen.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagged Variable mit 1h lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6l/nfq1629j4jg8fynztn5kxlph0000gn/T/ipykernel_93768/1408990632.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_data_cutted['Datum'] = pd.to_datetime(target_data_cutted['Datum']).copy()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datum</th>\n",
       "      <th>PM10_Combined_Trend_Residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-31 23:00:00+00:00</td>\n",
       "      <td>75.197962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 00:00:00+00:00</td>\n",
       "      <td>51.472071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 01:00:00+00:00</td>\n",
       "      <td>32.710483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 02:00:00+00:00</td>\n",
       "      <td>24.801767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 03:00:00+00:00</td>\n",
       "      <td>9.683660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Datum  PM10_Combined_Trend_Residual\n",
       "0 2021-12-31 23:00:00+00:00                     75.197962\n",
       "1 2022-01-01 00:00:00+00:00                     51.472071\n",
       "2 2022-01-01 01:00:00+00:00                     32.710483\n",
       "3 2022-01-01 02:00:00+00:00                     24.801767\n",
       "4 2022-01-01 03:00:00+00:00                      9.683660"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure 'Datum' column is in datetime format\n",
    "target_data_cutted['Datum'] = pd.to_datetime(target_data_cutted['Datum']).copy()\n",
    "\n",
    "# Create a copy of target_data to apply the offset\n",
    "lagged_target_variable_1h = target_data.copy()\n",
    "\n",
    "\n",
    "# Offset von -1 Stunde anwenden\n",
    "lagged_target_variable_1h['Datum'] = target_data['Datum'] + pd.Timedelta(hours=-1)\n",
    "\n",
    "\n",
    "\n",
    "lagged_target_variable_1h.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datum</th>\n",
       "      <th>Basel Temperature [2 m elevation corrected]</th>\n",
       "      <th>Basel Precipitation Total</th>\n",
       "      <th>Basel Wind Speed [10 m]</th>\n",
       "      <th>Basel Wind Direction [10 m]</th>\n",
       "      <th>Stromverbrauch</th>\n",
       "      <th>350n_sumLief</th>\n",
       "      <th>350v_sumLW</th>\n",
       "      <th>352v_sumPW</th>\n",
       "      <th>352v_sumLief</th>\n",
       "      <th>...</th>\n",
       "      <th>660v_sumPW</th>\n",
       "      <th>660v_sumLW</th>\n",
       "      <th>660n_sumPW</th>\n",
       "      <th>660n_sumLW</th>\n",
       "      <th>84111104n_sumLief</th>\n",
       "      <th>84111104v_sumLief</th>\n",
       "      <th>84111108v_sumLief</th>\n",
       "      <th>Gasverbrauch</th>\n",
       "      <th>Traffic</th>\n",
       "      <th>PM10_Combined_Trend_Residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01 00:00:00+00:00</td>\n",
       "      <td>-0.926467</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.694066</td>\n",
       "      <td>0.115891</td>\n",
       "      <td>-1.116259</td>\n",
       "      <td>-0.969412</td>\n",
       "      <td>-0.731284</td>\n",
       "      <td>-0.253232</td>\n",
       "      <td>-0.810792</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.540182</td>\n",
       "      <td>-1.300587</td>\n",
       "      <td>-1.679601</td>\n",
       "      <td>-1.288546</td>\n",
       "      <td>2.164977</td>\n",
       "      <td>1.404508</td>\n",
       "      <td>-0.413276</td>\n",
       "      <td>0.294494</td>\n",
       "      <td>-1.142151</td>\n",
       "      <td>51.472071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 01:00:00+00:00</td>\n",
       "      <td>-1.041967</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.918024</td>\n",
       "      <td>0.047361</td>\n",
       "      <td>-1.368317</td>\n",
       "      <td>-0.910171</td>\n",
       "      <td>-0.809324</td>\n",
       "      <td>-0.370929</td>\n",
       "      <td>-0.810792</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.577611</td>\n",
       "      <td>-1.300587</td>\n",
       "      <td>-1.679601</td>\n",
       "      <td>-1.288546</td>\n",
       "      <td>1.515830</td>\n",
       "      <td>0.733423</td>\n",
       "      <td>-0.767192</td>\n",
       "      <td>0.357518</td>\n",
       "      <td>-1.295479</td>\n",
       "      <td>32.710483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 02:00:00+00:00</td>\n",
       "      <td>-1.162435</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.857675</td>\n",
       "      <td>-0.143112</td>\n",
       "      <td>-1.523703</td>\n",
       "      <td>-0.969412</td>\n",
       "      <td>-0.731284</td>\n",
       "      <td>-0.841720</td>\n",
       "      <td>-0.909298</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.568254</td>\n",
       "      <td>-1.300587</td>\n",
       "      <td>-1.591872</td>\n",
       "      <td>-1.288546</td>\n",
       "      <td>0.798352</td>\n",
       "      <td>0.565652</td>\n",
       "      <td>-0.413276</td>\n",
       "      <td>0.385414</td>\n",
       "      <td>-1.295479</td>\n",
       "      <td>24.801767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 03:00:00+00:00</td>\n",
       "      <td>-1.135112</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.831118</td>\n",
       "      <td>-0.006355</td>\n",
       "      <td>-1.546518</td>\n",
       "      <td>-0.880550</td>\n",
       "      <td>-0.809324</td>\n",
       "      <td>-1.004687</td>\n",
       "      <td>-0.712287</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.399824</td>\n",
       "      <td>-1.198189</td>\n",
       "      <td>-1.143479</td>\n",
       "      <td>-1.076564</td>\n",
       "      <td>0.832517</td>\n",
       "      <td>0.263664</td>\n",
       "      <td>-0.413276</td>\n",
       "      <td>0.617131</td>\n",
       "      <td>-1.295479</td>\n",
       "      <td>9.683660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 04:00:00+00:00</td>\n",
       "      <td>-1.163677</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.687750</td>\n",
       "      <td>-0.148316</td>\n",
       "      <td>-1.432685</td>\n",
       "      <td>-0.910171</td>\n",
       "      <td>-0.887364</td>\n",
       "      <td>-1.059009</td>\n",
       "      <td>-0.958550</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.960036</td>\n",
       "      <td>-0.890998</td>\n",
       "      <td>-0.714582</td>\n",
       "      <td>-0.864583</td>\n",
       "      <td>0.695855</td>\n",
       "      <td>0.565652</td>\n",
       "      <td>-0.649220</td>\n",
       "      <td>1.109860</td>\n",
       "      <td>-1.295479</td>\n",
       "      <td>5.787813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Datum  Basel Temperature [2 m elevation corrected]  \\\n",
       "0 2022-01-01 00:00:00+00:00                                    -0.926467   \n",
       "1 2022-01-01 01:00:00+00:00                                    -1.041967   \n",
       "2 2022-01-01 02:00:00+00:00                                    -1.162435   \n",
       "3 2022-01-01 03:00:00+00:00                                    -1.135112   \n",
       "4 2022-01-01 04:00:00+00:00                                    -1.163677   \n",
       "\n",
       "   Basel Precipitation Total  Basel Wind Speed [10 m]  \\\n",
       "0                  -0.314805                -0.694066   \n",
       "1                  -0.314805                -0.918024   \n",
       "2                  -0.314805                -0.857675   \n",
       "3                  -0.314805                -0.831118   \n",
       "4                  -0.314805                -0.687750   \n",
       "\n",
       "   Basel Wind Direction [10 m]  Stromverbrauch  350n_sumLief  350v_sumLW  \\\n",
       "0                     0.115891       -1.116259     -0.969412   -0.731284   \n",
       "1                     0.047361       -1.368317     -0.910171   -0.809324   \n",
       "2                    -0.143112       -1.523703     -0.969412   -0.731284   \n",
       "3                    -0.006355       -1.546518     -0.880550   -0.809324   \n",
       "4                    -0.148316       -1.432685     -0.910171   -0.887364   \n",
       "\n",
       "   352v_sumPW  352v_sumLief  ...  660v_sumPW  660v_sumLW  660n_sumPW  \\\n",
       "0   -0.253232     -0.810792  ...   -1.540182   -1.300587   -1.679601   \n",
       "1   -0.370929     -0.810792  ...   -1.577611   -1.300587   -1.679601   \n",
       "2   -0.841720     -0.909298  ...   -1.568254   -1.300587   -1.591872   \n",
       "3   -1.004687     -0.712287  ...   -1.399824   -1.198189   -1.143479   \n",
       "4   -1.059009     -0.958550  ...   -0.960036   -0.890998   -0.714582   \n",
       "\n",
       "   660n_sumLW  84111104n_sumLief  84111104v_sumLief  84111108v_sumLief  \\\n",
       "0   -1.288546           2.164977           1.404508          -0.413276   \n",
       "1   -1.288546           1.515830           0.733423          -0.767192   \n",
       "2   -1.288546           0.798352           0.565652          -0.413276   \n",
       "3   -1.076564           0.832517           0.263664          -0.413276   \n",
       "4   -0.864583           0.695855           0.565652          -0.649220   \n",
       "\n",
       "   Gasverbrauch   Traffic  PM10_Combined_Trend_Residual  \n",
       "0      0.294494 -1.142151                     51.472071  \n",
       "1      0.357518 -1.295479                     32.710483  \n",
       "2      0.385414 -1.295479                     24.801767  \n",
       "3      0.617131 -1.295479                      9.683660  \n",
       "4      1.109860 -1.295479                      5.787813  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure 'Datum' column in features_data is in datetime format\n",
    "features_data['Datum'] = pd.to_datetime(features_data['Datum'])\n",
    "\n",
    "# Merge the dataframes\n",
    "features_data_lagged = pd.merge(features_data, lagged_target_variable_1h, on=\"Datum\", how=\"left\")\n",
    "\n",
    "features_data_lagged.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models mit Lagged Value trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 17201\n",
      "Validation Size: 2458\n",
      "Test Size: 4915\n"
     ]
    }
   ],
   "source": [
    "# Split our data intro features and targets\n",
    "# Teile das Dataset in Features und Zielvariable\n",
    "y = target_data_cutted[\"PM10_Combined_Trend_Residual\"]  # Zielvariable\n",
    "L = features_data_lagged.drop(columns=[\"Datum\"])  # Alle Spalten außer der Zielvariable\n",
    "\n",
    "X.head(10)\n",
    "\n",
    "# Split into training and test sets\n",
    "#L_train, L_test, y_train, y_test = train_test_split(L, y, random_state=72)\n",
    "\n",
    "# Daten splitten\n",
    "L_train_val, L_test, y_train_val, y_test = train_test_split(L, y, test_size=0.2, random_state=72)\n",
    "L_train, L_val, y_train, y_val = train_test_split(L_train_val, y_train_val, test_size=0.125, random_state=72)  # 10% von Gesamt\n",
    "\n",
    "print(\"Train Size:\", len(L_train))\n",
    "print(\"Validation Size:\", len(L_val))\n",
    "print(\"Test Size:\", len(L_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normaler Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 13.537028539915747\n",
      "Fold 2: MSE = 18.60029993147356\n",
      "Fold 3: MSE = 23.966496569692804\n",
      "Fold 4: MSE = 20.06800177232026\n",
      "Fold 5: MSE = 20.05240215174017\n",
      "Durchschnittlicher MSE über alle Folds: 19.24484579302851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Expanding Cross-Validation (5 Splits)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "L_results = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "L_train_init = L_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(L_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    L_train_fold = L_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    L_val_fold = L_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    L_model = DecisionTreeRegressor()\n",
    "    L_model.fit(L_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict and calculate MSE\n",
    "    L_y_pred = L_model.predict(L_val_fold)\n",
    "    L_mse = mean_squared_error(y_val_fold, L_y_pred)\n",
    "    L_results.append(L_mse)\n",
    "    print(f\"Fold {fold + 1}: MSE = {L_mse}\")\n",
    "\n",
    "# Average MSE across all folds\n",
    "L_average_mse = np.mean(L_results)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {L_average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree mit Optimierung ccp_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 32.23701133051591\n",
      "Fold 2: MSE = 28.836340135052897\n",
      "Fold 3: MSE = 31.77497464437444\n",
      "Fold 4: MSE = 30.954521747456724\n",
      "Fold 5: MSE = 26.823752606536026\n",
      "Durchschnittlicher MSE über alle Folds: 30.1253200927872\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "L_results_ccp = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(L_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    L_train_fold = L_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    L_val_fold = L_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    L_tree_ccp = DecisionTreeRegressor(ccp_alpha=0.01)\n",
    "    L_tree_ccp.fit(L_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    L_y_pred_ccp = L_tree_ccp.predict(L_test)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    L_mse_ccp = mean_squared_error(y_test, L_y_pred_ccp)\n",
    "    L_results_ccp.append(L_mse_ccp)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {L_mse_ccp}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "L_average_mse_ccp = np.mean(L_results_ccp)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {L_average_mse_ccp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 25.39964964410839\n",
      "Fold 2: MSE = 25.552739894438897\n",
      "Fold 3: MSE = 25.799822895678957\n",
      "Fold 4: MSE = 25.208308313002473\n",
      "Fold 5: MSE = 25.356321936708795\n",
      "Durchschnittlicher MSE über alle Folds: 25.463368536787502\n"
     ]
    }
   ],
   "source": [
    "# Import the regression tree from scikit-learn and a plotting helper\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# Import our train_test_split helper\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import the mean_squared_error function under the alias mse\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "# Import the resampling helper\n",
    "from sklearn.utils import resample\n",
    "# Import the sklearn implementation of bagging\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "B = 100\n",
    "\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "L_results_bag = []\n",
    "\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(L_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    L_train_fold = L_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    L_val_fold = L_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    L_bagged_trees = BaggingRegressor(DecisionTreeRegressor(), n_estimators=B, random_state=42)\n",
    "    L_bagged_trees.fit(L_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    L_y_pred = L_bagged_trees.predict(L_test)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    L_mse = mean_squared_error(y_test, L_y_pred)\n",
    "    L_results_bag.append(L_mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {L_mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "L_average_mse_bag = np.mean(L_results_bag)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {L_average_mse_bag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 30.71223336537229\n",
      "Fold 2: MSE = 29.44769652119523\n",
      "Fold 3: MSE = 28.65957232456554\n",
      "Fold 4: MSE = 27.945608916354974\n",
      "Fold 5: MSE = 27.5356900056587\n",
      "Durchschnittlicher MSE über alle Folds: 28.860160226629347\n"
     ]
    }
   ],
   "source": [
    "# Import the random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "L_results_rf = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(L_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    L_train_fold = L_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    L_val_fold = L_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    L_rf = RandomForestRegressor(n_estimators=B, max_features=\"sqrt\", random_state=42)\n",
    "    L_rf.fit(L_train_fold, y_train_fold)\n",
    "\n",
    "    # Vorhersagen machen\n",
    "    L_y_pred = L_rf.predict(L_test)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    L_fold_mse = mean_squared_error(y_test, L_y_pred)\n",
    "    L_results_rf.append(L_fold_mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {L_fold_mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "L_average_mse_rf = np.mean(L_results_rf)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {L_average_mse_rf}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
