{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use a nicer style for plots\n",
    "plt.style.use(\"seaborn-v0_8-muted\")\n",
    "\n",
    "# Import the regression tree from scikit-learn and a plotting helper\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# Import our train_test_split helper\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset into target and features and split them into test train Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datum</th>\n",
       "      <th>pm10_stundenmittelwerte_ug_m3</th>\n",
       "      <th>pm2_5_stundenmittelwerte_ug_m3</th>\n",
       "      <th>PM10 [ug/m3]</th>\n",
       "      <th>PM2.5 [ug/m3]</th>\n",
       "      <th>NO2 [ug/m3]</th>\n",
       "      <th>NOX [ug/m3 eq. NO2]</th>\n",
       "      <th>no2_stundenmittelwerte_ug_m3</th>\n",
       "      <th>CPC [1/cm3]</th>\n",
       "      <th>Basel Wind Direction [10 m]</th>\n",
       "      <th>...</th>\n",
       "      <th>Basel Wind Speed [10 m]_x_Hour</th>\n",
       "      <th>Basel Wind Direction [10 m]_x_Basel Wind Direction [10 m]</th>\n",
       "      <th>Basel Wind Direction [10 m]_x_Hour</th>\n",
       "      <th>406v_sumLW_x_408n_sumLief</th>\n",
       "      <th>660n_sumPW_x_Gasverbrauch</th>\n",
       "      <th>Gasverbrauch_x_Gasverbrauch</th>\n",
       "      <th>Gasverbrauch_x_Hour</th>\n",
       "      <th>PM10_1h_lag</th>\n",
       "      <th>PM10_2h_lag</th>\n",
       "      <th>PM10_24h_lag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24539</th>\n",
       "      <td>2024-10-19 11:00:00+00:00</td>\n",
       "      <td>-0.737744</td>\n",
       "      <td>-0.749346</td>\n",
       "      <td>-0.346081</td>\n",
       "      <td>-0.320055</td>\n",
       "      <td>-0.781123</td>\n",
       "      <td>-0.595912</td>\n",
       "      <td>-1.023309</td>\n",
       "      <td>-0.808688</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193149</td>\n",
       "      <td>-0.891299</td>\n",
       "      <td>0.077697</td>\n",
       "      <td>-0.563230</td>\n",
       "      <td>-1.090158</td>\n",
       "      <td>-0.362200</td>\n",
       "      <td>-0.592623</td>\n",
       "      <td>-0.532504</td>\n",
       "      <td>-0.297281</td>\n",
       "      <td>-0.371916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24540</th>\n",
       "      <td>2024-10-19 12:00:00+00:00</td>\n",
       "      <td>-0.837208</td>\n",
       "      <td>-0.918920</td>\n",
       "      <td>-0.539055</td>\n",
       "      <td>-0.491971</td>\n",
       "      <td>-0.853593</td>\n",
       "      <td>-0.689768</td>\n",
       "      <td>-0.940082</td>\n",
       "      <td>-1.007878</td>\n",
       "      <td>0.287639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.607000</td>\n",
       "      <td>-0.822287</td>\n",
       "      <td>0.255062</td>\n",
       "      <td>-0.560406</td>\n",
       "      <td>-0.253743</td>\n",
       "      <td>-0.339461</td>\n",
       "      <td>-0.670354</td>\n",
       "      <td>-0.576585</td>\n",
       "      <td>-0.532492</td>\n",
       "      <td>-0.477674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24541</th>\n",
       "      <td>2024-10-19 13:00:00+00:00</td>\n",
       "      <td>-0.872483</td>\n",
       "      <td>-0.992216</td>\n",
       "      <td>-0.686624</td>\n",
       "      <td>-0.606582</td>\n",
       "      <td>-0.917004</td>\n",
       "      <td>-0.764852</td>\n",
       "      <td>-0.877662</td>\n",
       "      <td>-1.059252</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.775058</td>\n",
       "      <td>-0.878230</td>\n",
       "      <td>0.144771</td>\n",
       "      <td>-0.514032</td>\n",
       "      <td>-0.358548</td>\n",
       "      <td>-0.329900</td>\n",
       "      <td>-0.736084</td>\n",
       "      <td>-0.609584</td>\n",
       "      <td>-0.576573</td>\n",
       "      <td>-0.502646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24542</th>\n",
       "      <td>2024-10-19 14:00:00+00:00</td>\n",
       "      <td>-0.685577</td>\n",
       "      <td>-0.946253</td>\n",
       "      <td>-0.618515</td>\n",
       "      <td>-0.692540</td>\n",
       "      <td>-0.926063</td>\n",
       "      <td>-0.783623</td>\n",
       "      <td>-0.606067</td>\n",
       "      <td>-1.060710</td>\n",
       "      <td>-0.122968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.665063</td>\n",
       "      <td>-0.892194</td>\n",
       "      <td>-0.137979</td>\n",
       "      <td>-0.438105</td>\n",
       "      <td>-0.545229</td>\n",
       "      <td>-0.385836</td>\n",
       "      <td>-0.719965</td>\n",
       "      <td>-0.593864</td>\n",
       "      <td>-0.609572</td>\n",
       "      <td>-0.423335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24543</th>\n",
       "      <td>2024-10-19 15:00:00+00:00</td>\n",
       "      <td>-0.716082</td>\n",
       "      <td>-0.798709</td>\n",
       "      <td>-0.527703</td>\n",
       "      <td>-0.678213</td>\n",
       "      <td>-0.898887</td>\n",
       "      <td>-0.764852</td>\n",
       "      <td>-0.395143</td>\n",
       "      <td>-0.845031</td>\n",
       "      <td>-0.339880</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.008301</td>\n",
       "      <td>-0.806183</td>\n",
       "      <td>-0.391169</td>\n",
       "      <td>-0.468502</td>\n",
       "      <td>-0.966932</td>\n",
       "      <td>-0.468171</td>\n",
       "      <td>-0.642886</td>\n",
       "      <td>-0.554180</td>\n",
       "      <td>-0.593852</td>\n",
       "      <td>-0.095974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24544</th>\n",
       "      <td>2024-10-19 16:00:00+00:00</td>\n",
       "      <td>-0.404773</td>\n",
       "      <td>-0.469487</td>\n",
       "      <td>-0.482298</td>\n",
       "      <td>-0.577929</td>\n",
       "      <td>-0.609008</td>\n",
       "      <td>-0.558370</td>\n",
       "      <td>0.359611</td>\n",
       "      <td>-0.245979</td>\n",
       "      <td>-0.273198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.914987</td>\n",
       "      <td>-0.841672</td>\n",
       "      <td>-0.334219</td>\n",
       "      <td>-0.330645</td>\n",
       "      <td>-0.744906</td>\n",
       "      <td>-0.527143</td>\n",
       "      <td>-0.570088</td>\n",
       "      <td>-0.223545</td>\n",
       "      <td>-0.554168</td>\n",
       "      <td>-0.210796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24545</th>\n",
       "      <td>2024-10-19 17:00:00+00:00</td>\n",
       "      <td>0.023787</td>\n",
       "      <td>-0.108854</td>\n",
       "      <td>-0.266621</td>\n",
       "      <td>-0.305729</td>\n",
       "      <td>0.151925</td>\n",
       "      <td>-0.039036</td>\n",
       "      <td>0.885725</td>\n",
       "      <td>0.452545</td>\n",
       "      <td>-0.209278</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.162907</td>\n",
       "      <td>-0.868151</td>\n",
       "      <td>-0.271354</td>\n",
       "      <td>-0.122155</td>\n",
       "      <td>0.167454</td>\n",
       "      <td>-0.522496</td>\n",
       "      <td>-0.616499</td>\n",
       "      <td>0.336329</td>\n",
       "      <td>-0.223532</td>\n",
       "      <td>-0.203787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24546</th>\n",
       "      <td>2024-10-19 18:00:00+00:00</td>\n",
       "      <td>-0.000954</td>\n",
       "      <td>-0.103414</td>\n",
       "      <td>-0.005538</td>\n",
       "      <td>-0.076508</td>\n",
       "      <td>0.622978</td>\n",
       "      <td>0.286330</td>\n",
       "      <td>0.765664</td>\n",
       "      <td>1.144091</td>\n",
       "      <td>-0.031366</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.341916</td>\n",
       "      <td>-0.902991</td>\n",
       "      <td>-0.046348</td>\n",
       "      <td>-0.192074</td>\n",
       "      <td>0.736670</td>\n",
       "      <td>-0.489125</td>\n",
       "      <td>-0.727713</td>\n",
       "      <td>0.408429</td>\n",
       "      <td>0.336343</td>\n",
       "      <td>-0.222235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24547</th>\n",
       "      <td>2024-10-19 19:00:00+00:00</td>\n",
       "      <td>-0.077068</td>\n",
       "      <td>-0.202140</td>\n",
       "      <td>0.244194</td>\n",
       "      <td>0.152713</td>\n",
       "      <td>0.885681</td>\n",
       "      <td>0.480299</td>\n",
       "      <td>-0.014386</td>\n",
       "      <td>1.107367</td>\n",
       "      <td>-0.076264</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.270318</td>\n",
       "      <td>-0.899593</td>\n",
       "      <td>-0.111200</td>\n",
       "      <td>-0.052237</td>\n",
       "      <td>0.230290</td>\n",
       "      <td>-0.375189</td>\n",
       "      <td>-0.992467</td>\n",
       "      <td>0.507488</td>\n",
       "      <td>0.408443</td>\n",
       "      <td>-0.232518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24548</th>\n",
       "      <td>2024-10-19 20:00:00+00:00</td>\n",
       "      <td>-0.058586</td>\n",
       "      <td>-0.250823</td>\n",
       "      <td>0.210139</td>\n",
       "      <td>0.210018</td>\n",
       "      <td>0.478039</td>\n",
       "      <td>0.223760</td>\n",
       "      <td>-0.046965</td>\n",
       "      <td>0.870196</td>\n",
       "      <td>-0.163207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.865419</td>\n",
       "      <td>-0.882659</td>\n",
       "      <td>-0.244816</td>\n",
       "      <td>-0.315090</td>\n",
       "      <td>0.104526</td>\n",
       "      <td>-0.256195</td>\n",
       "      <td>-1.244850</td>\n",
       "      <td>0.342302</td>\n",
       "      <td>0.507503</td>\n",
       "      <td>-0.010410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Datum  pm10_stundenmittelwerte_ug_m3  \\\n",
       "24539  2024-10-19 11:00:00+00:00                      -0.737744   \n",
       "24540  2024-10-19 12:00:00+00:00                      -0.837208   \n",
       "24541  2024-10-19 13:00:00+00:00                      -0.872483   \n",
       "24542  2024-10-19 14:00:00+00:00                      -0.685577   \n",
       "24543  2024-10-19 15:00:00+00:00                      -0.716082   \n",
       "24544  2024-10-19 16:00:00+00:00                      -0.404773   \n",
       "24545  2024-10-19 17:00:00+00:00                       0.023787   \n",
       "24546  2024-10-19 18:00:00+00:00                      -0.000954   \n",
       "24547  2024-10-19 19:00:00+00:00                      -0.077068   \n",
       "24548  2024-10-19 20:00:00+00:00                      -0.058586   \n",
       "\n",
       "       pm2_5_stundenmittelwerte_ug_m3  PM10 [ug/m3]  PM2.5 [ug/m3]  \\\n",
       "24539                       -0.749346     -0.346081      -0.320055   \n",
       "24540                       -0.918920     -0.539055      -0.491971   \n",
       "24541                       -0.992216     -0.686624      -0.606582   \n",
       "24542                       -0.946253     -0.618515      -0.692540   \n",
       "24543                       -0.798709     -0.527703      -0.678213   \n",
       "24544                       -0.469487     -0.482298      -0.577929   \n",
       "24545                       -0.108854     -0.266621      -0.305729   \n",
       "24546                       -0.103414     -0.005538      -0.076508   \n",
       "24547                       -0.202140      0.244194       0.152713   \n",
       "24548                       -0.250823      0.210139       0.210018   \n",
       "\n",
       "       NO2 [ug/m3]  NOX [ug/m3 eq. NO2]  no2_stundenmittelwerte_ug_m3  \\\n",
       "24539    -0.781123            -0.595912                     -1.023309   \n",
       "24540    -0.853593            -0.689768                     -0.940082   \n",
       "24541    -0.917004            -0.764852                     -0.877662   \n",
       "24542    -0.926063            -0.783623                     -0.606067   \n",
       "24543    -0.898887            -0.764852                     -0.395143   \n",
       "24544    -0.609008            -0.558370                      0.359611   \n",
       "24545     0.151925            -0.039036                      0.885725   \n",
       "24546     0.622978             0.286330                      0.765664   \n",
       "24547     0.885681             0.480299                     -0.014386   \n",
       "24548     0.478039             0.223760                     -0.046965   \n",
       "\n",
       "       CPC [1/cm3]  Basel Wind Direction [10 m]  ...  \\\n",
       "24539    -0.808688                     0.103516  ...   \n",
       "24540    -1.007878                     0.287639  ...   \n",
       "24541    -1.059252                     0.154800  ...   \n",
       "24542    -1.060710                    -0.122968  ...   \n",
       "24543    -0.845031                    -0.339880  ...   \n",
       "24544    -0.245979                    -0.273198  ...   \n",
       "24545     0.452545                    -0.209278  ...   \n",
       "24546     1.144091                    -0.031366  ...   \n",
       "24547     1.107367                    -0.076264  ...   \n",
       "24548     0.870196                    -0.163207  ...   \n",
       "\n",
       "       Basel Wind Speed [10 m]_x_Hour  \\\n",
       "24539                       -0.193149   \n",
       "24540                       -0.607000   \n",
       "24541                       -0.775058   \n",
       "24542                       -0.665063   \n",
       "24543                       -1.008301   \n",
       "24544                       -0.914987   \n",
       "24545                       -1.162907   \n",
       "24546                       -1.341916   \n",
       "24547                       -1.270318   \n",
       "24548                       -0.865419   \n",
       "\n",
       "       Basel Wind Direction [10 m]_x_Basel Wind Direction [10 m]  \\\n",
       "24539                                          -0.891299           \n",
       "24540                                          -0.822287           \n",
       "24541                                          -0.878230           \n",
       "24542                                          -0.892194           \n",
       "24543                                          -0.806183           \n",
       "24544                                          -0.841672           \n",
       "24545                                          -0.868151           \n",
       "24546                                          -0.902991           \n",
       "24547                                          -0.899593           \n",
       "24548                                          -0.882659           \n",
       "\n",
       "       Basel Wind Direction [10 m]_x_Hour  406v_sumLW_x_408n_sumLief  \\\n",
       "24539                            0.077697                  -0.563230   \n",
       "24540                            0.255062                  -0.560406   \n",
       "24541                            0.144771                  -0.514032   \n",
       "24542                           -0.137979                  -0.438105   \n",
       "24543                           -0.391169                  -0.468502   \n",
       "24544                           -0.334219                  -0.330645   \n",
       "24545                           -0.271354                  -0.122155   \n",
       "24546                           -0.046348                  -0.192074   \n",
       "24547                           -0.111200                  -0.052237   \n",
       "24548                           -0.244816                  -0.315090   \n",
       "\n",
       "       660n_sumPW_x_Gasverbrauch  Gasverbrauch_x_Gasverbrauch  \\\n",
       "24539                  -1.090158                    -0.362200   \n",
       "24540                  -0.253743                    -0.339461   \n",
       "24541                  -0.358548                    -0.329900   \n",
       "24542                  -0.545229                    -0.385836   \n",
       "24543                  -0.966932                    -0.468171   \n",
       "24544                  -0.744906                    -0.527143   \n",
       "24545                   0.167454                    -0.522496   \n",
       "24546                   0.736670                    -0.489125   \n",
       "24547                   0.230290                    -0.375189   \n",
       "24548                   0.104526                    -0.256195   \n",
       "\n",
       "       Gasverbrauch_x_Hour  PM10_1h_lag  PM10_2h_lag  PM10_24h_lag  \n",
       "24539            -0.592623    -0.532504    -0.297281     -0.371916  \n",
       "24540            -0.670354    -0.576585    -0.532492     -0.477674  \n",
       "24541            -0.736084    -0.609584    -0.576573     -0.502646  \n",
       "24542            -0.719965    -0.593864    -0.609572     -0.423335  \n",
       "24543            -0.642886    -0.554180    -0.593852     -0.095974  \n",
       "24544            -0.570088    -0.223545    -0.554168     -0.210796  \n",
       "24545            -0.616499     0.336329    -0.223532     -0.203787  \n",
       "24546            -0.727713     0.408429     0.336343     -0.222235  \n",
       "24547            -0.992467     0.507488     0.408443     -0.232518  \n",
       "24548            -1.244850     0.342302     0.507503     -0.010410  \n",
       "\n",
       "[10 rows x 70 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "features_data = pd.read_csv('../../../4 - Data/04_WorkingDatasets/Top50CombLagged/50CombLagged.csv') #Top 50 Feature + comination Features without Lagged Target\n",
    "target_data = pd.read_csv('../../../4 - Data/04_WorkingDatasets/Top50CombLagged/TargetOutliersTreated.csv')\n",
    "\n",
    "features_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 17184\n",
      "Validation Size: 2455\n",
      "Test Size: 4910\n"
     ]
    }
   ],
   "source": [
    "# Konvertiere die Datumsspalte in einen datetime-Index (falls nicht bereits)\n",
    "target_data['Datum'] = pd.to_datetime(target_data['Datum'])\n",
    "\n",
    "# Definiere das Cut-Off-Datum\n",
    "#cutoff_date = pd.Timestamp('2024-10-20 21:00:00+00:00')\n",
    "\n",
    "# Filtere das Dataset auf Einträge bis einschließlich des Cut-Off-Datums\n",
    "#target_data_cutted = target_data[target_data['Datum'] <= cutoff_date]\n",
    "\n",
    "\n",
    "# Split our data intro features and targets\n",
    "# Teile das Dataset in Features und Zielvariable\n",
    "y = target_data_cutted[\"PM10_Combined_Trend_Residual\"]  # Zielvariable\n",
    "X = features_data.drop(columns=[\"Datum\"])  # Alle Spalten außer der Zielvariable\n",
    "\n",
    "X.head(10)\n",
    "\n",
    "# Daten splitten\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=11) # letter K in Alphabet\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=11)  # 10% von Gesamt  # letter K in Alphabet\n",
    "\n",
    "print(\"Train Size:\", len(X_train))\n",
    "print(\"Validation Size:\", len(X_val))\n",
    "print(\"Test Size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 19.489655958010612\n",
      "Fold 2: MSE = 13.999832423566717\n",
      "Fold 3: MSE = 9.96722939149452\n",
      "Fold 4: MSE = 11.210610127366563\n",
      "Fold 5: MSE = 9.874101244545018\n",
      "Durchschnittlicher MSE über alle Folds: 12.908285828996688\n",
      "Test MSE: 13.81389705108869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "# Expanding Cross-Validation (5 Splits)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "results = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict and calculate MSE\n",
    "    y_pred = model.predict(X_val_fold)\n",
    "    mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results.append(mse)\n",
    "    print(f\"Fold {fold + 1}: MSE = {mse}\")\n",
    "\n",
    "# Average MSE across all folds\n",
    "average_mse = np.mean(results)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the MSE for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14319"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tree_.node_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ccp_alpha (and other parameters) to optimize the Decision Tree for example when it comes to overfitting\n",
    "\n",
    "[`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation to find best alpha \n",
    "\n",
    " Geht nicht mit so vielen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools for model selection\n",
    "from sklearn.model_selection import cross_validate, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimales ccp_alpha: 0.033932217718953266\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "# Definiere die Werte für ccp_alpha (Cost Complexity Pruning)\n",
    "alphas = np.logspace(-4, 0, 50)  # Werte zwischen 10^-4 und 10^0\n",
    "\n",
    "# Initialisiere Cross-Validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=11)\n",
    "\n",
    "# Speicher für Ergebnisse\n",
    "scores = []  # Durchschnittliche MSE für jedes ccp_alpha\n",
    "scores_std = []  # Standardabweichung der Scores für Stabilitätsanalyse\n",
    "\n",
    "# Cross-Validation für jedes ccp_alpha\n",
    "for alpha in alphas:\n",
    "    # Decision Tree mit aktuellem ccp_alpha-Wert\n",
    "    tree_cv = DecisionTreeRegressor(ccp_alpha=alpha, random_state=11)\n",
    "    \n",
    "    # Negative MSE, da cross_val_score maximiert; wir wollen minimieren\n",
    "    mse_scores = cross_val_score(tree_cv, X_train, y_train, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "    \n",
    "    # Durchschnittlichen MSE speichern (negativ, daher multiplizieren mit -1)\n",
    "    scores.append(-mse_scores.mean())\n",
    "    scores_std.append(mse_scores.std())\n",
    "\n",
    "# Optimiere ccp_alpha: Der Wert mit dem niedrigsten MSE\n",
    "optimal_alpha = alphas[np.argmin(scores)]\n",
    "\n",
    "print(f\"Optimales ccp_alpha: {optimal_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 16.44175806878632\n",
      "Fold 2: MSE = 10.578749486844142\n",
      "Fold 3: MSE = 6.395691940684702\n",
      "Fold 4: MSE = 8.35112744547648\n",
      "Fold 5: MSE = 6.087561879785182\n",
      "Durchschnittlicher MSE über alle Folds: 9.570977764315366\n",
      "Test MSE: 9.908935449527014\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_ccp = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Modell trainieren\n",
    "    tree_ccp = DecisionTreeRegressor(ccp_alpha=optimal_alpha)\n",
    "    tree_ccp.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred_ccp = tree_ccp.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    mse_ccp = mean_squared_error(y_val_fold, y_pred_ccp)\n",
    "    results_ccp.append(mse_ccp)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {mse_ccp}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_ccp = np.mean(results_ccp)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_ccp}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = tree_ccp.predict(X_test)\n",
    "\n",
    "# Calculate the MSE for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimale Parameter: {'ccp_alpha': 0.01, 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameter-Raster definieren\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'ccp_alpha': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# GridSearchCV initialisieren\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# GridSearchCV ausführen, um die optimalen Parameter zu finden\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Beste Parameter aus GridSearchCV\n",
    "optimal_params = grid_search.best_params_\n",
    "\n",
    "# Optimalen Parameter-Werte explizit definieren\n",
    "optimal_ccp_alpha = optimal_params['ccp_alpha']\n",
    "optimal_max_depth = optimal_params['max_depth']\n",
    "optimal_min_samples_split = optimal_params['min_samples_split']\n",
    "optimal_min_samples_leaf = optimal_params['min_samples_leaf']\n",
    "\n",
    "# Die optimalen Werte sind jetzt:\n",
    "print(f\"Optimale Parameter: {optimal_params}\")\n",
    "\n",
    "# Sie können diese optimalen Werte direkt in den nächsten Code-Block (z. B. das Training und Testen des Modells) übernehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Validation MSE = 8.51553222599059\n",
      "Fold 2: Validation MSE = 10.28692127653177\n",
      "Fold 3: Validation MSE = 6.605556947804184\n",
      "Fold 4: Validation MSE = 9.224495716088153\n",
      "Fold 5: Validation MSE = 5.583779468322595\n",
      "Durchschnittlicher Validation-MSE über alle Folds: 8.04325712694746\n",
      "Test-MSE: 9.487485070295662\n"
     ]
    }
   ],
   "source": [
    "# Set a seed for reproducibility\n",
    "np.random.seed(11)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_ccp = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Test-Dataset festlegen (z. B. die letzten 20% der Daten)\n",
    "test_size = int(0.2 * len(X_train))\n",
    "X_test = X_train.iloc[-test_size:]\n",
    "y_test = y_train.iloc[-test_size:]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Modell trainieren with multiple hyperparameters\n",
    "    tree_ccp = DecisionTreeRegressor(\n",
    "        ccp_alpha=optimal_alpha,\n",
    "        max_depth=optimal_max_depth,\n",
    "        min_samples_split=optimal_min_samples_split,\n",
    "        min_samples_leaf=optimal_min_samples_leaf,\n",
    "    )\n",
    "    tree_ccp.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred_ccp = tree_ccp.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den MSE für das Validation-Set\n",
    "    mse_ccp = mean_squared_error(y_val_fold, y_pred_ccp)\n",
    "    results_ccp.append(mse_ccp)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: Validation MSE = {mse_ccp}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_ccp = np.mean(results_ccp)\n",
    "print(f\"Durchschnittlicher Validation-MSE über alle Folds: {average_mse_ccp}\")\n",
    "\n",
    "\n",
    "# Vorhersagen für das Test-Set machen\n",
    "y_test_pred = tree_ccp.predict(X_test)\n",
    "\n",
    "# Berechne den Test-MSE\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test-MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_ccp.tree_.node_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging (Bootstrap Aggregating) ist eine Technik, um die Stabilität und Genauigkeit von Machine-Learning-Algorithmen zu verbessern, insbesondere bei Modellen wie Entscheidungsbäumen, die anfällig für hohe Varianz sind. Es basiert auf dem Bootstrapping-Prinzip, bei dem mehrere Trainingssets durch Zufallsstichproben mit Zurücklegen erzeugt werden.\n",
    "\n",
    "Jeder Baum wird auf einem dieser zufälligen Datensets trainiert, und die Vorhersagen der B Modelle werden durch Mittelung kombiniert. Mathematisch reduziert Bagging die Varianz der Modelle, weil unabhängige Fehler über die Modelle hinweg geglättet werden. So wird die Vorhersage insgesamt stabiler und robuster gegen Variationen in den Trainingsdaten.\n",
    "\n",
    "Das Ziel ist, Vorhersagefehler durch Mittelung der Outputs der individuellen Modelle zu minimieren, was insgesamt zu einer besseren Modellleistung führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 9.542784920425863\n",
      "Fold 2: MSE = 7.599629439353786\n",
      "Fold 3: MSE = 5.563090454388047\n",
      "Fold 4: MSE = 7.980927327830085\n",
      "Fold 5: MSE = 4.776170019129685\n",
      "Durchschnittlicher MSE über alle Folds: 7.092520432225493\n",
      "Test MSE: 7.688128769743882\n"
     ]
    }
   ],
   "source": [
    "# Import the regression tree from scikit-learn and a plotting helper\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# Import our train_test_split helper\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import the mean_squared_error function under the alias mse\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "# Import the resampling helper\n",
    "from sklearn.utils import resample\n",
    "# Import the sklearn implementation of bagging\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_bag = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "\n",
    "    # Create a bagged tree estimator with B=100 trees\n",
    "    bagged_trees = BaggingRegressor(DecisionTreeRegressor(), n_estimators=100)\n",
    "    \n",
    "    # Modell trainieren\n",
    "    bagged_trees.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred = bagged_trees.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results_bag.append(mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_bag = np.mean(results_bag)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_bag}\")\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = bagged_trees.predict(X_test)\n",
    "\n",
    "# Calculate the MSE for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest\n",
    "\n",
    "Random Forests erweitern Bagging, indem sie jedem Baum eine zusätzliche Zufallskomponente hinzufügen. Jeder Baum wird mit einem bootstrap-Sample der Trainingsdaten trainiert, wobei nur ein zufälliger Teil der Features für die Konstruktion des Baums verwendet wird. Dadurch unterscheidet sich Random Forests von klassischem Bagging, bei dem alle Features verfügbar sind.\n",
    "\n",
    "Die zufällige Auswahl der Features reduziert die Korrelation zwischen den Bäumen und verbessert die Generalisierung des Modells. Üblicherweise wird die Anzahl der verwendeten Features  m  so gewählt, dass  m \\approx \\sqrt{p} , wobei  p  die Gesamtzahl der Features ist. Wenn  m = p  gesetzt wird, ist Random Forest gleichbedeutend mit einem Bagging-Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 7.62656094324164\n",
      "Fold 2: MSE = 9.188005083316913\n",
      "Fold 3: MSE = 8.530674614937496\n",
      "Fold 4: MSE = 14.5053402755548\n",
      "Fold 5: MSE = 5.121834372852604\n",
      "Durchschnittlicher MSE über alle Folds: 8.994483057980691\n",
      "Test MSE: 7.6422910087307505\n"
     ]
    }
   ],
   "source": [
    "# Import the random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_rf = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    X_train_fold, X_val_fold = X_train_init.iloc[train_index], X_train_init.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_init.iloc[train_index], y_train_init.iloc[val_index]\n",
    "\n",
    "    # Initialize the random forest regressor\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_features=\"sqrt\")\n",
    "    \n",
    "    # Modell trainieren\n",
    "    rf.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred = rf.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    fold_mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results_rf.append(fold_mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {fold_mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_rf = np.mean(results_rf)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_rf}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate the MSE for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rubenstark/Library/Python/3.9/lib/python/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimale Parameter für Random Forest: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Fold 1: MSE = 7.582253359319438\n",
      "Fold 2: MSE = 9.63414338517883\n",
      "Fold 3: MSE = 8.059045399557556\n",
      "Fold 4: MSE = 14.391545354144627\n",
      "Fold 5: MSE = 5.114110592908587\n",
      "Durchschnittlicher Validation-MSE über alle Folds: 8.956219618221807\n",
      "Test MSE for Random Forest: 7.633898704092657\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameter-Raster definieren\n",
    "param_grid_rf = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [50, 100, 200],  # Anzahl der Bäume für den Random Forest\n",
    "    'max_features': ['sqrt', 'log2']  # Auswahl der Merkmale pro Split\n",
    "}\n",
    "\n",
    "# GridSearchCV initialisieren\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=11),\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring='neg_mean_squared_error',  # Maximierung des negativen MSE\n",
    "    cv=5,  # 5-Fold Cross-Validation\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# GridSearchCV ausführen, um die optimalen Parameter zu finden\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Beste Parameter aus GridSearchCV\n",
    "optimal_params_rf = grid_search_rf.best_params_\n",
    "\n",
    "# Optimalen Parameter-Werte explizit definieren\n",
    "optimal_max_depth = optimal_params_rf['max_depth']\n",
    "optimal_min_samples_split = optimal_params_rf['min_samples_split']\n",
    "optimal_min_samples_leaf = optimal_params_rf['min_samples_leaf']\n",
    "optimal_n_estimators = optimal_params_rf['n_estimators']\n",
    "optimal_max_features = optimal_params_rf['max_features']\n",
    "\n",
    "# Die optimalen Werte sind jetzt:\n",
    "print(f\"Optimale Parameter für Random Forest: {optimal_params_rf}\")\n",
    "\n",
    "# --- Modelltraining mit den optimalen Hyperparametern ---\n",
    "np.random.seed(11)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_rf = []\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    X_train_fold, X_val_fold = X_train_init.iloc[train_index], X_train_init.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_init.iloc[train_index], y_train_init.iloc[val_index]\n",
    "\n",
    "    # Initialize the random forest regressor mit optimalen Hyperparametern\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=optimal_n_estimators,\n",
    "        max_depth=optimal_max_depth,\n",
    "        min_samples_split=optimal_min_samples_split,\n",
    "        min_samples_leaf=optimal_min_samples_leaf,\n",
    "        max_features=optimal_max_features,\n",
    "        random_state=11\n",
    "    )\n",
    "    \n",
    "    # Modell trainieren\n",
    "    rf.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred = rf.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    fold_mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results_rf.append(fold_mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {fold_mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_rf = np.mean(results_rf)\n",
    "print(f\"Durchschnittlicher Validation-MSE über alle Folds: {average_mse_rf}\")\n",
    "\n",
    "# Vorhersagen für das Test-Set machen\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "# Berechne den Test-MSE\n",
    "test_mse_rf = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE for Random Forest: {test_mse_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
