{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use a nicer style for plots\n",
    "plt.style.use(\"seaborn-v0_8-muted\")\n",
    "\n",
    "# Import the regression tree from scikit-learn and a plotting helper\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# Import our train_test_split helper\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset into target and features and split them into test train Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_data = pd.read_csv('../../../4 - Data/04_WorkingDatasets/Top50CombLagged/50CombLagged.csv') #Top 50 Feature + comination Features without Lagged Target\n",
    "target_data = pd.read_csv('../../../4 - Data/04_WorkingDatasets/Top50CombLagged/TargetOutliersTreated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konvertiere die Datumsspalte in einen datetime-Index (falls nicht bereits)\n",
    "target_data['Datum'] = pd.to_datetime(target_data['Datum'])\n",
    "\n",
    "# Definiere das Cut-Off-Datum\n",
    "cutoff_date = pd.Timestamp('2024-10-20 21:00:00+00:00')\n",
    "\n",
    "# Filtere das Dataset auf Einträge bis einschließlich des Cut-Off-Datums\n",
    "target_data_cutted = target_data[target_data['Datum'] <= cutoff_date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 17184\n",
      "Validation Size: 2455\n",
      "Test Size: 4910\n"
     ]
    }
   ],
   "source": [
    "# Split our data intro features and targets\n",
    "# Teile das Dataset in Features und Zielvariable\n",
    "y = target_data_cutted[\"PM10_Combined_Trend_Residual\"]  # Zielvariable\n",
    "X = features_data.copy()      #drop(columns=[\"Datum\"])  # Alle Spalten außer der Zielvariable\n",
    "\n",
    "X.head(10)\n",
    "\n",
    "# Daten splitten\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=11) # letter K in Alphabet\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=11)  # 10% von Gesamt  # letter K in Alphabet\n",
    "\n",
    "print(\"Train Size:\", len(X_train))\n",
    "print(\"Validation Size:\", len(X_val))\n",
    "print(\"Test Size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 19.489655958010612\n",
      "Fold 2: MSE = 13.999832423566717\n",
      "Fold 3: MSE = 9.96722939149452\n",
      "Fold 4: MSE = 11.210610127366563\n",
      "Fold 5: MSE = 9.874101244545018\n",
      "Durchschnittlicher MSE über alle Folds: 12.908285828996688\n",
      "Test MSE: 13.81389705108869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "# Expanding Cross-Validation (5 Splits)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "results = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict and calculate MSE\n",
    "    y_pred = model.predict(X_val_fold)\n",
    "    mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results.append(mse)\n",
    "    print(f\"Fold {fold + 1}: MSE = {mse}\")\n",
    "\n",
    "# Average MSE across all folds\n",
    "average_mse = np.mean(results)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the MSE for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14319"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tree_.node_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ccp_alpha (and other parameters) to optimize the Decision Tree for example when it comes to overfitting\n",
    "\n",
    "[`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation to find best alpha \n",
    "\n",
    " Geht nicht mit so vielen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools for model selection\n",
    "from sklearn.model_selection import cross_validate, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimales ccp_alpha: 0.033932217718953266\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "# Definiere die Werte für ccp_alpha (Cost Complexity Pruning)\n",
    "alphas = np.logspace(-4, 0, 50)  # Werte zwischen 10^-4 und 10^0\n",
    "\n",
    "# Initialisiere Cross-Validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=11)\n",
    "\n",
    "# Speicher für Ergebnisse\n",
    "scores = []  # Durchschnittliche MSE für jedes ccp_alpha\n",
    "scores_std = []  # Standardabweichung der Scores für Stabilitätsanalyse\n",
    "\n",
    "# Cross-Validation für jedes ccp_alpha\n",
    "for alpha in alphas:\n",
    "    # Decision Tree mit aktuellem ccp_alpha-Wert\n",
    "    tree_cv = DecisionTreeRegressor(ccp_alpha=alpha, random_state=11)\n",
    "    \n",
    "    # Negative MSE, da cross_val_score maximiert; wir wollen minimieren\n",
    "    mse_scores = cross_val_score(tree_cv, X_train, y_train, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "    \n",
    "    # Durchschnittlichen MSE speichern (negativ, daher multiplizieren mit -1)\n",
    "    scores.append(-mse_scores.mean())\n",
    "    scores_std.append(mse_scores.std())\n",
    "\n",
    "# Optimiere ccp_alpha: Der Wert mit dem niedrigsten MSE\n",
    "optimal_alpha = alphas[np.argmin(scores)]\n",
    "\n",
    "print(f\"Optimales ccp_alpha: {optimal_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 16.44175806878632\n",
      "Fold 2: MSE = 10.578749486844142\n",
      "Fold 3: MSE = 6.395691940684702\n",
      "Fold 4: MSE = 8.35112744547648\n",
      "Fold 5: MSE = 6.087561879785182\n",
      "Durchschnittlicher MSE über alle Folds: 9.570977764315366\n",
      "Test MSE: 9.908935449527014\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_ccp = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Modell trainieren\n",
    "    tree_ccp = DecisionTreeRegressor(ccp_alpha=optimal_alpha)\n",
    "    tree_ccp.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred_ccp = tree_ccp.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    mse_ccp = mean_squared_error(y_val_fold, y_pred_ccp)\n",
    "    results_ccp.append(mse_ccp)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {mse_ccp}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_ccp = np.mean(results_ccp)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_ccp}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = tree_ccp.predict(X_test)\n",
    "\n",
    "# Calculate the MSE for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimale Parameter: {'ccp_alpha': 0.001, 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameter-Raster definieren\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'ccp_alpha': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# GridSearchCV initialisieren\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# GridSearchCV ausführen, um die optimalen Parameter zu finden\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Beste Parameter aus GridSearchCV\n",
    "optimal_params = grid_search.best_params_\n",
    "\n",
    "# Optimalen Parameter-Werte explizit definieren\n",
    "optimal_ccp_alpha = optimal_params['ccp_alpha']\n",
    "optimal_max_depth = optimal_params['max_depth']\n",
    "optimal_min_samples_split = optimal_params['min_samples_split']\n",
    "optimal_min_samples_leaf = optimal_params['min_samples_leaf']\n",
    "\n",
    "# Die optimalen Werte sind jetzt:\n",
    "print(f\"Optimale Parameter: {optimal_params}\")\n",
    "\n",
    "# Sie können diese optimalen Werte direkt in den nächsten Code-Block (z. B. das Training und Testen des Modells) übernehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Validation MSE = 14.52758930579122\n",
      "Fold 2: Validation MSE = 11.445519222442572\n",
      "Fold 3: Validation MSE = 6.589694997637775\n",
      "Fold 4: Validation MSE = 8.716791500908046\n",
      "Fold 5: Validation MSE = 6.326175331004765\n",
      "Durchschnittlicher Validation-MSE über alle Folds: 9.521154071556875\n",
      "Test-MSE: 9.723721466452107\n"
     ]
    }
   ],
   "source": [
    "# Set a seed for reproducibility\n",
    "np.random.seed(11)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_ccp = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Test-Dataset festlegen (z. B. die letzten 20% der Daten)\n",
    "test_size = int(0.2 * len(X_train))\n",
    "X_test = X_train.iloc[-test_size:]\n",
    "y_test = y_train.iloc[-test_size:]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "    \n",
    "    # Modell trainieren with multiple hyperparameters\n",
    "    tree_ccp = DecisionTreeRegressor(\n",
    "        ccp_alpha=optimal_alpha,\n",
    "        max_depth=optimal_max_depth,\n",
    "        min_samples_split=optimal_min_samples_split,\n",
    "        min_samples_leaf=optimal_min_samples_leaf,\n",
    "    )\n",
    "    tree_ccp.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred_ccp = tree_ccp.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den MSE für das Validation-Set\n",
    "    mse_ccp = mean_squared_error(y_val_fold, y_pred_ccp)\n",
    "    results_ccp.append(mse_ccp)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: Validation MSE = {mse_ccp}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_ccp = np.mean(results_ccp)\n",
    "print(f\"Durchschnittlicher Validation-MSE über alle Folds: {average_mse_ccp}\")\n",
    "\n",
    "\n",
    "# Vorhersagen für das Test-Set machen\n",
    "y_test_pred = tree_ccp.predict(X_test)\n",
    "\n",
    "# Berechne den Test-MSE\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test-MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_ccp.tree_.node_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging (Bootstrap Aggregating) ist eine Technik, um die Stabilität und Genauigkeit von Machine-Learning-Algorithmen zu verbessern, insbesondere bei Modellen wie Entscheidungsbäumen, die anfällig für hohe Varianz sind. Es basiert auf dem Bootstrapping-Prinzip, bei dem mehrere Trainingssets durch Zufallsstichproben mit Zurücklegen erzeugt werden.\n",
    "\n",
    "Jeder Baum wird auf einem dieser zufälligen Datensets trainiert, und die Vorhersagen der B Modelle werden durch Mittelung kombiniert. Mathematisch reduziert Bagging die Varianz der Modelle, weil unabhängige Fehler über die Modelle hinweg geglättet werden. So wird die Vorhersage insgesamt stabiler und robuster gegen Variationen in den Trainingsdaten.\n",
    "\n",
    "Das Ziel ist, Vorhersagefehler durch Mittelung der Outputs der individuellen Modelle zu minimieren, was insgesamt zu einer besseren Modellleistung führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 9.542784920425863\n",
      "Fold 2: MSE = 7.599629439353786\n",
      "Fold 3: MSE = 5.563090454388047\n",
      "Fold 4: MSE = 7.980927327830085\n",
      "Fold 5: MSE = 4.776170019129685\n",
      "Durchschnittlicher MSE über alle Folds: 7.092520432225493\n",
      "Test MSE: 7.579578989624366\n"
     ]
    }
   ],
   "source": [
    "# Import the regression tree from scikit-learn and a plotting helper\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# Import our train_test_split helper\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import the mean_squared_error function under the alias mse\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "# Import the resampling helper\n",
    "from sklearn.utils import resample\n",
    "# Import the sklearn implementation of bagging\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_bag = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    # Expanding training data with each fold\n",
    "    X_train_fold = X_train_init.iloc[train_index]\n",
    "    y_train_fold = y_train_init.iloc[train_index]\n",
    "    \n",
    "    # Validation data stays fixed (next chunk after the training data)\n",
    "    X_val_fold = X_train_init.iloc[val_index]\n",
    "    y_val_fold = y_train_init.iloc[val_index]\n",
    "\n",
    "    # Create a bagged tree estimator with B=100 trees\n",
    "    bagged_trees = BaggingRegressor(DecisionTreeRegressor(), n_estimators=100)\n",
    "    \n",
    "    # Modell trainieren\n",
    "    bagged_trees.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred = bagged_trees.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results_bag.append(mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_bag = np.mean(results_bag)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_bag}\")\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = bagged_trees.predict(X_test)\n",
    "\n",
    "# Calculate the MSE for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest\n",
    "\n",
    "Random Forests erweitern Bagging, indem sie jedem Baum eine zusätzliche Zufallskomponente hinzufügen. Jeder Baum wird mit einem bootstrap-Sample der Trainingsdaten trainiert, wobei nur ein zufälliger Teil der Features für die Konstruktion des Baums verwendet wird. Dadurch unterscheidet sich Random Forests von klassischem Bagging, bei dem alle Features verfügbar sind.\n",
    "\n",
    "Die zufällige Auswahl der Features reduziert die Korrelation zwischen den Bäumen und verbessert die Generalisierung des Modells. Üblicherweise wird die Anzahl der verwendeten Features  m  so gewählt, dass  m \\approx \\sqrt{p} , wobei  p  die Gesamtzahl der Features ist. Wenn  m = p  gesetzt wird, ist Random Forest gleichbedeutend mit einem Bagging-Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MSE = 7.62656094324164\n",
      "Fold 2: MSE = 9.188005083316913\n",
      "Fold 3: MSE = 8.530674614937496\n",
      "Fold 4: MSE = 14.5053402755548\n",
      "Fold 5: MSE = 5.121834372852604\n",
      "Durchschnittlicher MSE über alle Folds: 8.994483057980691\n",
      "Test MSE: 7.4966187086303435\n"
     ]
    }
   ],
   "source": [
    "# Import the random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_rf = []\n",
    "\n",
    "# Use the initial 50% of the data for training (optional, but keeps your requirement in mind)\n",
    "train_size = int(0.5 * len(X_train))\n",
    "X_train_init = X_train.iloc[:train_size]\n",
    "y_train_init = y_train.iloc[:train_size]\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    X_train_fold, X_val_fold = X_train_init.iloc[train_index], X_train_init.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_init.iloc[train_index], y_train_init.iloc[val_index]\n",
    "\n",
    "    # Initialize the random forest regressor\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_features=\"sqrt\")\n",
    "    \n",
    "    # Modell trainieren\n",
    "    rf.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred = rf.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    fold_mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results_rf.append(fold_mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {fold_mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_rf = np.mean(results_rf)\n",
    "print(f\"Durchschnittlicher MSE über alle Folds: {average_mse_rf}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate the MSE for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 24\u001b[0m\n\u001b[1;32m     15\u001b[0m grid_search_rf \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     16\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mRandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m),\n\u001b[1;32m     17\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid_rf,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# GridSearchCV ausführen, um die optimalen Parameter zu finden\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mgrid_search_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Beste Parameter aus GridSearchCV\u001b[39;00m\n\u001b[1;32m     27\u001b[0m optimal_params_rf \u001b[38;5;241m=\u001b[39m grid_search_rf\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameter-Raster definieren\n",
    "param_grid_rf = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [50, 100, 200],  # Anzahl der Bäume für den Random Forest\n",
    "    'max_features': ['sqrt', 'log2']  # Auswahl der Merkmale pro Split\n",
    "}\n",
    "\n",
    "# GridSearchCV initialisieren\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=11),\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring='neg_mean_squared_error',  # Maximierung des negativen MSE\n",
    "    cv=5,  # 5-Fold Cross-Validation\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# GridSearchCV ausführen, um die optimalen Parameter zu finden\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Beste Parameter aus GridSearchCV\n",
    "optimal_params_rf = grid_search_rf.best_params_\n",
    "\n",
    "# Optimalen Parameter-Werte explizit definieren\n",
    "optimal_max_depth = optimal_params_rf['max_depth']\n",
    "optimal_min_samples_split = optimal_params_rf['min_samples_split']\n",
    "optimal_min_samples_leaf = optimal_params_rf['min_samples_leaf']\n",
    "optimal_n_estimators = optimal_params_rf['n_estimators']\n",
    "optimal_max_features = optimal_params_rf['max_features']\n",
    "\n",
    "# Die optimalen Werte sind jetzt:\n",
    "print(f\"Optimale Parameter für Random Forest: {optimal_params_rf}\")\n",
    "\n",
    "# --- Modelltraining mit den optimalen Hyperparametern ---\n",
    "np.random.seed(11)\n",
    "\n",
    "# Expanding Cross Validation mit 5 Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Liste zur Speicherung der Ergebnisse\n",
    "results_rf = []\n",
    "\n",
    "# Perform expanding window cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_init)):\n",
    "    X_train_fold, X_val_fold = X_train_init.iloc[train_index], X_train_init.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_init.iloc[train_index], y_train_init.iloc[val_index]\n",
    "\n",
    "    # Initialize the random forest regressor mit optimalen Hyperparametern\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=optimal_n_estimators,\n",
    "        max_depth=optimal_max_depth,\n",
    "        min_samples_split=optimal_min_samples_split,\n",
    "        min_samples_leaf=optimal_min_samples_leaf,\n",
    "        max_features=optimal_max_features,\n",
    "        random_state=11\n",
    "    )\n",
    "    \n",
    "    # Modell trainieren\n",
    "    rf.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Vorhersagen machen\n",
    "    y_pred = rf.predict(X_val_fold)\n",
    "    \n",
    "    # Berechne den Fehler\n",
    "    fold_mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    results_rf.append(fold_mse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: MSE = {fold_mse}\")\n",
    "\n",
    "# Durchschnittlichen Fehler über alle Folds berechnen\n",
    "average_mse_rf = np.mean(results_rf)\n",
    "print(f\"Durchschnittlicher Validation-MSE über alle Folds: {average_mse_rf}\")\n",
    "\n",
    "# Vorhersagen für das Test-Set machen\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "# Berechne den Test-MSE\n",
    "test_mse_rf = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE for Random Forest: {test_mse_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
