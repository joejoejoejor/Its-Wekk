{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features = pd.read_csv('../../../4 - Data/04_WorkingDatasets/Top50CombLagged/50CombLagged.csv') #Top 50 Feature + comination Features + Lagged Target\n",
    "target = pd.read_csv('../../../4 - Data/04_WorkingDatasets/Top50CombLagged/TargetOutliersTreated.csv')\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(11)\n",
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   1.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   1.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   1.6s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   1.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   1.5s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   1.5s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   1.5s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   1.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   1.7s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   1.5s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   1.6s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   8.9s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   8.9s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   9.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   9.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   9.4s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  12.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  12.8s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  12.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   8.4s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   8.6s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   1.7s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   1.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   1.6s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   7.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   9.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   9.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   8.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   8.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   8.6s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   8.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   9.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  12.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  12.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  12.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  13.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  13.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  13.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  13.7s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  14.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   9.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  14.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  13.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   1.5s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   8.8s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   8.8s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=None; total time=   1.0s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   2.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   8.9s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   9.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l2; total time=   8.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l2; total time=   8.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l2; total time=   8.6s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l2; total time=   8.6s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l2; total time=   8.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  12.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  12.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  12.8s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l1; total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  12.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  12.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  12.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  12.8s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=None; total time=   8.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=None; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=None; total time=   8.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  12.6s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  12.7s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   3.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=None; total time=   9.0s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=None; total time=   9.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   9.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   9.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   9.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   9.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   9.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  12.9s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  12.7s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  12.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.4s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  12.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.5s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   8.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   8.6s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   2.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   1.9s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.6s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.7s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   1.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   8.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   8.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   8.8s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   8.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   8.9s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   9.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   9.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  12.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  12.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  12.5s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  12.5s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  12.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  12.6s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  13.0s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   8.9s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  12.8s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  12.7s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   9.0s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   9.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   9.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   9.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  10.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  10.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  10.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  10.4s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  10.6s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l1; total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  14.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  14.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  14.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  12.7s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  13.0s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=None; total time=   8.6s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  12.9s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=None; total time=   8.7s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  13.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  12.8s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=None; total time=   6.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=None; total time=   9.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=None; total time=   8.9s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   9.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None; total time=   8.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1; total time=  12.8s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   9.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None; total time=   8.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  12.6s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  12.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   8.9s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   8.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=  12.8s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   8.8s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=   4.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   8.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  12.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  13.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  12.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.6s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.7s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   5.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   8.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   9.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   9.6s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   9.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  13.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   9.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=None; total time=   8.9s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=None; total time=   8.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  13.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  13.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=  13.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   8.9s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   8.8s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   4.7s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   9.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   8.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  12.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  12.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  12.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  12.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  13.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  13.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  13.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  13.0s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  11.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   8.7s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   9.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  12.9s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   8.9s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   8.9s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   9.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l2; total time=   9.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=None; total time=   9.0s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=None; total time=   8.9s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=None; total time=   9.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l1; total time=  13.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l2; total time=   3.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l1; total time=  13.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=  13.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l1; total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l2; total time=   9.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l2; total time=   9.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l2; total time=   9.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l2; total time=   9.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l1; total time=  13.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l1; total time=  13.8s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l1; total time=  13.8s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=   8.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l1; total time=  13.9s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=None; total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  14.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  14.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  14.6s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=None; total time=   9.9s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=None; total time=  10.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=None; total time=  10.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  16.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=None; total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2; total time=  10.9s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2; total time=  11.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1; total time=  16.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1; total time=  16.4s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1; total time=  16.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1; total time=  16.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1; total time=  15.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=  15.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=  15.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=  15.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None; total time=   9.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None; total time=   9.5s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None; total time=   9.7s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None; total time=   9.8s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None; total time=   9.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   9.7s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   9.8s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  14.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  14.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  14.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  14.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  14.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=  14.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None; total time=   9.8s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=  14.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=  14.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None; total time=   9.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None; total time=   9.7s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None; total time=  10.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None; total time=   9.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=  14.8s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=   3.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   7.8s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   7.8s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   8.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   8.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  15.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  14.9s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  15.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  13.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  13.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  14.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  14.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.6s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   3.6s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   9.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   9.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   9.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   9.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   9.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   9.5s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   9.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  13.9s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  14.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  14.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  14.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  14.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=  14.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=  14.5s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=  14.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=None; total time=  10.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=None; total time=  10.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=None; total time=  10.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=None; total time=  10.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   4.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   4.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   4.5s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=None; total time=  10.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=   7.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=   7.7s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=   7.9s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=   2.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  14.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  14.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  14.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  14.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  14.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=   8.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   9.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  10.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  10.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  10.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=None; total time=   9.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l2; total time=   9.4s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l2; total time=   9.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l2; total time=   9.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l1; total time=  14.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l1; total time=  14.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l1; total time=  14.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l1; total time=  13.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l1; total time=  14.0s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=None; total time=   9.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=  14.0s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=  14.0s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l2; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=None; total time=   9.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=None; total time=   9.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=None; total time=   9.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=None; total time=   9.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l1; total time=   2.7s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l2; total time=   8.7s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l2; total time=   9.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l2; total time=   9.7s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l2; total time=   9.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l1; total time=  14.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l1; total time=  14.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l1; total time=  14.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l1; total time=  14.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  15.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  15.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  15.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=None; total time=   9.5s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=None; total time=   9.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=None; total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=None; total time=   8.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  14.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=None; total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 708497.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 0.1, 'eta0': 0.001, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "Best Score: -6.908058485039133\n",
      "Mean Squared Error on Test Set: 5.113515278750086\n",
      "Mean Squared Error on Validation Set: 7.994132374431479\n",
      "Fold 1/5, MSE: 4.798085227085186\n",
      "Fold 2/5, MSE: 6.309839094437753\n",
      "Fold 3/5, MSE: 4.726220434583295\n",
      "Fold 4/5, MSE: 5.2101711223175355\n",
      "Fold 5/5, MSE: 0.7262933589008354\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Ensure the features and target dataframes are correctly assigned\n",
    "#features = features.drop(columns=['Datum'])\n",
    "target = target['PM10_Combined_Trend_Residual']\n",
    "\n",
    "# Split the data into train, test, and validate sets\n",
    "train_data, temp_data, train_target, temp_target = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "test_data, validate_data, test_target, validate_target = train_test_split(temp_data, temp_target, test_size=0.3333, random_state=42)\n",
    "\n",
    "# Add squared features\n",
    "features_squared = features ** 2\n",
    "features = pd.concat([features, features_squared.add_suffix('_squared')], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Save the scaler using pickle\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Update the train, test, and validate sets with the scaled features\n",
    "train_data, temp_data, train_target, temp_target = train_test_split(features_scaled, target, test_size=0.3, random_state=42)\n",
    "test_data, validate_data, test_target, validate_target = train_test_split(temp_data, temp_target, test_size=0.3333, random_state=42)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'eta0': [0.001, 0.01, 0.1, 1],\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet', None]\n",
    "}\n",
    "\n",
    "# Initialize the SGDRegressor\n",
    "sgd = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(sgd, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(train_data, train_target.values.ravel())\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Plot the progress bar\n",
    "progress_bar = tqdm(total=100)\n",
    "for i in range(100):\n",
    "    progress_bar.update(1)\n",
    "progress_bar.close()\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = grid_search.predict(test_data)\n",
    "predictions = np.maximum(predictions, 0)  # Ensure predictions are not lower than zero\n",
    "mse = mean_squared_error(test_target, predictions)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validate_predictions = grid_search.predict(validate_data)\n",
    "validate_predictions = np.maximum(validate_predictions, 0)  # Ensure predictions are not lower than zero\n",
    "validate_mse = mean_squared_error(validate_target, validate_predictions)\n",
    "print(\"Mean Squared Error on Validation Set:\", validate_mse)\n",
    "\n",
    "# Expanding CV Fold Sizes\n",
    "initial_train_size = int(0.5 * len(train_data))\n",
    "test_size = int(0.1 * len(train_data))\n",
    "num_folds = (len(train_data) - initial_train_size) // test_size\n",
    "\n",
    "def train_and_evaluate_fold(fold):\n",
    "    start_train_size = initial_train_size + fold * test_size\n",
    "    end_train_size = start_train_size + test_size\n",
    "\n",
    "    X_train_fold = train_data[:end_train_size]\n",
    "    y_train_fold = train_target[:end_train_size]\n",
    "    X_test_fold = train_data[end_train_size:end_train_size + test_size]\n",
    "    y_test_fold = train_target[end_train_size:end_train_size + test_size]\n",
    "\n",
    "    model = SGDRegressor(**best_params, max_iter=1000, tol=1e-3)\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    fold_predictions = model.predict(X_test_fold)\n",
    "    fold_predictions = np.maximum(fold_predictions, 0)  # Ensure predictions are not lower than zero\n",
    "    fold_mse = mean_squared_error(y_test_fold, fold_predictions)\n",
    "    return fold, fold_mse\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(train_and_evaluate_fold)(fold) for fold in range(num_folds))\n",
    "\n",
    "for fold, fold_mse in results:\n",
    "    print(f\"Fold {fold + 1}/{num_folds}, MSE: {fold_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   2.9s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   3.0s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   3.2s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   3.7s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   4.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=1.0, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   4.7s\n",
      "[CV] END alpha=1.0, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   5.2s\n",
      "[CV] END alpha=1.0, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=1.0, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=1.0, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   5.4s\n",
      "[CV] END alpha=1.0, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=1.0, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=1.0, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=1.0, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=1.0, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=1.0, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   4.1s\n",
      "[CV] END alpha=1.0, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   5.0s\n",
      "[CV] END alpha=1.0, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   5.3s\n",
      "[CV] END alpha=1.0, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   5.2s\n",
      "[CV] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   3.8s\n",
      "[CV] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   4.3s\n",
      "[CV] END alpha=1.0, eta0=0.0001, learning_rate=invscaling, penalty=l1; total time=   2.5s\n",
      "Refined Best Parameters: {'alpha': 0.1, 'eta0': 0.001, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "Refined Best Score: -6.9061124371535385\n",
      "Refined Mean Squared Error on Test Set: 5.086650127656327\n",
      "Refined Mean Squared Error on Validation Set: 7.9466503415045615\n"
     ]
    }
   ],
   "source": [
    "# Define a more focused parameter grid for GridSearchCV based on the best parameters\n",
    "refined_param_grid = {\n",
    "    'alpha': [best_params['alpha'] * 0.1, best_params['alpha'], best_params['alpha'] * 10],\n",
    "    'eta0': [best_params['eta0'] * 0.1, best_params['eta0'], best_params['eta0'] * 10],\n",
    "    'learning_rate': [best_params['learning_rate']],\n",
    "    'penalty': [best_params['penalty']]\n",
    "}\n",
    "\n",
    "# Initialize a new GridSearchCV with the refined parameter grid\n",
    "refined_grid_search = GridSearchCV(sgd, refined_param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model with the refined parameter grid\n",
    "refined_grid_search.fit(train_data, train_target.values.ravel())\n",
    "\n",
    "# Get the best parameters and best score from the refined search\n",
    "refined_best_params = refined_grid_search.best_params_\n",
    "refined_best_score = refined_grid_search.best_score_\n",
    "\n",
    "# Print the refined best parameters and best score\n",
    "print(\"Refined Best Parameters:\", refined_best_params)\n",
    "print(\"Refined Best Score:\", refined_best_score)\n",
    "\n",
    "# Evaluate the refined model on the test set\n",
    "refined_predictions = refined_grid_search.predict(test_data)\n",
    "refined_mse = mean_squared_error(test_target, refined_predictions)\n",
    "print(\"Refined Mean Squared Error on Test Set:\", refined_mse)\n",
    "\n",
    "# Evaluate the refined model on the validation set\n",
    "refined_validate_predictions = refined_grid_search.predict(validate_data)\n",
    "refined_validate_mse = mean_squared_error(validate_target, refined_validate_predictions)\n",
    "print(\"Refined Mean Squared Error on Validation Set:\", refined_validate_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the indices of the 10 highest absolute betas\n",
    "top_6_indices = np.argsort(np.abs(betas))[-6:]\n",
    "\n",
    "# Get the corresponding feature names\n",
    "top_10_features = features.columns[top_6_indices]\n",
    "\n",
    "# Get the corresponding betas\n",
    "top_10_betas = betas[top_6_indices]\n",
    "\n",
    "# Plot the top 10 features with their betas\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_10_features, top_10_betas, color='skyblue')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 10 Features with Highest Absolute Betas')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
