{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features = pd.read_csv('../../../4 - Data/04_WorkingDatasets/Top50CombLagged/50CombLagged.csv') #Top 50 Feature + comination Features + Lagged Target\n",
    "target = pd.read_csv('../../../4 - Data/04_WorkingDatasets/Top50CombLagged/TargetOutliersTreated.csv')\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(11)\n",
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   1.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   1.7s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   1.9s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   1.9s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   3.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   3.5s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   2.8s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   3.9s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   3.3s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   1.8s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   1.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   3.5s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   2.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   4.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   1.7s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   4.6s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   2.8s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   1.8s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   1.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   1.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   2.3s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.4s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   3.5s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   3.3s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   2.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   1.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   1.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   1.5s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   1.7s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   2.6s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   2.6s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   3.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   2.5s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   4.6s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   4.7s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   4.5s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   1.4s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   4.6s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   4.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   2.8s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   5.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   3.9s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   5.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   1.9s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   2.6s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   2.5s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  25.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  25.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  26.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  27.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  27.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=   7.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  36.6s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  37.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  37.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  34.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  34.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  33.4s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  23.4s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  23.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  34.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   2.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  34.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   3.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   3.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   4.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   4.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  36.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   2.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   4.7s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   3.3s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   1.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   1.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  15.9s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   3.9s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   3.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   3.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   1.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   2.3s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.7s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  25.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  26.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=  24.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=  24.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=  24.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=  25.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=  25.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  37.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  37.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  38.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  40.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  40.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  39.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  41.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  42.0s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=constant, penalty=None; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  29.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l2; total time=   2.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l2; total time=   1.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l2; total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  41.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l2; total time=   2.5s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l2; total time=   2.5s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l1; total time=   3.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l1; total time=   2.6s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l1; total time=   4.7s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   1.7s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l1; total time=   3.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=l1; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  42.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   1.8s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   3.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   3.5s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   4.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=None; total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  27.3s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  27.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=None; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  27.7s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=None; total time=   3.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=None; total time=   2.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   1.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  26.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=optimal, penalty=None; total time=   2.6s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   1.5s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   2.0s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   2.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   2.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   2.6s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   3.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=None; total time=   1.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   2.5s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   3.6s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.9s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   2.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=None; total time=   1.4s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=None; total time=   1.0s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=invscaling, penalty=None; total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  28.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  29.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  30.2s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  30.0s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  30.1s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l1; total time=   9.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  41.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  41.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  42.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  42.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  43.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  42.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=None; total time=  30.6s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  44.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=None; total time=  32.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   1.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   1.6s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   2.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   1.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   1.8s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   4.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   2.7s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  47.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   1.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   1.8s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   2.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   2.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None; total time=   1.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  46.9s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   1.6s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   2.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   1.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   2.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   1.5s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=None; total time=  19.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   2.5s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   1.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   1.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   1.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   1.7s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   1.8s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   2.9s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   2.5s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   2.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   1.7s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   1.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=None; total time=  31.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.0001, eta0=1, learning_rate=adaptive, penalty=None; total time=  31.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  24.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  25.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  25.6s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  25.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  26.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=   8.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  33.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  35.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  33.7s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  33.7s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  34.5s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  34.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  34.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  23.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  24.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   1.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   1.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   1.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   1.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  35.9s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   1.5s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   2.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   1.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=None; total time=   1.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   6.7s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  36.9s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   1.7s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  25.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  26.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=  25.8s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=  25.7s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=  25.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=  25.9s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=  26.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  37.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  40.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  41.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  41.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  41.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  42.6s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  42.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  42.8s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  28.3s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  43.8s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l2; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l1; total time=   1.7s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l1; total time=   1.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=l1; total time=   1.5s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   1.5s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   2.0s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   2.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   2.0s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=optimal, penalty=None; total time=   0.9s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   1.0s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   1.0s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l2; total time=   1.0s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   1.4s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   1.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  39.4s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=l1; total time=   1.7s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  26.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=None; total time=   1.3s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.6s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.9s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   2.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  25.8s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=None; total time=   1.0s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=invscaling, penalty=None; total time=   1.4s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  26.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  27.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  26.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  25.9s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  26.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  26.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l2; total time=  26.8s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  10.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  37.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  38.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  37.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  38.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=None; total time=  26.5s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  38.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  39.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=l1; total time=  39.8s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=None; total time=  27.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=None; total time=   4.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   2.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  39.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  40.8s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   2.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None; total time=   1.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=None; total time=  29.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.001, eta0=1, learning_rate=adaptive, penalty=None; total time=  30.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None; total time=   1.8s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2; total time=  29.8s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   2.8s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   2.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   2.7s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   2.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   2.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   1.8s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   3.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1; total time=  41.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   2.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1; total time=  41.7s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None; total time=  28.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=  40.8s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   4.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=  37.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1; total time=   4.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2; total time=  29.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None; total time=  29.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None; total time=  28.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  28.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  29.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  39.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  39.6s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   8.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=  39.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  27.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  19.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=  27.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  18.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  43.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  42.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  43.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  44.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  45.9s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   6.9s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  42.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  43.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l1; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  27.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  29.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  27.4s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   2.5s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  26.5s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=None; total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  40.5s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=None; total time=   1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l2; total time=  28.5s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  31.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  31.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=None; total time=  17.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  28.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=None; total time=  16.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  27.8s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=  24.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=  10.3s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   9.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   9.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   9.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  13.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  13.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  13.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  13.5s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  14.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  14.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  14.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  15.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  11.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  11.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l1; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  10.9s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  15.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=None; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  10.9s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  11.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l2; total time=  10.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l1; total time=  14.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=None; total time=  10.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l1; total time=  14.8s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=l1; total time=  15.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=None; total time=  10.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=  14.8s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l2; total time=   3.8s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l1; total time=   4.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l2; total time=  10.0s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l2; total time=   9.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l2; total time=  10.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l2; total time=  10.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l1; total time=  14.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l1; total time=  14.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l1; total time=  14.0s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=   8.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=l1; total time=  14.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  14.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  14.4s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  14.4s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=None; total time=   1.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=None; total time=  10.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=None; total time=  10.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=None; total time=  10.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  15.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, penalty=None; total time=  11.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2; total time=  10.5s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2; total time=  10.8s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1; total time=  15.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1; total time=  14.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1; total time=  15.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1; total time=  14.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1; total time=  14.6s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None; total time=  10.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=  14.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=elasticnet; total time=  14.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l2; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None; total time=  10.3s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None; total time=  10.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None; total time=  10.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=l1; total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, penalty=elasticnet; total time=   1.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None; total time=   9.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2; total time=  10.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2; total time=  10.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2; total time=  10.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  14.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  14.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  14.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  14.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1; total time=  14.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=  14.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None; total time=  10.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None; total time=   9.8s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=  14.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None; total time=   9.9s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=  14.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None; total time=  10.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=elasticnet; total time=  14.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   7.8s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   7.9s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   8.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l2; total time=   8.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None; total time=  10.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=   3.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  14.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  14.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  14.7s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.8s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  13.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  12.8s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=l1; total time=  14.8s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=None; total time=   4.4s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  10.7s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  10.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=elasticnet; total time=  14.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, penalty=None; total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l2; total time=  10.5s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l2; total time=  10.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l2; total time=  10.8s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  14.9s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  14.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  14.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  14.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=l1; total time=  15.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=  14.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=  15.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=None; total time=  10.6s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l2; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=  15.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=None; total time=  10.7s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=elasticnet; total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, penalty=None; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=None; total time=  10.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=elasticnet; total time=  15.7s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   4.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   4.5s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=None; total time=  10.9s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l2; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, penalty=None; total time=  10.8s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=   8.7s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=   9.7s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=   9.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=   2.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  16.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  18.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  18.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  18.0s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=l1; total time=  18.5s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l2; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=None; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=constant, penalty=None; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=elasticnet; total time=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  15.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  16.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  15.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  13.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, penalty=None; total time=  13.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l2; total time=  13.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l2; total time=  13.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l2; total time=  12.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l1; total time=  16.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l1; total time=  16.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l1; total time=  16.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l1; total time=  16.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=l1; total time=  16.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=  15.7s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=  16.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=elasticnet; total time=  16.4s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=l1; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=None; total time=  11.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=None; total time=  11.4s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=invscaling, penalty=None; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=None; total time=  11.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l2; total time=   1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=None; total time=  12.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=optimal, penalty=None; total time=  12.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l1; total time=   2.4s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l2; total time=  10.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l2; total time=  11.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l2; total time=  11.0s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l2; total time=  11.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l1; total time=  16.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l1; total time=  16.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l1; total time=  16.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=l1; total time=  16.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=None; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=None; total time=  11.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  17.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  18.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  18.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=None; total time=  11.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=None; total time=  10.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=elasticnet; total time=  16.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannessolibieda/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:1616: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, penalty=None; total time=   6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 249364.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 0.1, 'eta0': 0.001, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "Best Score: -6.849652993655974\n",
      "Mean Squared Error on Test Set: 5.462614360360832\n",
      "Mean Squared Error on Validation Set: 8.381248337852279\n",
      "Fold 1/5, MSE: 4.731652300452814\n",
      "Fold 2/5, MSE: 6.887069269438181\n",
      "Fold 3/5, MSE: 4.885781509647411\n",
      "Fold 4/5, MSE: 5.30858902406746\n",
      "Fold 5/5, MSE: 0.5789387401359388\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Ensure the features and target dataframes are correctly assigned\n",
    "features = features.drop(columns=['Datum'])\n",
    "target = target['PM10_Combined_Trend_Residual']\n",
    "\n",
    "# Split the data into train, test, and validate sets\n",
    "train_data, temp_data, train_target, temp_target = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "test_data, validate_data, test_target, validate_target = train_test_split(temp_data, temp_target, test_size=0.3333, random_state=42)\n",
    "\n",
    "# Add squared features\n",
    "features_squared = features ** 2\n",
    "features = pd.concat([features, features_squared.add_suffix('_squared')], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Save the scaler using pickle\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Update the train, test, and validate sets with the scaled features\n",
    "train_data, temp_data, train_target, temp_target = train_test_split(features_scaled, target, test_size=0.3, random_state=42)\n",
    "test_data, validate_data, test_target, validate_target = train_test_split(temp_data, temp_target, test_size=0.3333, random_state=42)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'eta0': [0.001, 0.01, 0.1, 1],\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet', None]\n",
    "}\n",
    "\n",
    "# Initialize the SGDRegressor\n",
    "sgd = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(sgd, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(train_data, train_target.values.ravel())\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Plot the progress bar\n",
    "progress_bar = tqdm(total=100)\n",
    "for i in range(100):\n",
    "    progress_bar.update(1)\n",
    "progress_bar.close()\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = grid_search.predict(test_data)\n",
    "predictions = np.maximum(predictions, 0)  # Ensure predictions are not lower than zero\n",
    "mse = mean_squared_error(test_target, predictions)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validate_predictions = grid_search.predict(validate_data)\n",
    "validate_predictions = np.maximum(validate_predictions, 0)  # Ensure predictions are not lower than zero\n",
    "validate_mse = mean_squared_error(validate_target, validate_predictions)\n",
    "print(\"Mean Squared Error on Validation Set:\", validate_mse)\n",
    "\n",
    "# Expanding CV Fold Sizes\n",
    "initial_train_size = int(0.5 * len(train_data))\n",
    "test_size = int(0.1 * len(train_data))\n",
    "num_folds = (len(train_data) - initial_train_size) // test_size\n",
    "\n",
    "def train_and_evaluate_fold(fold):\n",
    "    start_train_size = initial_train_size + fold * test_size\n",
    "    end_train_size = start_train_size + test_size\n",
    "\n",
    "    X_train_fold = train_data[:end_train_size]\n",
    "    y_train_fold = train_target[:end_train_size]\n",
    "    X_test_fold = train_data[end_train_size:end_train_size + test_size]\n",
    "    y_test_fold = train_target[end_train_size:end_train_size + test_size]\n",
    "\n",
    "    model = SGDRegressor(**best_params, max_iter=1000, tol=1e-3)\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    fold_predictions = model.predict(X_test_fold)\n",
    "    fold_predictions = np.maximum(fold_predictions, 0)  # Ensure predictions are not lower than zero\n",
    "    fold_mse = mean_squared_error(y_test_fold, fold_predictions)\n",
    "    return fold, fold_mse\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(train_and_evaluate_fold)(fold) for fold in range(num_folds))\n",
    "\n",
    "for fold, fold_mse in results:\n",
    "    print(f\"Fold {fold + 1}/{num_folds}, MSE: {fold_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.010000000000000002, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=1.0, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=1.0, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=1.0, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=1.0, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=1.0, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=1.0, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=1.0, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=1.0, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=1.0, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=1.0, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=1.0, eta0=0.0001, learning_rate=constant, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=1.0, eta0=0.001, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "[CV] END alpha=1.0, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=1.0, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=1.0, eta0=0.01, learning_rate=constant, penalty=l1; total time=   0.3s\n",
      "Refined Best Parameters: {'alpha': 0.1, 'eta0': 0.0001, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "Refined Best Score: -7.004479383001739\n",
      "Refined Mean Squared Error on Test Set: 5.1798306803825565\n",
      "Refined Mean Squared Error on Validation Set: 8.083798482128838\n"
     ]
    }
   ],
   "source": [
    "# Define a more focused parameter grid for GridSearchCV based on the best parameters\n",
    "refined_param_grid = {\n",
    "    'alpha': [best_params['alpha'] * 0.1, best_params['alpha'], best_params['alpha'] * 10],\n",
    "    'eta0': [best_params['eta0'] * 0.1, best_params['eta0'], best_params['eta0'] * 10],\n",
    "    'learning_rate': [best_params['learning_rate']],\n",
    "    'penalty': [best_params['penalty']]\n",
    "}\n",
    "\n",
    "# Initialize a new GridSearchCV with the refined parameter grid\n",
    "refined_grid_search = GridSearchCV(sgd, refined_param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model with the refined parameter grid\n",
    "refined_grid_search.fit(train_data, train_target.values.ravel())\n",
    "\n",
    "# Get the best parameters and best score from the refined search\n",
    "refined_best_params = refined_grid_search.best_params_\n",
    "refined_best_score = refined_grid_search.best_score_\n",
    "\n",
    "# Print the refined best parameters and best score\n",
    "print(\"Refined Best Parameters:\", refined_best_params)\n",
    "print(\"Refined Best Score:\", refined_best_score)\n",
    "\n",
    "# Evaluate the refined model on the test set\n",
    "refined_predictions = refined_grid_search.predict(test_data)\n",
    "refined_mse = mean_squared_error(test_target, refined_predictions)\n",
    "print(\"Refined Mean Squared Error on Test Set:\", refined_mse)\n",
    "\n",
    "# Evaluate the refined model on the validation set\n",
    "refined_validate_predictions = refined_grid_search.predict(validate_data)\n",
    "refined_validate_mse = mean_squared_error(validate_target, refined_validate_predictions)\n",
    "print(\"Refined Mean Squared Error on Validation Set:\", refined_validate_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABM8AAALECAYAAAAfABc3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd1hT1/8H8PdN2HsJqCAgIGodaHHvvffWijirrXXUbatVa2tbV7W2Wm0VV2ttnVVbq1ZwrypqXXWjFVcVcCCa5Pz+8Jf7NZJAIHgj8H49D8+jd57cvJOQD+eeIwkhBIiIiIiIiIiIiCgDlbUbQERERERERERE9Lpi8YyIiIiIiIiIiMgEFs+IiIiIiIiIiIhMYPGMiIiIiIiIiIjIBBbPiIiIiIiIiIiITGDxjIiIiIiIiIiIyAQWz4iIiIiIiIiIiExg8YyIiIiIiIiIiMgEFs+IiIiIiIiIiIhMYPGMiIgAAJIkZfunbt261m42AODo0aOYMWMGunXrhhIlSkClUkGSJKxYscKs/X/++WfUrVsXnp6ecHZ2Rvny5fHFF1/g2bNn2W7LpEmT5OtTqFChTI+RlJQEGxsbeXtz2/s6iYuLk9tP+UfdunWzfP0PGzbM2s0skJR8vcXExECSJMTGxipyPnouPT0d48ePR3h4OOzt7SFJEoKDg63drBzRf0a8Lr8vZCYlJQVTp05FlSpV4O7uDltbW/j5+aFs2bLo2bMnvv32Wzx69Mjk/jdu3MCkSZNQq1Yt+Pv7w87ODq6urggPD0fHjh3x3XffITk5OcN+L36O6n9sbW3h5eWFEiVKoGPHjvjyyy9x+/btV/bY09PTMXfuXNSuXRteXl6wtbWFj48PSpUqhc6dO2POnDm4c+fOKzt/fnTlypU8/dqljGys3QAiIno99OrVK8OymzdvYuvWrSbXlyxZ8pW3yxxTpkzBhg0bcrTvsGHDMGfOHNjY2KB+/fpwcXHBn3/+iTFjxuDXX3/FH3/8AUdHxxwd++7du9i4cSM6dOhgdP3SpUuh1WpzdOzccuXKFYSEhCAoKAhXrlyxalvyqtfxGsbExGDp0qVYsmQJYmJicnyc8uXLIzIy0ui6ypUr5/i4ltAXjoQQVjk/vR5ex9ddbpkwYQKmT58OPz8/tGnTBk5OTvDx8bF2s/K1c+fOoWHDhrh+/Trs7e1RpUoVFClSBE+ePMGZM2ewYsUKrFixAjVq1ECZMmUy7D9jxgx8+OGHSE9Ph6OjIypVqoTChQtDo9EgMTER69evx5o1azBixAhs3rwZNWvWNNoO/e9aQgikpqbi2rVr2LhxI9asWYPRo0djzJgxmDhxImxtbXPtsd+6dQuNGjXCyZMnoVarUblyZQQGBkKn0+Gff/7BmjVr8PPPPyM0NBQtW7bMtfMS5TUsnhEREQAY7VkQFxcnF89e554HVatWxRtvvIGKFSuiQoUK6NOnD+Lj47Pcb/369ZgzZw5cXFwQHx+PihUrAnhe9Kpfvz727NmDCRMmYMaMGdluU1RUFI4cOYLFixebLJ4tWbIE9vb2iIiIwIkTJ7J9DqJXrW3btpg0aZK1m0EvOHPmjLWbQK/Y6tWrAQC7d+9GeHi4lVtjmcqVK+PMmTNwcnKydlMy9dZbb+H69euoV68efvrpJxQqVMhgfWJiIpYuXQoXF5cM+44ZMwZffPEF7Ozs8MUXX2Dw4MEZ/uiWkpKCJUuW4PPPP8f169dNtsPY71rJycmYN28epkyZgqlTp+L8+fP48ccfc60H6uDBg3Hy5Em88cYb2Lx5M4KCggzW3759Gz/++CP8/Pxy5XxEeRWLZ0RElOeNHTs2R/t9+umn8v76whkA+Pj44JtvvkGtWrUwb948TJgwAe7u7tk6dvny5aHT6bB161bcuHEDRYoUMVi/e/du/PPPP+jSpQtu3ryZo/YTUcHzuvT4pVcnMTERAPJ84QwAnJycXvvMXrx4EUeOHAEALFiwIEPhDACKFSuGCRMmZFi+fft2fPHFFwCeFz3btGlj9Bzu7u4YNmwYevXqhf/++y9b7fPw8MCHH36IyMhItG7dGj/99BNatGiBnj17Zus4xjx58kTuuT9r1qwMhTMA8PX1xdChQy0+F1FexzHPiIgox65fv4733nsP4eHhcHBwgLu7O2rUqIFvv/3W6O2IsbGxkCQJMTEx+O+///Duu++iWLFisLe3R1BQEIYPH4779+8r0vZ///0Xhw8fBgB07949w/qaNWsiMDAQ6enp2LJlS47O0adPH2i1WixdujTDusWLF8vbZGXVqlVo0KABvLy85GvVp08f/PPPP0a3T0pKwtChQ1GiRAk4ODjAyckJgYGBaNCggUEvupiYGISEhAAArl69mmHMFUsFBwdDkiRcuXIFv/32G+rWrQt3d3d4enqiZcuWOHnypLztDz/8gGrVqsHV1RUeHh5o3749Ll68mOGYL46f8/jxY4wfPx5hYWFwcHBAkSJF0LdvX/z7778m23T27Fn07t0bQUFBsLe3h5eXFxo0aCD39HiZfgy7SZMmITExEX379kVgYCBsbW0RExNj9jV88OABFi1ahPbt2yM8PBzOzs5wdnZG2bJl8cEHHxgdB+fla7hz5040btwYnp6ecHR0RMWKFbFs2TKD7fVjrOgz17t3b4P2vMpeZP/88w/efvtthIaGyu8HtWvXNjmW39WrV/H555+jfv368vuAh4cHatasiW+//RY6nc5ge/1zoffytdbfuvfic2aMqTGYXs7WxIkTUapUKTg5OWUYs+avv/5Cjx495HZ7eXmhSZMmJt8rzH1NmsPU6zO7Wcmu48ePo3379ihUqBAcHR1Rrlw5zJkzJ9Nbz1/Vdcqt9y5LP5Nu3LiB999/X86Jq6srKlWqhHnz5kGj0WTY/sUx5P7++2906dIFhQsXhlqtxqRJk+TnUH9L8ouP6eUeSdn9XHgxHxs2bED9+vXh5eUFSZIQFxdncD4AWLFiBSpXrgwXFxcUKlQI3bp1k4t6QgjMmzcPkZGRcHZ2ho+PD2JiYoyOyWXq9fbieFBCCCxcuBBvvvkmnJ2d4e7ujsaNG2P//v0mr/3ff/+NDh06wMfHB05OTihbtiy+/PJL6HQ6g8dqjlu3bsn/9vX1NWsfvalTpwIA2rVrZ7Jw9iJPT0+EhYVl6xx6LVu2RMeOHQFALthZ6t69e/LYrNl97ACg0Wjw5ZdfomzZsnBwcEChQoXQoUMHnDx50uD19aKcvkcDwNq1a9GvXz+UKVMGnp6ecHBwQEhICPr06YNz584ZPV5Wr7sXH8t3332HunXryq+rkJAQDBo0CNeuXTN5DTZt2oQ6derA1dUV7u7uqFWrVo6HEqHXnCAiIjJh586dAoAw9nFx6NAh4eXlJQCIYsWKiS5duoimTZsKBwcHAUA0adJEpKenG+yzZMkSAUC0bt1ahIaGCg8PD9G2bVvRrl074enpKQCIiIgIcfv2bYvaXadOHQFALF++3OQ2v/76qwAgvLy8TG7Trl07AUCMGjXK7HN/9NFHAoDo27evuHfvnnBwcBDh4eEG26SmpgpnZ2dRrFgxodVqTbZXp9OJ6OhoAUDY2NiI+vXri65du4oSJUoIAMLJyUn89ttvBvskJSWJIkWKyM9LmzZtRJcuXUStWrWEl5eXcHd3l7ddtGiR6NChgwAgnJ2dRa9evQx+zJFZRoKCggQAMXbsWCFJkqhRo4bo3Lmz3H4PDw9x4cIFMWrUKPnxdezYUQQGBgoAokiRIuLevXtGz1etWjVRtWpV4eTkJJo3by46deokChcuLAAIf39/8c8//2Roz6ZNm+R8RkREiK5du4r69esLtVotAIg+ffpk2Ef/fHbv3l14eXkJf39/0aFDB9G+fXsxYsQIs6/h7t27BQBRqFAhUbNmTdGlSxfRuHFj4e3tLQCIsLAwcffuXZPXcMKECUKSJPHmm2+Krl27iqpVq8rXffbs2fL2d+7cEb169RKhoaECgKhRo4ZBe9atW2fW86rP5EcffWTW9qtXr5avbcmSJUW7du1E/fr1hbOzswAgevfunWGfjz/+WAAQISEhokGDBqJr166iTp06ws7OTgAQ7du3FzqdTt5+3bp1olevXvLjfvla37lzx+A5M9V2fYbq1KljdHmVKlVEpUqVhLOzs2jWrJno0qWLaNiwobzdl19+KVQqlQAgIiMjRceOHUXNmjXldk+ePNnguNl5TZojq9ebuVkxh/56Dxo0SDg4OIjg4GA5u/rH27FjR4PnSe9VXqfceO8SwrLPpPj4eHmb4OBg0bp1a9GkSRN5WePGjcXTp0+NXs/+/fsLe3t7ERwcLDp37ixatWolZsyYIUaMGGEy47t37xZC5OxzQYj/5WPw4MECgIiKihLdunUTderUEbt27RJC/C9bY8eONXhPLlasmAAgAgMDxb1790Tnzp2Fg4ODaNq0qWjXrp3w9fUVAES5cuUyfO6ber1dvnxZABBBQUGiV69ewtbWVtSvX9/gM8Le3l4cOHAgw2OJi4sTjo6OAoAIDQ0VXbt2FY0aNRJ2dnaiS5cu8mO9fPmyWTm4du2a/NgnTZpk1j5CCHHv3j0542vXrjV7v5dl9jn6sg0bNsjbJiUlGazTZyc7r4H09HTh5OQkfwZqtVqz99VqtaJt27YCgLCzsxONGzcWXbp0EcHBwcLBwUG88847RtuT0/doIYRQq9XCyclJREVFifbt24vWrVuL4sWLy+8Fe/fuzbBPVq87IZ7/Xla3bl0BQLi4uIg6deqIjh07ioiICAFAeHt7i6NHj2Y49qxZs+Tno3LlyqJbt24iKipKABDvv/++nHHKH1g8IyIik0z9QvfkyRP5l9OBAwcafEG4ePGiCA4OFgDE+PHjDfbTf1EBIKpWrSr+++8/ed39+/dF9erVBQDRtWtXi9ptTvFs7ty58pc6U4YMGSJ/OTTXi8UzIYTo1q2bACB/ORHi+Rc/AGLixImZtnf+/PkCgPDx8RHHjh2Tl+t0Ovk8Hh4eBl/sJk+eLACIAQMGZPhC+/TpU7F9+3aDZS9+gckJc4pn9vb2BufVaDSiU6dOAoAoU6aM8Pb2FgkJCfL6R48eyVmYOnWqyfOFhYWJq1evyuvS0tLkL9RVq1Y12O/mzZvC3d1dPuaL1+bw4cPyF96FCxca7Ke/zgDEW2+9JZ48eZLhcZpzDa9duya2b9+e4YvJo0eP5C/C77zzjslraGtrK3799VeDdfrXk7u7u3j8+LHBOv2XhSVLlphsU2ayUzw7ceKEsLe3Fw4ODmLNmjUG665cuSLKli0rAIilS5carDt06JA4efJkhuP9+++/onz58gKAWL16dYb1WX3JtLR4pi8CvPzFVAghfv/9dyFJkvDx8RHx8fEG606cOCECAgIEABEXFycvz+5rMitZvd6ym5XMvFjIeeedd8SzZ8/kdX///bcoVKiQACAWLFhgsJ8S18nS9y4hcv6ZlJSUJLy9vYUkSeKbb74xeF3fvXtX1K9f32iB8MXrOXbsWJOFiswynpPPBSH+lw+1Wi02bNiQ6Xlffk9+/PixqFmzpgAgypYtK0JDQ8WVK1fk9Xfu3BFhYWECgFixYoXBMbMqnumfw3PnzsnrNBqN6NOnj1yEfNHjx49F0aJFBQAxYsQIg2t46tQp4efnJx/X3OKZEEK0adNG3q906dJi5MiR4qeffhIXLlwwuc+OHTvkfa5du2b2uV6WneLZ9evX5W1ffk3kpHgmhBBDhw6VjxkcHCzee+89sXz5cnHq1CmjhXG9efPmCQDCz89PnD59Wl7+7NkzMWjQIIMi8IssKZ6tWrVKPHz40GCZTqcTX3/9tQAg3njjjQxtNud11717dwFAtGzZUty6dctg3ezZswUAER4eLjQajbz8+PHjQq1WC5VKJX7++WeDfVasWCEkSWLxLJ9h8YyIiEwy9Qvd8uXLBfC8Z5CxYsIvv/wiAAhXV1eRlpYmL3/xi8qLv/TrnThxQkiSJFQqlUW/iJpTPPvkk08E8Lxnjinjx483+st7Zl4unm3btk0AEDExMfI2VatWFZIkyb/Ym2qvvvfQ3LlzM5xHp9OJcuXKCQDik08+kZfr/9Jr7l/BlSieGeu5d/ToUXm/r7/+OsP6NWvWCACiXr16Js+3fv36DPvdunVL/iv6i3+B1vdyevPNN40+jhkzZsi/HL9I/3x6eXmJ5ORko/taeg0fPXokbGxsRKFChTKs01/D999/3+i+JUuWzFCcFSL3imemfl58rF26dBEA5L/gv+zQoUOZXntjtm7dKgCITp06ZVinRPHs5eupV6VKFQFA/PLLL0bXr169WgAQHTp0kJdl9zWZlaxeb9nNSmb0OSpcuLDBe7neV199ZfR1o8R1yu3iWXY+k8aMGSOA5724jLl+/bqwtbUVhQoVMvgir7+eJUqUMPgS/rLMMp6TzwUh/pcPYz1sXz6vsffktWvXyus3b96cYf3MmTMFkLGXqTnFs40bN2Y4XlJSkgCe//HlxT/QLVu2TH7eX+7ZJ8T/CjrZLZ6lpqaKt956Sy54vPgTEBAgxo0bl6En9E8//SRvY+x3ISGEGDx4cIaekdOmTTPYJjvFsydPnsjb/vTTTwbrxo4dKyIiIsTYsWPNftxCPC9ODxs2TNja2mZ47D4+PuLdd98V169fz7CfvmA6f/78DOvS0tKEv79/rhfPMlOtWjUBQJw6dcpgeVavu9OnTwtJkkSRIkVEamqq0WM3b95cADD4w0S/fv0EANGlSxej++gLsiye5R8c84yIiLJNPz5K165dYW9vn2F9+/bt4enpiQcPHuCvv/7KsL58+fKIjIzMsLxs2bKoUKECdDoddu3aldvNtooGDRogKCgIP//8Mx4+fIgzZ87gwIEDqFevXoZxlF50/fp1ecwv/dT1L5IkCb179wYA7Ny5U15euXJlAM8nQVi7di0ePnyYi48mZ5o3b55h2YsDYWe2/saNG0aP6eHhgdatW2dY7uvri6ZNmwL4X05f/LexawkAffv2BQCcP3/e6DkbNmyY7UkjjNm3bx8+//xzvPvuu+jduzdiYmLwzjvvwM7ODnfu3DE5vlKrVq2MLi9VqhQAZDrOmyXKly+PXr16ZfjRj7uj0+nw22+/AQC6dOli9BhRUVFwcXHBsWPH8OTJE4N16enp+PXXXzFx4kQMHDhQvibffvstAJgcw+ZV8vX1Ra1atTIsv3v3Lg4dOgRHR0eTz4d+jJ59+/bJy5R+Tb6KrHTu3BkODg4ZlutfTy++bvLKdXpRdj+TNm/eDMB05osWLYrw8HDcuXMH58+fz7C+bdu2UKvV2W5nTj8XXqR/7WYms/dkGxsbNG7c2OR6U+/ZptjY2Mjv2S/y9/eHp6cn0tPTDQbY18+k3alTJ9ja2mbYr0ePHtk6v56rqyuWL1+OixcvYtasWejYsSOKFy8O4Pl1nzZtGiIjI80eR03vxx9/xNKlSw1+fv/99xy1EYDBWJAvj+83bdo0nD17FtOmTcvWMW1tbTF79mwkJiZi/vz56N69O0qWLAlJknD37l18/fXXKFeunMHvc//++y8uXLgA4PlMpS9zcHBA586ds9UOc124cAHz5s3DsGHD0LdvX3n8Uf3YdaY+N0y97rZs2QIhBJo1awZXV1ej+xp7z9L/XmHs8QOmf9+gvIuzbRIRUbbpv3zpB2x+mSRJCAkJwf37941+UTO1n37d0aNHM53KPTfof0F69OiRyW30X97c3NxyfB79YLmTJ0/GTz/9hLNnzwLIeqIA/XXz9vY2ef7Q0FCDbQGgZ8+e2LZtG1auXIkOHTpArVajdOnSqFmzJjp27Ij69evn+LHkVLFixTIsc3FxyXS9/vl5udiipx8Q2hh9vl7MUFaZ9fDwgJeXF+7du4fr169nmB01s0KnOW7fvo0OHTpgz549mW6XmpoKT0/PDMuNXSPgf9k0dZ0s1bZt20wnGfjvv/+QmpoKAAgMDMzyeP/99x+KFi0KADhw4AC6dOkiD0JujP7YSjL1XF++fBlCCKSlpRn9o8GL7ty5I/9b6dfkq8iKqdeNq6srvL298d9//8mvm9flOu3ZswffffddhuVt27ZF27ZtzXp8+nUvfyZdunQJAIwWWV92584dlChRwmBZTt9Pcvq5kN1zZ/aeXbhwYdjYZPwKmdV7timFCxc2WgQDnmf2/v37BsfUPw+mHoeHhwfc3d2RkpKSrXbohYSEYPjw4Rg+fDiA5xNSfP/99/jiiy+QmJiId999Vy6e+vj4yPvduXMHAQEBGY539+5d+d8rVqyweIbMF4/n5eVl0bFe5u/vj4EDB2LgwIEAnk+k8MMPP2Dy5Mm4d+8eoqOjcerUKQD/ex58fHwMPs9flNnrKie0Wi0GDx6Mb7/9Vp5UwxhTnxumMqN/PX///ff4/vvvM23Di+9Z+mtg6nHm9uMn62PxjIiIXkuZ/WKUG/S/RGU2g5J+naWFk969e2PKlClYuHAhrl69Cnd3d7Rv396iY5qiUqmwYsUKjB8/Hps3b8bevXuxd+9ezJ8/H/Pnz0erVq2wbt26HPV6sKRNlqzPqdzMkKOjo0X79+vXD3v27EG1atUwefJklC9fHp6envKXxiJFiiApKclkm1/VNbLUi70gzPkru76Y8vjxY7Rt2xa3bt1C7969MWjQIISFhcHNzQ1qtRr//PMPIiIiXsn7wMuzeL7M1HOt38/FxQUdOnQw+3xKvyatlRX9c/W6XKcLFy4Ynek4ODg4Q/HMHC9mUf8YO3bsCGdn50z38/b2zrDM0vcTS5hz7swylNv5yunxMptVNTdmi9YLCgrClClT4Onpiffffx9//PEH0tLS4OjoiMjISKhUKuh0Ohw5csRo8Sy3HT16VP532bJlX+m5/Pz8MHz4cAQHB6N9+/Y4ffo0zp8/b9Bz/FUw9R49Z84cLFiwAP7+/pg1axaqV68OPz8/uVds9+7d8eOPP5r83MjqvT0yMhLly5fPtG1VqlQx92FQPsTiGRERZZu+54j+r3XGXL582WBbY+uM0d8S8ap/Ca1QoQKA5z1hLl++bPQvhEeOHAEAVKxY0aJzBQUFoX79+tixYwcAYODAgVl+gdFfN33PHmO9DPTX39g1Ll26NEqXLo1Ro0ZBCIE///wT3bt3x6+//oply5bJt/bkVZndOmMsQ0WLFsXZs2dNZjYlJQX37t2Tt81Njx49wpYtW6BSqbBlyxZ4eHhkWH/z5s1cPadSfHx84OjoiLS0NMyYMcOgJ0Zmdu3ahVu3bqFixYpYvHhxhvXGbnUzl52dHQDgwYMHRtdfvXo1R8fV96yTJAmLFy/O9pf+vPyaNPWe/eDBA/mWOv3r7XW5TvpbucyR3c+kwMBAnD9/HmPGjEFUVJRZ58gNln4u5Af6x2XqMyAlJQXJycm5fl79raoajQbJyclwdHSEl5cXatSogd27d2PFihU5Kspm14oVKwA8v9XY19f3lZ8PgMFtunfv3kV4eLj8PNy9excPHz402vvM1HOU0/fo1atXAwC+/fZbo8M25PRzQ/+eVaNGDcybN8/s/YoWLYqLFy/iypUreOONNzKsz+4tvvT6ez3/jElERK81/dgPP/30k9FbNNatW4f79+/D1dUVb775Zob1J06cwIkTJzIsP3XqFI4ePQqVSoXatWvnertfFBAQgEqVKgEAfvjhhwzr9+zZg2vXrsHe3t7o+C/ZNWDAAHh7e8Pb21seXyur9ulvv4mNjc2wXgghL69Xr16mx5IkCQ0aNED37t0BAAkJCfI6/S+xGo3GjEfx+khOTsavv/6aYfmdO3fk8WT0OX3x38Z6ogCQCzgvfikwV1bXMCUlBVqtFm5ubhkKZ8DzL0O53cNKqedVrVajUaNGAP73xcYc+kKlqVsM9V8QjdH31jP12PTP35kzZ4yu199ylV1FihRBuXLl8ODBA4vGLAIyf02+jn7++Wekp6dnWL58+XIAQFhYmHzdlbpOuZnx7H4mNWvWDED2Mp8bcvNzIa/SPw8///yz0efe2Od5Vsx5/9XfXm5vb2/wR4IPP/wQALB27docv7eYa/PmzVizZg0AYPTo0blyzOw8duB/768BAQHymHDGrnl6ejp+/vlno8fL6Xu0/nMjKCgow7pTp07l+H1U/3reuHFjtm47rlOnDgBg5cqVRtcvW7YsR+2h1xeLZ0RElG2dOnVCsWLFcOPGDbz//vsGv8BevnwZI0aMAAC89957RgeZFkJg0KBBBoOjp6SkYNCgQRBCoEOHDmaNn2Sp8ePHAwA+++wzg1sh/vvvP7zzzjsAgMGDB+fKQPGdO3fG3bt3cffuXbN7KowcORIA8PHHH+P48ePyciEEpk6dioSEBHh4eKB///7yumXLlhmdpOHBgwfy4LYv/uJZqFAh2NnZ4ebNm/IvpnnFiBEjDMYhSk9Px7vvvotHjx6hcuXKqFGjhryuf//+cHNzw9GjR/Hpp58afGE4duwYpk6dCgAYNWpUttuR1TX08/ODp6cnkpOT5WKD3oEDBzBu3LhsnzMr+l4y+vFpXqWPPvoIdnZ2GDVqFJYuXWr0lpu///4ba9eulf+vH7x+x44dOH36tMG2CxcuxE8//WTyfFk9tvr160OlUmHr1q3y4OLA89fN3Llz5S+fOaHPSe/evY0Wb4UQOHjwIP744w95WXZfk6+jGzduYOTIkdBqtfKyM2fOYMqUKQAgjw+lp8R1ys33rux+Jo0aNQoeHh6YNWsWZs6ciadPn2Y45uXLlzMtAudUTj4X8pNOnTqhcOHCuHLlCj744AOD95uzZ8/KmcyOEydOoF69eli3bp3R5/L48eMYOnQoAKBDhw4GY7Q1btwYI0aMkHMya9YspKWlZThGenq63Js9u5KTk/HJJ5+gffv2EEKge/fu6NatW4btxo0bh5IlS2brMyUlJQUVK1bE8uXLjU7ScenSJXmM1urVqxv8wWPYsGEAgEmTJsnjuQLPxyYbOXKkyckjcvoerf/c+Prrrw2e96SkJERHR+e4kF6hQgV06NAB165dQ/v27Y32GHv06BFWrlwpT0oAPP8dV61WY/Xq1Vi3bp3B9qtWrcL69etz1B56jb36CT2JiCivymz69EOHDgkvLy95Gu4uXbqI5s2bCwcHBwFANGnSRKSnpxvss2TJEgFAtG7dWhQvXlx4eHiIdu3aifbt28vHCg8PF7du3cpWOzdt2iSqVKki/7i6ugoAIjQ01GC5MUOGDBEAhK2trWjatKno0KGD8PDwEABEjRo1xOPHj7PVFv0U7H379jV7nzp16ggAYvny5QbLdTqd6NmzpwAgbGxsRIMGDUS3bt1ERESEACAcHR3Fli1bDPbRT41epEgR0bx5c9GjRw/RvHlz4e7uLgCIMmXKZJiKvWPHjgKACAwMFN26dRN9+/Y1u/2ZZSQoKEgAEJcvXza6r6n9hBDi8uXLRqd415+vWrVqokqVKsLJyUm0bNlSdO7cWRQpUkQAEL6+vuLs2bMZjvnrr7/K+SxZsqTo1q2baNCggbCxsREARO/evTPso38+P/roo0yvQ1bXcPbs2fLjrVKliujWrZuoUaOGkCRJ9OzZ0+S1yuoa9urVSwAQS5YsMVh+/PhxoVKphEqlEg0bNhS9e/cWffv2FRs2bMj0cejpM5nV49ZbvXq1cHJyEgBEQECAaNy4sejRo4do1qyZCAgIEABEly5dDPbRZ9XOzk40btxYdO3aVZQsWVJIkiQ++OADo8+/EEKMHDlSABA+Pj6ic+fO8rW+e/euvM3QoUMFAKFWq0XdunVF+/btRWhoqLC1tRVjx44VAESdOnUMjqvP1svLXzZnzhw5M2FhYaJFixaie/fuolGjRsLX11cAEGPGjMnwOLPzmsxMTl9vprKSGf0+AwcOFA4ODiIkJER07dpVNGnSRNjZ2QkAol27dkKn02XYV4nrZMl7lxCWfSbFx8cLHx8f+T2nfv36okePHqJly5YiNDRUfq0bu55ZPQeZvTfm5HNBiKzzkdV5Tb0n65l6/ZhantXxMmvzjh075PfysLAw0bVrV9G4cWNhZ2cnOnXqJIoVKyYAiH///dfksV907Ngx+bE7OzuLmjVrii5duoh27dqJyMhIeV1kZKS4ffu20WNMmzZNfk04OTmJOnXqiK5du4pu3bqJunXrChcXFwFAuLq6innz5hm9RgBEr169RK9evUR0dLRo3769iIqKEra2tvLvKRMnThRPnz412gZ9vnr16mXW4xZCiPv378vntre3F5UrVxadOnUSHTt2FFWqVBEqlUp+nv755x+DfbVarWjVqpX8Pt6kSRPRtWtXERISIhwcHMSgQYNMticn79EHDhyQr3FYWJjo3LmzaNq0qXB0dBRvvPGGaNeundHXlzmvu9TUVNGgQQP5sVSqVEl07txZdOrUSVSqVEk+75kzZwz2++KLLww+27t37y4qVaokAIjhw4dnmXHKW1g8IyIikzIrjAghRGJionj33XdF8eLFhZ2dnXB1dRXVqlUT8+fPF8+ePcuwvf6LSq9evcTt27fF22+/LQICAoSdnZ0IDAwUQ4YMEf/991+226k/blY/pvz000+idu3aws3NTTg6OooyZcqIzz77LEPxzxy5WTzT++GHH0TdunWFh4eHsLW1FYGBgSImJsZogWjXrl1i2LBhonLlysLf31/Y2dkJf39/Ua1aNfHVV1+Jhw8fZtjnv//+E2+//bYoVqyY/Eu6uX9fs1bxrE6dOuLhw4di1KhRIiQkRNjZ2Qk/Pz8RExMjEhMTTbb39OnTolevXiIgIEDY2toKDw8PUa9ePbFq1Sqj25tbPDPnGq5fv15Ur15deHh4CBcXFxEVFSW++eYbodPpcr14JoQQ69atEzVq1BCurq5CkqRsFcOyWzwT4vlzNnz4cFGmTBnh7OwsHBwcRFBQkKhbt6747LPPxIULFwy2f/r0qZg+fbooW7ascHJyEl5eXqJx48bijz/+yPSLdVpamhg9erQICwuTv9C8fI10Op2YOXOmKFWqlLCzsxNeXl6iVatW4q+//sr2l3xjTp48KQYMGCDCw8OFg4ODcHJyEsWLFxdNmjQRc+fONfjSnpPXZGasUTxbsmSJOHr0qGjVqpXw9vYW9vb24o033hCzZs0y+l6v96qvkyXvXUJY/pl069YtMWHCBFGxYkXh6uoq7OzsREBAgKhevbr46KOPxIkTJ0xez8yY8ziy87kgRP4qngnx/A8E7dq1E15eXsLBwUGULl1aTJ8+XaSnpws7OzuhUqlEWlqayWO/6NmzZyI+Pl5MnDhR1K1bVxQvXlw4OTkJOzs7UaRIEdG0aVOxcOFCk0UrvWvXromJEyeKGjVqiEKFCgkbGxvh7OwsihcvLtq1aye+/fZbce/evQz7vfg5qv9Rq9XCw8NDhIWFifbt24vZs2ebLNzp5aR4ptPpxMGDB8Wnn34qGjduLMLDw4Wrq6uwtbUVvr6+ol69emLWrFkm36eePXsmZs6cKUqXLi3s7e2Ft7e3aNOmjUhISDB4fRk7b3bfo4UQ4sSJE6J169aicOHCwsHBQYSHh4vRo0eL1NRUk68vc193Wq1W/PDDD6J58+bCz89P2NraCm9vb1GmTBnRu3dvsW7dOqMZ2LBhg6hZs6ZwdnYWLi4uonr16uKXX34xK+OUt0hCvOLpzIiIiP5fbGwsevfujV69ehkdr4UoK3FxcahXrx7q1Kkj38pFRJQT/EzKf3bt2oU6deqgbNmyRsexI+Xw9UX5Dcc8IyIiIiIiojzhzp07RmdI/fvvv+Wx3l7n2WuJKG+ysXYDiIiIiIiIiMxx6tQp1KtXD6VLl0bx4sXh6OiIy5cv4+jRo9DpdGjUqBHee+89azeTiPIZFs+IiIiIiIgoTyhRogTeffddxMfHY+/evXjw4AFcXV1RvXp1dO/eHf3794eNDb/mElHu4phnREREREREREREJnDMMyIiIiIiIiIiIhNYPCMiIiIiIiIiIjKBN4MTkdXodDrcuHEDrq6ukCTJ2s0hIiIiIiKifE4IgQcPHqBIkSJQqczrU8biGRFZzY0bNxAYGGjtZhAREREREVEBc+3aNQQEBJi1LYtnRGQ1rq6uAJ6/abm5uVm5Na8/jUaDY8eOoUKFCpxFihTBzJHSmDlSGjNH1sDckdKYOUOpqakIDAyUv4+ag1eNiKxGf6umm5sbi2dm0Gg0cHZ2hpubGz/0SBHMHCmNmSOlMXNkDcwdKY2ZMy47QwdxwgAiIiIiIiIiIiITJCGEsHYjiKhgSk1Nhbu7O1JSUtjzzAxCCGi1WqjVak6wQIpg5khpzBwpjZkja2DuSGnMnKGcfA9lzzMiojzk6dOn1m4CFTDMHCmNmSOlMXNkDcwdKY2ZswyLZ0REeYRWq8WJEyeg1Wqt3RQqIJg5UhozR0pj5sgamDtSGjNnORbPiIiIiIiIiIiITGDxjIiIiIiIiIiIyAQWz4iI8hC1Wm3tJlABw8yR0pg5UhozR9bA3JHSmDnLcLZNIrIazrZJRERERERESuJsm0RE+ZgQAsnJyeDfPEgpzBwpjZkjpTFzZA3MHSmNmbMci2dERHmEVqvF2bNnOUsOKYaZI6Uxc6Q0Zo6sgbkjpTFzlmPxjIiIiIiIiIiIyAQWz4iIiIiIiIiIiExg8YyIKI+QJAmOjo6QJMnaTaECgpkjpTFzpDRmjqyBuSOlMXOW42ybRGQ1nG2TiIiIiIiIlMTZNomI8jGdTofbt29Dp9NZuylUQDBzpDRmjpTGzJE1MHekNGbOciyeERHlETqdDpcuXeKHHimGmSOlMXOkNGaOrIG5I6Uxc5Zj8YyIiIiIiIiIiMgEFs+IiIiIiIiIiIhMYPGMiCiPkCQJ7u7unCWHFMPMkdKYOVIaM0fWwNyR0pg5y3G2TSKyGs62SUREREREREribJtERPmYTqfD9evXOdAnKYaZI6Uxc6Q0Zo6sgbkjpTFzlmPxjIgoj+CHHimNmSOlMXOkNGaOrIG5I6Uxc5Zj8YyIiIiIiIiIiMgEFs+IiIiIiIiIiIhMYPGMiCiPUKlUKFSoEFQqvnWTMpg5UhozR0pj5sgamDtSGjNnORtrN4CIiMyjUqkQ0XuytZtBRERERESUgWbXMms34ZVh2ZGIKI/Q6XRoXiEUNirJ2k2hAsJGJTFzpChmjpTGzJE1MHekNGbOciyeERHlETqdDuWD/NjdmhSjUqmYOVIUM0dKY+bIGpg7UhozZzleOSIiIiIiIiIiIhNYPCMiIiIiIiIiIjKBxTMiojxCpVJh99lr0Op01m4KFRBanY6ZI0Uxc6Q0Zo6sgbkjpTFzlpOEEMLajSCigik1NRXu7u5ISUmBm5ubtZuTJ9jUjrZ2E4iIiIiIiDLIK7Nt5uR7KHueERHlEVqtFl2rl4atmm/dpAxbtYqZI0Uxc6Q0Zo6sgbkjpTFzluOVIyLKI4QQCPH1gCRximlShiRJzBwpipkjpTFzZA3MHSmNmbMci2dEREREREREREQmsHhGRERERERERERkAotnRER5hEqlwpZjF6DRcpYcUoZGq2PmSFHMHCmNmSNrYO5Iacyc5Wys3QAiIjKPSqXC8au3rd0MKkB0QjBzpChmjpTGzJE1MHekNGbOcux5RkSUR2i1WvRvEMlZckgxtmoVM0eKYuZIacwcWQNzR0pj5izHK0dElEcIIeDj6sRZckgxkiQxc6QoZo6UxsyRNTB3pDRmznIsnhERERERERER0SsXFxcHSZKM/hw4cCDTfc+dO4fhw4ejevXqcHBwgCRJuHLlisntHzx4gNGjRyMkJAT29vYoWrQoOnbsiMePH2e73RzzjIiIiIiIiIiIFDNkyBBUqlTJYFlYWFim++zfvx9z585F6dKlUapUKSQkJJjcNiUlBXXq1MH169cxYMAAhIWF4c6dO9i9ezfS09Oz3V4Wz4iI8gi1Wo1V+05Do9VauylUQGi0WmaOFMXMkdKYObIG5o6U9jpmrlatWujYsWO29mndujWSk5Ph6uqKGTNmZFo8GzduHK5evYqjR48iJCREXj5mzBikpqZmu728bZMoHwgODsaXX36Zo32vXLkCSZIyfeOh14MkSbh8Oxk6Ye2WUEGhE2DmSFHMHCmNmSNrYO5Iaa9r5h48eACNRmP29l5eXnB1dc1yu+TkZCxZsgQDBgxASEgInj59mqPeZi9i8YwKnJiYGPmeajs7O4SFhWHKlCnQaDTy/deenp548uSJwX6HDx+W99N78uQJYmJiULZsWdjY2KBt27ZGzxkXF4eKFSvC3t4eYWFhiI2NNbu9u3btQqtWrVCkSBFIkoT169fn4FFTfqDRaDCiZRXY2ait3RQqIOxs1MwcKYqZI6Uxc2QNzB0p7XXMXO/eveHm5gYHBwfUq1cPR44cybVj79mzB0+ePEFYWBg6duwIJycnODo6okaNGjnuNMLiGRVITZs2RVJSEs6fP48RI0Zg0qRJmD59urze1dUV69atM9jn+++/R7FixQyWabVaODo6YsiQIWjYsKHRc12+fBktWrRAvXr1kJCQgGHDhqFfv37YunWrWW199OgRypcvj6+//jqbj5Lyo9fpA48KBmaOlMbMkdKYObIG5o6U9rpkzs7ODh06dMCcOXOwYcMGTJ06FSdPnkStWrVw7NixXDnH+fPnATy/dfPatWtYtmwZvv76a1y8eBH169fHzZs3s31MFs+oQLK3t4e/vz+CgoIwaNAgNGzYEBs3bpTX9+rVC4sXL5b/n5aWhlWrVqFXr14Gx3F2dsb8+fPRv39/+Pv7Gz3XggULEBISgpkzZ6JUqVIYPHgwOnbsiNmzZ5vV1mbNmmHq1Klo165dpts9fvwYffr0gaurK4oVK4aFCxeadfyXabVa9O3bFyEhIXB0dERERATmzJljsI1Go8GQIUPg4eEBb29vjBkzBr169TLZ846IiIiIiIioevXq+OWXX9CnTx+0bt0aY8eOxYEDByBJEsaNG5cr53j48CGA58Pe7NixA927d8egQYOwfv163L9/H4sWLcr2MVk8IwLg6OiIp0+fyv/v2bMndu/ejcTERADAmjVrEBwcjIoVK2b72Pv378/QK61JkybYv3+/ZY1+ycyZMxEVFYVjx47hnXfewaBBg3Du3LlsH0en0yEgIAA///wzTp8+jYkTJ2L8+PFYvXq1vM3nn3+OlStXYsmSJdi7dy9SU1PNup00PT0dqampBj9ERERERERUcIWFhaFNmzbYuXMntLkwqYGjoyMAoFWrVnBxcZGXV61aFSEhITh48GC2j8niGRVoQghs374dW7duRf369eXlvr6+aNasmTw22eLFi9GnT58cnePmzZvw8/MzWObn54fU1FSkpaXluO0va968Od555x2EhYVhzJgx8PHxwc6dO7N9HFtbW0yePBlRUVEICQlBjx490Lt3b4Pi2VdffYVx48ahXbt2KFmyJObNmwcPD48sjz1t2jS4u7vLP4GBgdluX0GmVquxaMcxPNO8PrPkUP72TKNl5khRzBwpjZkja2DuSGl5IXOBgYF4+vQpHj16ZPGxihQpAgAZvocDz7/rJycnZ/uYLJ5RgbRp0ya4uLjAwcEBzZo1Q5cuXTBp0iSDbfr06YPY2FhcunQJ+/fvR48ePazTWDOVK1dO/rckSfD398ft27dzdKyvv/4ab775JgoVKgQXFxcsXLhQ7oWXkpKCW7duoXLlyvL2arUab775ZpbHHTduHFJSUuSfa9eu5ah9BVlq2lO8ZpPkUD4mwMyRspg5UhozR9bA3JHS8kLmLl26BAcHB4OeYjml/27677//Zlh348YN+Pj4ZPuYLJ5RgaQfvP/8+fNIS0vD0qVL4ezsbLBNs2bNkJaWhr59+6JVq1bw9vbO0bn8/f1x69Ytg2W3bt2Cm5ub3J00N9ja2hr8X5Ik6HS6bB9n1apVGDlyJPr27Ys//vgDCQkJ6N27t8FtrTllb28PNzc3gx8yn1arfe1myaH87XWcmYnyN2aOlMbMkTUwd6S01ylzd+7cybDs+PHj2LhxIxo3bgyV6nmZKjExEWfPns3ROSIiIlC+fHls2LABd+/elZf/8ccfuHbtGurVq5ftY9rkqCVEeZyzszPCwsIy3cbGxgbR0dH44osv8Ntvv+X4XNWqVcOWLVsMlm3btg3VqlXL8TFfpb1796J69ep455135GUXL16U/+3u7g4/Pz8cPnwYtWvXBvC8qHP06FFERkYq3VwiIiIiIiLKI7p06QJHR0dUr14dvr6+OH36NBYuXAgnJyd89tln8nbR0dGIj4+HEP/rL5eSkoKvvvoKwPPvrQDkIYQ8PDwwePBgedvZs2ejUaNGqFmzJt5++22kpKRg1qxZKFGiBPr27YuJEydmq90snhFl4uOPP8aoUaMy7XV2+vRpPH36FPfu3cODBw+QkJAAAHIhaeDAgZg3bx5Gjx6NPn364M8//8Tq1auxefNms9rw8OFDXLhwQf7/5cuXkZCQAC8vLxQrVizHj82U8PBwLFu2DFu3bkVISAiWL1+Ow4cPIyQkRN7mvffew7Rp0xAWFoaSJUviq6++wv379yFJUq63h4iIiIiIiPKHtm3bYuXKlZg1axZSU1NRqFAhtG/fHh999FGWHVzu37+PCRMmGCybOXMmACAoKMigeFavXj38/vvvmDBhAsaPHw8nJye0bdsWX3zxBZycnLLdbhbPiDJhZ2eX5f3QzZs3x9WrV+X/V6hQAQDkCnlISAg2b96M4cOHY86cOQgICMB3332HJk2amNWGI0eOGHQrff/99wEAvXr1kic0yE1vv/02jh07hi5dukCSJHTr1g3vvPOOQe+7MWPG4ObNm4iOjoZarcaAAQPQpEkTqNXW7wZMREREREREr6chQ4ZgyJAhWW4XFxeXYVlwcLBBT7SsNGzYEA0bNsywPDU11exj6EkiO2cmIjJCp9OhVKlS6Ny5Mz7++GOz90tNTYW7uztSUlI4/pkZhBBwbtAHT1/jWXIo/7GzUTNzpChmjpTGzJE1MHekNCUyp9m17JUeP7fk5HsoJwwgomy7evUqFi1ahH/++QcnT57EoEGDcPnyZXTv3t3aTcv33BztwJtjSSkSmDlSFjNHSmPmyBqYO1IaM2c5Fs+IrCgxMREuLi4mfxITEy0+x6effmry+M2aNcvRMVUqFWJjY1GpUiXUqFEDJ0+exPbt21GqVCmL20umabVa9G9QAbavwSw5VDDY2qiZOVIUM0dKY+bIGpg7UhozZzmOeUZkRUWKFJEnGDC13lIDBw5E586dja5zdHTM0TEDAwPl2U2IiIiIiIiI8jMWz4isyMbGJssZRSzl5eUFLy+vV3oOIiIiIiIiovyKt20SEeUhHFiWlMbMkdKYOVIaM0fWwNyR0pg5y3C2TSKyGs62mX02taOt3QQiIiIiIqIMONsmERFZnRACIb4eUHGaHFKISgIzR4pi5khpzBxZA3NHSmPmLMfiGRFRHqHVatG1emnYqDlLDinDRq1m5khRzBwpjZkja2DuSGnMnOVYPCMiIiIiIiIiIjKBxTMiIiIiIiIiIiITWDwjIsojJEnC3QePwXleSClCCGaOFMXMkdKYObIG5o6UxsxZjrNtEpHVcLbN7ONsm0RERERE9DribJtERGR1Op0O5YN8oZI4TQ4pQyVJzBwpipkjpTFzZA3MHSmNmbMci2dERHmETqdD8wphsFHzrZuUYaNWMXOkKGaOlMbMkTUwd6Q0Zs5yvHJEREREREREREQmcMwzIrIajnmWPRqNBkeOHEFUVBRsbGys3RwqAJg5UhozR0pj5sgamDtSGjNniGOeERHlY5Ikwd3dHRLHKiCFMHOkNGaOlMbMkTUwd6Q0Zs5y7HlGRFbDnmdERERERESkJPY8IyLKx3Q6Ha5fvw6dTmftplABwcyR0pg5UhozR9bA3JHSmDnLsXhGRJRH8EOPlMbMkdKYOVIaM0fWwNyR0pg5y7F4RkREREREREREZAKLZ0RERERERERERCaweEZElEeoVCoUKlQIKhXfukkZzBwpjZkjpTFzZA3MHSmNmbMcZ9skIqvhbJtERERERESkJM62SUSUj+l0Oly8eJEDfZJimDlSGjNHSmPmyBqYO1IaM2c5Fs+IiPIInU6HO3fu8EOPFMPMkdKYOVIaM0fWwNyR0pg5y7F4RkREREREREREZIKNtRtARETm27wvAfXHf4unGq21m0IW0OxaZu0mEBERERGRmdjzjIgoj1CpVNh99hq07G5NClGpVAgICODMTKQYZo6UxsyRNTB3pDRmznLseUZElEeoVCrsOXvN2s2gAkT/ixaRUpg5UhozR9bA3JHSmDnLsexIRJRHaLVadK1eGrZqvnWTMrRaLc6cOQOtlrcJkzKYOVIaM0fWwNyR0pg5y/EbGBFRHiGEQIivByRJsnZTqIAQQiAlJQVCCGs3hQoIZo6UxsyRNTB3pDRmznIsnhEREREREREREZnA4hkREREREREREZEJLJ4REeURKpUKW45dgEbL2TZJGSqVCsWLF+fMTKQYZo6UxsyRNTB3pDRmznKcbZOIKI9QqVQ4fvW2tZtBBYhKpYKvr6+1m0EFCDNHSmPmyBqYO1IaM2c5lh2JiPIIrVaL/g0iOdsmKUar1eL48eOcmYkUw8yR0pg5sgbmjpTGzFmO38CIiPIIIQR8XJ042yYpRgiBtLQ0zsxEimHmSGnMHFkDc0dKY+Ysx+IZERERERERERGRCSyeERERERERERERmcDiGRFRHqFWq7Fq32loOFYBKUStVqNkyZJQq9XWbgoVEMwcKY2ZI2tg7khpzJzlONsmEVEeIUkSLt9OtnYzqACRJAkeHh7WbgYVIMwcKY2ZI2tg7khpzJzl2POMiCiP0Gg0GNGyCuxs+BejgiI9PR1jxoxBkSJF4OjoiCpVqmDbtm1Z7hccHAxJkoz+hIeHy9ulpaWhb9++KFOmDNzd3eHi4oLy5ctjzpw5ePbsGTQaDQ4fPgyNRvMqHyaRjJkjpTFzZA3MHSmNmbMce54REeUhLJwVLDExMfjll18wbNgwhIeHIzY2Fs2bN8fOnTtRs2ZNk/t9+eWXePjwocGyq1ev4sMPP0Tjxo3lZWlpaTh16hSaN2+O4OBgqFQq7Nu3D8OHD8fBgwexbNkyTmlOimPmSGnMHFkDc0dKY+Ysw+IZERHRa+jQoUNYtWoVpk+fjpEjRwIAoqOjUaZMGYwePRr79u0zuW/btm0zLJs6dSoAoEePHvIyLy8vHDhwwGC7gQMHwt3dHfPmzcMXX3yRC4+EiIiIiChv422bRPlAXFwcJElCcnJyjvafNGkSIiMjc7VNRGSZX375BWq1GgMGDJCXOTg4oG/fvti/fz+uXbuWreP98MMPCAkJQfXq1bPcNjg4GABy/J5CRERERJSfsHhG+U5MTIw8to+dnR3CwsIwZcoUaDQaucjk6emJJ0+eGOx3+PBheT+9J0+eICYmBmXLloWNjY3R3hzA8+JVxYoVYW9vj7CwMMTGxprd3mnTpqFSpUpwdXWFr68v2rZti3PnzhndVgiBZs2aQZIkrF+/3uxzUP6gVquxaMcxPNOwy3VBcOzYMZQoUQJubm4GyytXrgwASEhIyNaxzpw5g+7duxtd//TpU9y9exfXrl3DunXrMGPGDAQFBSEiIgLlypXjzEykGLVazcyRopg5sgbmjpTGzFmOxTPKl5o2bYqkpCScP38eI0aMwKRJkzB9+nR5vaurK9atW2ewz/fff49ixYoZLNNqtXB0dMSQIUPQsGFDo+e6fPkyWrRogXr16iEhIQHDhg1Dv379sHXrVrPaGh8fj3fffRcHDhzAtm3b8OzZMzRu3BiPHj3KsO2XX35pUNyjgic17SmEtRtBikhKSkLhwoUzLNcvu3HjhtnHWrlyJQDDWzZftHbtWhQqVAjFihVD+/btERAQgF9//RU2Njaws7PLQeuJco6ZI6Uxc2QNzB0pjZmzDItnlC/Z29vD398fQUFBGDRoEBo2bIiNGzfK63v16oXFixfL/09LS8OqVavQq1cvg+M4Oztj/vz56N+/P/z9/Y2ea8GCBQgJCcHMmTNRqlQpDB48GB07dsTs2bPNauvvv/+OmJgYvPHGGyhfvjxiY2ORmJiIv/76y2C7hIQEzJw506DdL/vrr78QFRUFJycnVK9e3WQPtqwcPnwYjRo1go+PD9zd3VGnTh0cPXrUYJuzZ8+iZs2acHBwQOnSpbF9+3b2iHvFtFotZ9ssQNLS0mBvb59huYODg7zeHDqdDqtWrUKFChVQqlQpo9vUq1cP27Ztw88//4yBAwfC1tYWjx49glarxZEjRzjALCmGmSOlMXNkDcwdKY2ZsxyLZ1QgODo64unTp/L/e/bsid27dyMxMREAsGbNGgQHB6NixYrZPvb+/fsz9Epr0qQJ9u/fn6O2pqSkAHg+kLfe48eP0b17d3z99dcmi3gA8MEHH2DmzJk4cuQIbGxs0KdPnxy14cGDB+jVqxf27NmDAwcOIDw8HM2bN8eDBw8APH/zbdu2LZycnHDw4EEsXLgQH3zwQZbHTU9PR2pqqsEPERnn6OiI9PT0DMv1t5w7OjqadZz4+Hj8+++/JnudAYCfnx8aNmyIjh07Yv78+WjZsiUaNWqEmzdv5qzxRERERET5CItnlK8JIbB9+3Zs3boV9evXl5f7+vqiWbNm8thkixcvznGh6ebNm/Dz8zNY5ufnh9TUVLN7hujpdDoMGzYMNWrUQJkyZeTlw4cPR/Xq1dGmTZtM9//kk09Qp04dlC5dGmPHjsW+ffsyjO1mjvr16+Ott95CyZIlUapUKSxcuBCPHz9GfHw8AGDbtm24ePEili1bhvLly6NmzZr45JNPsjzutGnT4O7uLv8EBgZmu21EBUXhwoWRlJSUYbl+WZEiRcw6zsqVK6FSqdCtWzezz92xY0c8fPjQoMcuEREREVFBxeIZ5UubNm2Ci4sLHBwc0KxZM3Tp0gWTJk0y2KZPnz6IjY3FpUuXsH///kx7ZSjl3Xffxd9//41Vq1bJyzZu3Ig///wTX375ZZb7lytXTv63flyk27dvZ7sdt27dQv/+/REeHg53d3e4ubnh4cOHck+9c+fOITAw0KAXnH4Q88yMGzcOKSkp8k92ZwskKkgiIyPxzz//ZOihefDgQXl9VtLT07FmzRrUrVvX7GIb8L9bQtk7lIiIiIiIxTPKp/SD958/fx5paWlYunQpnJ2dDbZp1qwZ0tLS0LdvX7Rq1Qre3t45Ope/vz9u3bplsOzWrVtwc3Mz+7YqABg8eDA2bdqEnTt3IiAgQF7+559/4uLFi/Dw8ICNjQ1sbGwAAB06dEDdunUNjmFrayv/Wz+xgE6ny+5DQq9evZCQkIA5c+Zg3759SEhIgLe3t8Gtrzlhb28PNzc3gx8yn1qtxsxNB/GUs20WCB07doRWq8XChQvlZenp6ViyZAmqVKki99xMTEzE2bNnjR5jy5YtSE5ONvnHgbt370KIjFNQfPfddwCASpUqISoqijMzkWLUajUzR4pi5sgamDtSGjNnORtrN4DoVXB2dkZYWFim29jY2CA6OhpffPEFfvvttxyfq1q1atiyZYvBsm3btqFatWpm7S+EwHvvvYd169YhLi4OISEhBuvHjh2Lfv36GSwrW7YsZs+ejVatWuW43ZnZu3cvvvnmGzRv3hwAcO3aNdy9e1deHxERgWvXruHWrVvyLauHDx9+JW0hQ26OdvjvQRpn3CwAqlSpgk6dOmHcuHG4ffs2wsLCsHTpUly5cgXff/+9vF10dDTi4+ONFsFWrlwJe3t7dOjQweg5VqxYgQULFqBt27YoXrw4Hjx4gK1bt2Lbtm1o1aoV6tevj7S0tGz9IYDIUk+fPmXmSFHMHFkDc0dKY+Ysw55nVKB9/PHHuHPnDpo0aWJym9OnTyMhIQH37t1DSkoKEhISkJCQIK8fOHAgLl26hNGjR+Ps2bP45ptvsHr1agwfPtysNrz77rtYsWIFfvjhB7i6uuLmzZu4efOmfNuUv78/ypQpY/ADAMWKFctQaMst4eHhWL58Oc6cOYODBw+iR48eBm+0jRo1QmhoKHr16oUTJ05g7969+PDDDwH8r8cb5T6tVov+DSrAlrNtFhjLli3DsGHDsHz5cgwZMgTPnj3Dpk2bULt27Sz3TU1NxebNm9GiRQu4u7sb3aZmzZooV64cfvzxRwwZMgQfffQR/vvvP8yaNQtr166FVqvFiRMnODMTKYaZI6Uxc2QNzB0pjZmzHHueUYFmZ2cHHx+fTLdp3rw5rl69Kv+/QoUKACD38ggJCcHmzZsxfPhwzJkzBwEBAfjuu+8yLci9aP78+QCQ4RbMJUuWICYmxsxHkru+//57DBgwABUrVkRgYCA+/fRTjBw5Ul6vVquxfv169OvXD5UqVULx4sUxffp0tGrVCg4ODlZpM1F+5ODggOnTp2P69Okmt4mLizO63M3NLctJS6KiorB69WqT6zUajVntJCIiIiLKzyRh7D4PIqJs2rt3L2rWrIkLFy4gNDTUrH1SU1Ph7u6OlJQUjn9mBo1Gg8lzv+O4Z/mAZtcyazfBLBqNBkeOHEFUVJQ83iLRq8TMkdKYObIG5o6UxswZysn3UF41IsqRdevWwcXFBeHh4bhw4QKGDh2KGjVqmF04o5xh0YyUxoFlSWnMHCmNmSNrYO5IacycZdjzjOgVSkxMROnSpU2uP336NIoVK/bK2/HGG28Y3Hr6om+//dbkTHyZWbZsGaZOnYrExET4+PigYcOGmDlzZrZmLWXPs+yzqR1t7SZQLsgrPc+IiIiIiPKbnHwPZfGM6BXSaDS4cuWKyfXBwcGKdJu9evUqnj17ZnSdn58fXF1dX3kbjGHxLHuEECjRaSiu3kmGju/ceVpeKZ4JIZCSkgJ3d3dOBkKKYOZIacwcWQNzR0pj5gzxtk2i14yNjQ3CwsKs3QwEBQVZuwmUC7RaLbpWL80xz0gxWq0WZ8+e5fgYpBhmjpTGzJE1MHekNGbOciprN4CIiIiIiIiIiOh1xeIZERERERERERGRCSyeERHlEZIk4e6Dx+BQlaQUSZLg6OjIsTFIMcwcKY2ZI2tg7khpzJzlOGEAEVkNJwzIPs62mT/klQkDiIiIiIjym5x8D2XPMyKiPEKn06F8kC9U/IsRKUSn0+H27dvQ6XTWbgoVEMwcKY2ZI2tg7khpzJzlWDwjIsojdDodmlcIg42ab92kDJ1Oh0uXLvEXLVIMM0dKY+bIGpg7UhozZzl+AyMiIiIiIiIiIjKBxTMiIiIiIiIiIiITWDwjIsojJEnC5dvJnG2TFCNJEtzd3TkzEymGmSOlMXNkDcwdKY2Zsxxn2yQiq+Fsm9nH2TbzB862SURERERkHZxtk4goH9PpdKhZMhBqFf9iRMrQ6XS4fv06B5clxTBzpDRmjqyBuSOlMXOWY/GMiCiP0Ol0qFUyEGoV37pJGfxFi5TGzJHSmDmyBuaOlMbMWc7G2g0gIiLztageiY+G9IONDd++iYiIiIiIlMDuC0RERERERERERCaweEZElEeoVCoUKlQIKt62SQph5khpzBwpjZkja2DuSGnMnOU42yYRWQ1n2yQiIiIiIiIlcbZNIqJ8TKfT4eLFixzokxTDzJHSmDlSGjNH1sDckdKYOcuxeEZElEfodDrcuXOHH3qkGGaOlMbMkdKYObIG5o6UxsxZjsUzIiIiIiIiIiIiE1g8IyIiIiIiIiIiMoHFMyKiPEKlUiEgIICz5JBimDlSGjNHSmPmyBqYO1IaM2c5zrZJRFbD2TaJiIiIiIhISZxtk4goH9NqtThz5gy0Wq21m0IFBDNHSmPmSGnMHFkDc0dKY+YsZ2PtBhARkXmEEPjht3jMfPcLPNXk/Q8+za5l1m4CZUEIgZSUFLCTOimFmSOlMXNkDcwdKY2Zsxx7nhEREREREREREZnA4hkREREREREREZEJLJ4REeURKpUKW45dgEars3ZTqIBQqVQoXrw4Z2YixTBzpDRmjqyBuSOlMXOW45hnRER5hEqlwvGrt63dDCpAVCoVfH19rd0MKkCYOVIaM0fWwNyR0pg5y7HsSESUR2i1WvRvEAlbNd+6SRlarRbHjx/nzEykGGaOlMbMkTUwd6Q0Zs5y/AZGRJRHCCHg4+oESZKs3RQqIIQQSEtL48xMpBhmjpTGzJE1MHekNGbOciyeERERERERERERmcDiGRERERERERERkQksnhER5RFqtRqr9p2GhmMVkELUajVKliwJtVpt7aZQAcHMkdKYObIG5o6UxsxZjrNtEhHlEZIk4fLtZGs3gwoQSZLg4eFh7WZQAcLMkdKYObIG5o6UxsxZjj3PiIjyCI1GgxEtq8DOhn8xImVoNBocPnwYGo3G2k2hAoKZI6Uxc2QNzB0pjZmzHItnRER5CAtnpDROaU5KY+ZIacwcWQNzR0pj5izD4hkREREREREREZEJLJ4RERERERERERGZwOIZEVEeoVarsWjHMTzTFKwu1+np6RgzZgyKFCkCR0dHVKlSBdu2bctyv3Xr1qFJkyYoUqQI7O3tERAQgI4dO+Lvv//OsO3w4cNRsWJFeHl5wcnJCaVKlcKkSZPw8OHDV/GQ8gy1Wo1y5cpxZiZSDDNHSmPmyBqYO1IaM2c5zrZJRJSHpKY9hbB2IxQWExODX375BcOGDUN4eDhiY2PRvHlz7Ny5EzVr1jS538mTJ+Hp6YmhQ4fCx8cHN2/exOLFi1G5cmXs378f5cuXl7c9fPgwatWqhd69e8PBwQHHjh3DZ599hu3bt2PXrl1QqQru35rs7Oys3QQqYJg5UhozR9bA3JHSmDnLSEKIgvY9jIheE6mpqXB3d0dKSgrc3Nys3ZzXnkajweS532HmpoN4mg96n2l2Lctym0OHDqFKlSqYPn06Ro4cCQB48uQJypQpA19fX+zbty9b57x16xYCAgLQt29fLFiwINNtZ86ciZEjR2L//v2oWrVqts6TX2g0Ghw5cgRRUVGwseHf2+jVY+ZIacwcWQNzR0pj5gzl5Htowf1TegFVt25dDBs2zNrNwJUrVyBJEhISEl7J8V+Xx0lElvnll1+gVqsxYMAAeZmDgwP69u2L/fv349q1a9k6nq+vL5ycnJCcnJzltsHBwQBg1rZERERERJR/FYjiWVxcHNq0aYPChQvD2dkZkZGRWLlypdn7x8bGQpIkgx8HB4dcaVtBLfIEBgYiKSkJZcqUAfD8OZIkKcOX1Lx0fUw9hvyqdevWKFasGBwcHFC4cGH07NkTN27csHazKJ85duwYSpQokeEvQpUrVwYAswrwycnJuHPnDk6ePIl+/fohNTUVDRo0yLCdRqPB3bt3cePGDfzxxx/48MMP4erqKp+LiIiIiIgKpgLRX2/fvn0oV64cxowZAz8/P2zatAnR0dFwd3dHy5YtzTqGm5sbzp07J/9fkqRX1dwCQa1Ww9/f39rNyDXPnj2zdhMUV69ePYwfPx6FCxfGv//+i5EjR6Jjx47Zvo2OKDNJSUkoXLhwhuX6ZeYUbKtWrSq/f7u4uODDDz9E3759M2x35MgRVKtWTf5/REQENm7cCC8vr5w2n4iIiIiI8oHXrudZ3bp1MXjwYAwePBju7u7w8fHBhAkToB+aLTg4GFOnTkV0dDRcXFwQFBSEjRs34s6dO2jTpg1cXFxQrlw5HDlyRD7m+PHj8fHHH6N69eoIDQ3F0KFD0bRpU6xdu9bsdkmSBH9/f/nHz8/P7H2/+eYbhIeHw8HBAX5+fujYsSOA54Ngx8fHY86cOXKPtitXriA2NhYeHh4Gx1i/fr1BwW7SpEmIjIzE8uXLERwcDHd3d3Tt2hUPHjyQt3n06JF8nQoXLoyZM2dmaFt6ejpGjhyJokWLwtnZGVWqVEFcXJy8Xt+WrVu3olSpUnBxcUHTpk2RlJQkbxMTE4O2bdvi008/hZ+fHzw8PDBlyhRoNBqMGjUKXl5eCAgIwJIlS+R9Xrxt88qVK6hXrx4AwNPTE5IkISYmxuT1AYC///4bzZo1g4uLC/z8/NCzZ0/cvXvX6PWfN2+e3MPtxWv54nhHDRs2xIcffij/f8OGDahYsSIcHBxQvHhxTJ48GRqNRl4vSRLmz5+P1q1bw9nZGf379zf6GABAp9Nh2rRpCAkJgaOjI8qXL49ffvnFaFtfZk4WAGDq1Knw9fWFq6sr+vXrh7FjxyIyMtKsc+Tk+QOez05YtWpVBAUFoXr16hg7diwOHDhQIAuJSlGr1flmvDNzpaWlwd7ePsNyfe/ftLS0LI+xZMkS/P777/jmm29QqlQppKWlQavNeA1Lly6Nbdu2Yf369Rg9ejScnZ0526ZajaioKM7MRIph5khpzBxZA3NHSmPmLPfaFc8AYOnSpbCxscGhQ4cwZ84czJo1C9999528fvbs2ahRowaOHTuGFi1aoGfPnoiOjsZbb72Fo0ePIjQ0FNHR0chsLoSUlJRs9SZ4+PAhgoKCEBgYiDZt2uDUqVNm7XfkyBEMGTIEU6ZMwblz5/D777+jdu3aAIA5c+agWrVq6N+/P5KSkpCUlITAwECz23Tx4kWsX78emzZtwqZNmxAfH4/PPvtMXj9q1CjEx8djw4YN+OOPPxAXF4ejR48aHGPw4MHYv38/Vq1ahRMnTqBTp05o2rQpzp8/L2/z+PFjzJgxA8uXL8euXbuQmJgoD9yt9+eff+LGjRvYtWsXZs2ahY8++ggtW7aEp6cnDh48iIEDB+Ltt9/G9evXMzyOwMBArFmzBgBw7tw5JCUlYc6cOSavT3JyMurXr48KFSrgyJEj+P3333Hr1i107tzZ6HWqU6cOTp8+jTt37gAA4uPj4ePjIxcJnz17hv3796Nu3boAgN27dyM6OhpDhw7F6dOn8e233yI2NhaffPKJwXEnTZqEdu3a4eTJk5g8ebLRxwAA06ZNw7Jly7BgwQKcOnUKw4cPx1tvvYX4+PhMn19zrVy5Ep988gk+//xz/PXXXyhWrBjmz5+frWNY8vwBwL1797By5UpUr14dtra2Js+Tnp6O1NRUgx/KHjdHOxSkfq+Ojo5IT0/PsPzJkyfy+qxUq1YNTZo0waBBg7B161asWLEC48aNy7Cdm5sbGjZsiDZt2uDzzz/HiBEj0KZNGxw/ftzyB5KHPX361NpNoAKGmSOlMXNkDcwdKY2Zs8xrWTwLDAzE7NmzERERgR49euC9997D7Nmz5fXNmzfH22+/jfDwcEycOBGpqamoVKkSOnXqhBIlSmDMmDE4c+YMbt26ZfT4q1evxuHDh9G7d2+z2hMREYHFixdjw4YNWLFiBXQ6HapXr26ykPCixMREODs7o2XLlggKCkKFChUwZMgQAIC7uzvs7Ozg5OQk92jLTiVYp9MhNjYWZcqUQa1atdCzZ0/s2LEDwPNi3/fff48ZM2agQYMGKFu2LJYuXWrQeyoxMRFLlizBzz//jFq1aiE0NBQjR45EzZo1DXoZPXv2DAsWLEBUVBQqVqyIwYMHy+fR8/Lywty5cxEREYE+ffogIiICjx8/xvjx4xEeHo5x48bBzs4Oe/bsyfA41Gq1XMj09fWFv78/3N3dTV6fefPmoUKFCvj0009RsmRJVKhQAYsXL8bOnTvxzz//ZDh+mTJl4OXlJRer4uLiMGLECPn/hw4dwrNnz1C9enUAwOTJkzF27Fj06tULxYsXR6NGjfDxxx/j22+/NThu9+7d0bt3bxQvXhxBQUFGH0N6ejo+/fRTLF68GE2aNEHx4sURExODt956K8Pxcuqrr75C37590bt3b5QoUQITJ05E2bJls3WMnD5/Y8aMgbOzM7y9vZGYmIgNGzZkep5p06bJz627u3u2isUEaLVa9G9QAbY2BecvRoULFzbo6aqnX1akSJFsHc/T0xP169c3a9zL9u3bAwBWrVqVrXPkJ1qtFidOnDDaU4/oVWDmSGnMHFkDc0dKY+Ys91oWz6pWrWpwW1q1atVw/vx5+YkuV66cvE5/++SLxQL9stu3b2c49s6dO9G7d28sWrQIb7zxhlntqVatGqKjoxEZGYk6depg7dq1KFSokFnFj0aNGiEoKAjFixdHz549sXLlSjx+/Nis82YlODgYrq6u8v8LFy4sP+aLFy/i6dOnqFKlirzey8sLERER8v9PnjwJrVaLEiVKwMXFRf6Jj4/HxYsX5e2cnJwQGhpq9Dx6b7zxBlSq/8XJz8/P4DlRq9Xw9vY2+pxk1/Hjx7Fz506DNpcsWVJ+3C+TJAm1a9dGXFwckpOTcfr0abzzzjtIT0/H2bNnER8fj0qVKsHJyUk+/pQpUwyOr+/99uJzFxUVlWVbL1y4gMePH6NRo0YGx1u2bJnRtubEuXPnMgxont0BznP6/I0aNQrHjh3DH3/8AbVanWWPz3HjxiElJUX+ye5MiVTwREZG4p9//snQS/HgwYPy+uxKS0tDSkpKltulp6dDp9OZtS0REREREeVfeXLCgBdvC9MX2Ywt0+l0BvvFx8ejVatWmD17NqKjoy06f4UKFXDhwoUst3V1dcXRo0cRFxeHP/74AxMnTsSkSZNw+PDhDGNZ6alUqgwFCGPjSL18e5wkSRkec2YePnwItVqNv/76K0OPNxcXl0zP83L7jG1jafsya3erVq3w+eefZ1hnbGBx4PlYegsXLsTu3btRoUIFuLm5yQW1+Ph41KlTx+D4kydPlnudvOjFWVadnZ3NaisAbN68GUWLFjVYZ2wcp5eZmwVL5fT58/HxgY+PD0qUKIFSpUohMDAQBw4cMBh0/UX29vZmPW4ivY4dO2LGjBlYuHChfLt4eno6lixZgipVqsi9FxMTE/H48WO5kA48/wOKr6+vwfGuXLmCHTt2GBS/k5OT4ezsnCHz+uECzCmUExERERFR/vVaFs/0PQr0Dhw4gPDwcIsGt4uLi0PLli3x+eefY8CAARa1T6vV4uTJk2jevLlZ29vY2KBhw4Zo2LAhPvroI3h4eODPP/9E+/btYWdnl6HrZKFChfDgwQM8evRILtAkJCRkq42hoaGwtbXFwYMHUaxYMQDA/fv38c8//8iFogoVKkCr1eL27duoVatWto6f2+zs7AAgw7Uwdn0qVqyINWvWIDg4GDY25kW4Tp06GDZsGH7++Wd5bLO6deti+/bt2Lt3L0aMGGFw/HPnziEsLMzix1C6dGnY29sjMTHRoEBnLnOyEBERgcOHDxsUhA8fPpztc1lKX1gzNj4V5Z6CNFkAAFSpUgWdOnXCuHHjcPv2bYSFhWHp0qW4cuUKvv/+e3m76OhoxMfHGxSby5YtiwYNGiAyMhKenp44f/48vv/+ezx79sxgfMi4uDgMGTIEHTt2RHh4OJ4+fYrdu3dj7dq1iIqKwltvvaXoY37dcGBZUhozR0pj5sgamDtSGjNnmdeyeJaYmIj3338fb7/9No4ePYqvvvrK6EyR5tq5cydatmyJoUOHokOHDrh58yaA58UOcyYNmDJlCqpWrYqwsDAkJydj+vTpuHr1Kvr165flvps2bcKlS5dQu3ZteHp6YsuWLdDpdPLtk8HBwTh48CCuXLkCFxcXeHl5oUqVKnBycsL48eMxZMgQHDx4ELGxsdl6zC4uLujbty9GjRoFb29v+Pr64oMPPjC4Na9EiRLo0aMHoqOjMXPmTFSoUAF37tzBjh07UK5cObRo0SJb57REUFAQJEnCpk2b0Lx5czg6OsLFxcXo9Xn33XexaNEidOvWDaNHj4aXlxcuXLiAVatW4bvvvjP6plCuXDl4enrihx9+wKZNmwA8L56NHDkSkiShRo0a8rYTJ05Ey5YtUaxYMXTs2BEqlQrHjx/H33//jalTp2brMbi6umLkyJEYPnw4dDodatasiZSUFOzduxdubm7o1atXptfFnCy899576N+/P6KiolC9enX89NNPOHHiBIoXL56NZyB7Dh48iMOHD6NmzZrw9PTExYsXMWHCBISGhprsdUaWs7GxwcxNB7PeMJ9ZtmwZJkyYgOXLl+P+/fsoV64cNm3aJE++YsqgQYOwefNm/P7773jw4AF8fX3RuHFjjB8/3uC25LJly6JevXrYsGEDkpKSIIRAaGgoJk6ciFGjRsmF8YLIxsYGlSpVsnYzqABh5khpzBxZA3NHSmPmLPdajnkWHR2NtLQ0VK5cGe+++y6GDh1qUW+xpUuX4vHjx5g2bRoKFy4s/xi7Lc+Y+/fvo3///ihVqhSaN2+O1NRU7Nu3D6VLl85yXw8PD6xduxb169dHqVKlsGDBAvz444/yeGsjR46EWq1G6dKlUahQISQmJsLLywsrVqzAli1bULZsWfz444+YNGlSth/39OnTUatWLbRq1QoNGzZEzZo18eabbxpss2TJEkRHR2PEiBGIiIhA27ZtcfjwYbm3mlKKFi0qD9Tv5+eHwYMHAzB+fYoUKYK9e/dCq9WicePGKFu2LIYNGwYPDw+D4uCLJElCrVq1IEkSatasCeB5Qc3NzQ1RUVEGt2A2adIEmzZtwh9//IFKlSqhatWqmD17NoKCgnL0GD7++GNMmDAB06ZNQ6lSpdC0aVNs3rwZISEhWV4Xc7LQo0cPjBs3DiNHjkTFihVx+fJlxMTEGNximtucnJywdu1aNGjQABEREejbty/KlSuH+Ph43pb5CgkhEOLrAVVBmm4Tz2+Xnj59OpKSkvDkyRMcOnQITZo0MdgmLi4uwy3O+lvk7927h2fPnuHff//Fjz/+mGFCjdDQUCxduhQXL17E48ePkZaWhr///huTJk0y6/bs/EwIgeTk5EzHMiTKTcwcKY2ZI2tg7khpzJzlJPGaXb26desiMjISX375pbWbQpRnNWrUCP7+/li+fLm1m5Kp1NRUuLu7IyUlBW5ubtZuzmtPo9Fg8tzvMHPTwXxx+6Zm1zJrN4GyoNFocOTIEURFRZl9mzyRJZg5UhozR9bA3JHSmDlDOfkeyqtGlMc9fvwYCxYsQJMmTaBWq/Hjjz9i+/bt2LZtm7WbRkRERERERJTnvZa3bSrNxcXF5M/u3bsz3Xf37t2Z7k+UmYEDB5rMzsCBA806hiRJ2LJlC2rXro0333wTv/76K9asWYOGDRsCsCzfRERERERERAXda9fzLC4uTvFzZjaTZdGiRTPdNyoqKtszYRLpTZkyBSNHjjS6ztzuo46Ojti+fbvJ9Zbkm14vkiTh7oPHHKuAFCNJEhwdHSFJBWygPbIaZo6UxsyRNTB3pDRmznKv3ZhnRFRwcMyz7LOpHW3tJuQajnlGRERERERKy8n3UN62SUSUR+h0OpQP8oWKfzEiheh0Oty+fRs6nc7aTaECgpkjpTFzZA3MHSmNmbMci2dERHmETqdD8wphsFHzrZuUodPpcOnSJf6iRYph5khpzBxZA3NHSmPmLMdvYERERERERERERCaweEZERERERERERGQCi2dERHmEJEm4fDuZs22SYiRJgru7O2dmIsUwc6Q0Zo6sgbkjpTFzluNsm0RkNZxtM/s42yYREREREVHOcbZNIqJ8TKfToWbJQKhV/IsRKUOn0+H69escXJYUw8yR0pg5sgbmjpTGzFmOxTMiojxCp9OhVslAqFV86yZl8BctUhozR0pj5sgamDtSGjNnOX4DIyIiIiIiIiIiMoHFMyIiIiIiIiIiIhNYPCMiyiNUKhWOX73F7takGJVKhUKFCkHFW4VJIcwcKY2ZI2tg7khpzJzlONsmEVkNZ9vMPs62SURERERElHM5+R5q84rbREREuUSn0+Hcko8QEhLCvxqRInQ6HS5fvszMkWKYOVIaM0fWwNyR0pg5y/GqERHlETqdDnfu3OFtm6QYZo6UxsyR0pg5sgbmjpTGzFmOxTMiIiIiIiIiIiITWDwjIiIiIiIiIiIygcUzIqI8QqVSISAggOMUkGKYOVIaM0dKY+bIGpg7UhozZznOtklEVsPZNomIiIiIiEhJOfkeyrIjEVEeodVqcebMGWi1Wms3hQoIZo6UxsyR0pg5sgbmjpTGzFmOxTMiojxCCIGUlBSwwzAphZkjpTFzpDRmjqyBuSOlMXOWY/GMiIiIiIiIiIjIBBbPiIiIiIiIiIiITGDxjIgoj1CpVChevDhnySHFMHOkNGaOlMbMkTUwd6Q0Zs5ynG2TiKwmL822aVM72tpNAABodi2zdhOIiIiIiIjyLM62SUSUj9mqVTh+/DhnySHFaLVaZo4UxcyR0pg5sgbmjpTGzFmOxTMiojxCkiSkpaVxlhxSjBCCmSNFMXOkNGaOrIG5I6Uxc5Zj8YyIiIiIiIiIiMgEFs+IiIiIiIiIiIhMYPGMiCiP0Gi1KFmyJNRqtbWbQgWEWq1m5khRzBwpjZkja2DuSGnMnOVsrN0AIiIyj04AHh4e1m4GFSCSJDFzpChmjpTGzJE1MHekNGbOcux5RkSUR9jZqHH48GFoNBprN4UKCI1Gw8yRopg5UhozR9bA3JHSmDnLsXhGRJSHcHppUhozR0pj5khpzBxZA3NHSmPmLMPiGRERERERERERkQksnhEREREREREREZnA4hkRUR7xTKNFuXLlOEsOKUatVjNzpChmjpTGzJE1MHekNGbOciyeERHlEQKAnZ2dtZtBBQwzR0pj5khpzBxZA3NHSmPmLMPiGRFRHmFno8aRI0c42CcpRqvVMnOkKGaOlMbMkTUwd6Q0Zs5yLJ4RERERERERERGZwOIZERERERERERGRCSyeEREpQGieQnd+P7T7V0O79wdoT/wB8fC/rPcTArpbF6E9tRNP9q1GvXr1EBkZialTp+LJkycG2167dg2TJ09G5cqV4enpCR8fH9StWxfbt29/VQ+LiIiIiIgo32PxjIjoFRNCQHfqT4jbVyAViYAU8ibw7Al0J7ZBpKVmvrNOA/HPPuDZE8C/BGbPno1KlSrho48+QrNmzSCEkDfdsGEDPv/8c4SFhWHq1KmYMGECHjx4gEaNGmHJkiWv+FFSfqRWqxEVFcWZmUgxzBwpjZkja2DuSGnMnOVYPKNcFRwcjC+//NLazcggu+26cuUKJElCQkLCK2lPTEwM2rZt+0qObU2xsbHw8PCwdjNeO+LuVSD1DlQlqkMVVB6qIhFQlW0MSBLE1eOZ7yypoCrfBOrIZlAXK4vo6GgsXrwYH330EeLi4rBjxw5503r16iExMRE//PAD3n33XQwdOhT79u1DyZIlMXHixFf8KCm/evr0qbWbQAUMM0dKY+bIGpg7UhozZxkWz/KJmJgYSJIk/3h7e6Np06Y4ceKEtZtmYNKkSXIbbWxsEBwcjOHDh+Phw4ev9LyHDx/GgAEDzN4+MDAQSUlJKFOmDAAgLi4OkiQhOTk5W+c1VYSbM2cOYmNjs3Ws7IiNjTXIg7GfK1eumNx/0qRJiIyMfGXtK3DuJgK2DoBPMXmRZOcAyScI4r9rEDrTs95IKjUkN18AgK2NGidOnIBWq0W7du0AAGfOnJG3feONN+Dj42Owv729PZo3b47r16/jwYMHufmoqADQarVy5oiUwMyR0pg5sgbmjpTGzFmOxbN8pGnTpkhKSkJSUhJ27NgBGxsbtGzZ0trNyuCNN95AUlISrly5gs8//xwLFy7EiBEjjG6bW9XxQoUKwcnJyezt1Wo1/P39YWNjkyvnf5m7u/sr7aHVpUsXOQtJSUmoVq0a+vfvb7AsMDDwlZ2fDImH9wAXL0iSZLjC1RvQaYGsbt004ubNmwCQoVhmalsnJ6dsvQaIiIiIiIjoORbP8hF7e3v4+/vD398fkZGRGDt2LK5du4Y7d+7I24wZMwYlSpSAk5MTihcvjgkTJuDZs2fy+uPHj6NevXpwdXWFm5sb3nzzTRw5ckRev2fPHtSqVQuOjo4IDAzEkCFD8OjRo2y108bGBv7+/ggICECXLl3Qo0cPbNy4EcD/ejx99913CAkJgYODAwAgOTkZ/fr1Q6FCheDm5ob69evj+HHD291+/fVXVKpUCQ4ODvDx8ZF75gAZb9uUJAnz589Hs2bN4OjoiOLFi+OXX36R17/YY+zKlSuoV68eAMDT0xOSJCEmJgYA8Pvvv6NmzZrw8PCAt7c3WrZsiYsXL8rHCQkJAQBUqFABkiShbt26ADLetpmeno4hQ4bA19cXDg4OqFmzJg4fPiyv1/d827FjB6KiouDk5ITq1avj3LlzRq+xo6OjnAV/f3/Y2dnByclJ/v/Tp0/Rvn17uLi4wM3NDZ07d8atW7cAPO+1NnnyZBw/flzupabvJTdr1iyULVsWzs7OCAwMxDvvvPPKew3mC0/TINk5Zlgs2TnJ67Priy++gJubG5o1a5bpdhcuXMDatWvRoUMHjnFARERERESUAyye5VMPHz7EihUrEBYWBm9vb3m5q6srYmNjcfr0acyZMweLFi3C7Nmz5fU9evRAQEAADh8+jL/++gtjx46Fra0tAODixYto2rQpOnTogBMnTuCnn37Cnj17MHjwYIva6ujoaNDD7MKFC1izZg3Wrl0r3+7YqVMn3L59G7/99hv++usvVKxYEQ0aNMC9e/cAAJs3b0a7du3QvHlzHDt2DDt27EDlypUzPe+ECRPQoUMHHD9+HD169EDXrl0NboHTCwwMxJo1awAA586dQ1JSEubMmQMAePToEd5//30cOXIEO3bsgEqlQrt27aDT6QAAhw4dAgBs374dSUlJWLt2rdG2jB49GmvWrMHSpUtx9OhRhIWFoUmTJvLj0/vggw8wc+ZMHDlyBDY2NujTp09WlzcDnU6HNm3a4N69e4iPj8e2bdtw6dIldOnSBcDzXmsjRoyQewgmJSXJ61QqFebOnYtTp05h6dKl+PPPPzF69Gizz52eno7U1FSDnwJBpwVURgpXqv9/C9ZqzD6UWq3GZ599hu3bt+Ozzz7LtAfj48eP0alTJzg6OuKzzz7LZqOJnmPRlZTGzJHSmDmyBuaOlMbMWebV3JNGVrFp0ya4uLgAeF7UKVy4MDZt2gSV6n810g8//FD+d3BwMEaOHIlVq1bJBZDExESMGjUKJUuWBACEh4fL20+bNg09evTAsGHD5HVz585FnTp1MH/+fLmXWHb89ddf+OGHH1C/fn152dOnT7Fs2TIUKlQIwPPebocOHcLt27dhb28PAJgxYwbWr1+PX375BQMGDMAnn3yCrl27YvLkyfJxypcvn+m5O3XqhH79+gEAPv74Y2zbtg1fffUVvvnmG4Pt1Go1vLy8AAC+vr4GxYoOHToYbLt48WIUKlQIp0+fRpkyZeTH4O3tDX9/f6PtePToEebPn4/Y2Fi5F9GiRYuwbds2fP/99xg1apS87SeffII6deoAAMaOHYsWLVrgyZMn2br2O3bswMmTJ3H58mX51s1ly5bhjTfewOHDh1GpUiW4uLjIPQRfpH/ugef5mTp1KgYOHJjhmpkybdo0g+covxE6LaB56VZjW/vnhTNj45r9f5EVavPeip9qtLh06RImTpyIvn37YtCgQSa31Wq16Nq1K06fPo3ffvsNRYoUMfdhEMlsbGxQqVIlazeDChBmjpTGzJE1MHekNGbOcux5lo/Uq1cPCQkJSEhIwKFDh9CkSRM0a9YMV69elbf56aefUKNGDfj7+8PFxQUffvghEhMT5fXvv/8++vXrh4YNG+Kzzz4zuAXx+PHjiI2NhYuLi/zTpEkT6HQ6XL582ex2njx5Ei4uLnB0dETlypVRrVo1zJs3T14fFBQkF53053348CG8vb0Nzn358mW5fQkJCWjQoEG2rle1atUy/N9Yz7PMnD9/Ht26dUPx4sXh5uaG4OBgADC4plm5ePEinj17hho1asjLbG1tUbly5QztKVeunPzvwoULAwBu376drTafOXMGgYGBBmOelS5dGh4eHlk+/u3bt6NBgwYoWrQoXF1d0bNnT/z33394/PixWeceN24cUlJS5J9r165lq+2vvdQ70B38xeAH6Y8BO0cII7dmiqf/f92M3NJpVPINREdHo0WLFliwYEGmm/bv3x+bNm1CbGysQXGaKDuEEEhOToYQwtpNoQKCmSOlMXNkDcwdKY2ZsxyLZ/mIs7MzwsLCEBYWhkqVKuG7777Do0ePsGjRIgDA/v370aNHDzRv3hybNm3CsWPH8MEHHxjcMjlp0iScOnUKLVq0wJ9//onSpUtj3bp1AJ7fCvr222/LBbqEhAQcP34c58+fR2hoqNntjIiIQEJCAs6cOYO0tDRs3LgRfn5+Bo/jRQ8fPkThwoUNzpuQkIBz587JvbIcHc0sPuSyVq1a4d69e1i0aBEOHjyIgwcPAnh10wDrb6EFIA8+r79F9FW7cuUKWrZsiXLlymHNmjX466+/8PXXXwMw//Ha29vDzc3N4CdfcfaEqkxDgx/YOQIunsDDexk/rB7cfd4rzTHr6yBS70B7Oh4RERH44YcfMp3MYtSoUViyZAlmz56Nbt26WfqoqADTarU4e/YsZ2YixTBzpDRmjqyBuSOlMXOW422b+ZgkSVCpVEhLe97jZd++fQgKCsIHH3wgb/NirzS9EiVKoESJEhg+fDi6deuGJUuWoF27dqhYsSJOnz6NsLAwi9plZ2eXrWNUrFgRN2/ehI2Njdyz62XlypXDjh070Lt3b7OPe+DAAURHRxv8v0KFCibbDMDgzea///7DuXPnsGjRItSqVQvA81tMs9rvZaGhobCzs8PevXsRFBQEAHj27BkOHz5scJtkbilVqhSuXbuGa9euyb3PTp8+jeTkZJQuXVpu98tt/uuvv6DT6TBz5kz5VuDVq1fnevvyMsnWHvAsnHG5TxDE3UTgbiJQ6PlzLJ49gbibCMkrANIL46GJtAfP93F0/d+yxynQndoJycEFM2bMyLRYPH36dMyYMQPjx4/H0KFDc+uhERERERERFVgsnuUj6enpuHnzJgDg/v37mDdvHh4+fIhWrVoBeD5GWWJiIlatWoVKlSph8+bNcq8yAEhLS8OoUaPQsWNHhISE4Pr16zh8+LA8rteYMWNQtWpVDB48GP369YOzszNOnz6Nbdu2Gdx2mdsaNmyIatWqoW3btvjiiy9QokQJ3LhxQ54kICoqCh999BEaNGiA0NBQdO3aFRqNBlu2bMGYMWNMHvfnn39GVFQUatasiZUrV+LQoUP4/vvvjW4bFBQESZKwadMmNG/eHI6OjvD09IS3tzcWLlyIwoULIzExEWPHjjXYz9fXF46Ojvj9998REBAABwcHuLu7G2zj7OyMQYMGYdSoUfDy8kKxYsXwxRdf4PHjx+jbt6/lF/AlDRs2RNmyZdGjRw98+eWX0Gg0eOedd1CnTh1ERUUBeD6e2eXLl5GQkICAgAC4uroiLCwMz549w1dffYVWrVph7969Wd46SM9JPsUgXH2gO78P0uMUwNYeIukcIASkIMOx+XQntwEA1JXbAwCE5hl0f28HNE+hLlYG+/btQ1JSkjzgZ2hoqHwL8rp16zB69GiEh4ejVKlSWLFihcGxGzVqZNDLk4iIiIiIiLLG4lk+8vvvv8vjYLm6uqJkyZL4+eefUbduXQBA69atMXz4cAwePBjp6elo0aIFJkyYgEmTJgF4PjD+f//9h+joaNy6dQs+Pj5o3769PMB7uXLlEB8fjw8++AC1atWCEAKhoaHyTIyviiRJ2LJlCz744AP07t0bd+7cgb+/P2rXri0XAurWrYuff/4ZH3/8MT777DO4ubmhdu3amR538uTJWLVqFd555x0ULlwYP/74o9zz6mVFixbF5MmTMXbsWPTu3RvR0dGIjY3FqlWrMGTIEJQpUwYRERGYO3eufL2B5wMzzp07F1OmTMHEiRNRq1YtxMXFZTj+Z599Bp1Oh549e+LBgweIiorC1q1b4enpmePrZookSdiwYQPee+891K5dGyqVCk2bNsVXX30lb9OhQwesXbsW9erVQ3JyMpYsWYKYmBjMmjULn3/+OcaNG4fatWtj2rRpBr33yDhJUkH1Rn2Iy0chbpwFdBrA1QeqEjUgOblnvrMm/fm4aQA0l/7C5Ml/Gazu1auXXDw7fvw4gOdj8fXs2TPDoXbu3MniGWWLJElwdHSUbxMnetWYOVIaM0fWwNyR0pg5y0mCI8ZRASRJEtatW4e2bdtauykFWmpqKtzd3ZGSkvLaj39mU/v1KBJqdi2zdhOIiIiIiIjyrJx8D+WEAUREeYRKknD79m3FJokg0ul0zBwpipkjpTFzZA3MHSmNmbMci2dERHmEjVqFS5cu8UOPFKPT6Zg5UhQzR0pj5sgamDtSGjNnOY55RgUS71YmIiIiIiIiInOw5xkREREREREREZEJLJ4REeURQgi4u7tzlhxSjCRJzBwpipkjpTFzZA3MHSmNmbMcZ9skIqvhbJvZx9k2iYiIiIiIco6zbRIR5WNqlYTr169zoE9SjE6nY+ZIUcwcKY2ZI2tg7khpzJzlWDwjIsoj1CoVP/RIUfxFi5TGzJHSmDmyBuaOlMbMWY7FMyIiIiIiIiIiIhNYPCMiIiIiIiIiIjKBxTMiojxCp9OhUKFCUKn41k3KUKlUzBwpipkjpTFzZA3MHSmNmbOcjbUbQERE5tHoBEJDQ63dDCpAVCoVM0eKYuZIacwcWQNzR0pj5izHsiMRUR5ho5Jw8eJFDvRJitHpdMwcKYqZI6Uxc2QNzB0pjZmzHItnRER5hEqlwp07d/ihR4rR6XTMHCmKmSOlMXNkDcwdKY2ZsxyLZ0RERERERERERCaweEZERERERERERGQCJwwgIjKDZtcyazcBOp0ON27c4Cw5pBiVSoWAgABmjhTDzJHSmDmyBuaOlMbMWU4SQghrN4KICqbU1FS4u7sjJSUFbm5u1m4OERERERER5XM5+R7KsiMRUR6h1Wpx5swZaLVaazeFCghmjpTGzJHSmDmyBuaOlMbMWY7FMyKiPEIIgZSUFLDDMCmFmSOlMXOkNGaOrIG5I6Uxc5Zj8YyIiIiIiIiIiMgEFs+IiIiIiIiIiIhMYPGMiCiPUKlUKF68OGfJIcUwc6Q0Zo6UxsyRNTB3pDRmznKcbZOIrIazbRIREREREZGSONsmEVE+ptVqcfz4cc6SQ4ph5khpzBwpjZkja2DuSGnMnOVYPCMiyiOEEEhLS+MsOaQYZo6UxsyR0pg5sgbmjpTGzFmOxTMiIiIiIiIiIiITbKzdACIiMt/mfQmoP/5bPNUo0+Vas2uZIuchIiIiIiJ6XbHnGRFRHqFWq7Fq32loOFYBKUStVqNkyZJQq9XWbgoVEMwcKY2ZI2tg7khpzJzl2POMiCiPkCQJl28nW7sZVIBIkgQPDw9rN4MKEGaOlMbMkTUwd6Q0Zs5y7HlGRJRHaDQajGhZBXY2/IsRKUOj0eDw4cPQaDTWbgoVEMwcKY2ZI2tg7khpzJzlWDwjIspDWDgjpXFKc1IaM0dKY+bIGpg7UhozZxkWz4iIiIiIiIiIiExg8YyIiIiIiIiIiMgEFs+IiPIItVqNRTuO4ZmGXa5JGWq1GuXKlePMTKQYZo6UxsyRNTB3pDRmznIsnhER5SGpaU8hrN0IKlDs7Oys3QQqYJg5UhozR9bA3JHSmDnLsHhGRJRHaLVazrZJitJqtThy5AgHmCXFMHOkNGaOrIG5I6Uxc5Zj8YyIiIiIiIiIiMgEFs+IiIiIiIiIiIhMYPGMiIiIiIiIiIjIBEkIwbGnicgqUlNT4e7ujpSUFLi5uVm7Oa89IQScG/TBUwVn29TsWqbYuej1I4SAVquFWq2GJEnWbg4VAMwcKY2ZI2tg7khpzJyhnHwPZc8zIqI8xM3RDvy4IyU9ffrU2k2gAoaZI6Uxc2QNzB0pjZmzDItnRER5hFarRf8GFWDL2TZJIVqtFidOnODMTKQYZo6UxsyRNTB3pDRmznIsnhERkUXS09MxZswYFClSBI6OjqhSpQq2bduW5X7nzp3D8OHDUb16dTg4OECSJFy5csXotk+ePMG0adNQunRpODk5oWjRoujUqRNOnTqVy4+GiIiIiIjIEItnRERkkZiYGMyaNQs9evTAnDlzoFar0bx5c+zZsyfT/fbv34+5c+fiwYMHKFWqVKbb9ujRAxMnTkTdunUxd+5cvP3229i1axeqVauGq1ev5ubDISIiIiIiMsDiWQETExODtm3bWrsZAABJkrB+/fpXcuzX6XES5SYlJwswx6FDh7Bq1SpMmzYN06dPx4ABA/Dnn38iKCgIo0ePznTf1q1bIzk5GSdPnkSPHj1Mbvfvv/9i7dq1GDZsGL755hv069cPEydOxKpVq/DgwQOsXbs2tx8WvUCt5m3CpCxmjpTGzJE1MHekNGbOMrlSPLt58ya++eYbDBkyBP369ZOX37lzB4cOHUJaWlpunOaVePLkCWJiYlC2bFnY2NiYLLjExcWhYsWKsLe3R1hYGGJjY3Pl/AW5yJOUlIRmzZoBAK5cuQJJkpCQkGCwTV66PqYeQ3719ttvIzQ0FI6OjihUqBDatGmDs2fPWrtZ+ZqNjQ1mbjr4WhXQfvnlF6jVagwYMEBe5uDggL59+2L//v24du2ayX29vLzg6uqa5TkePHgAAPDz8zNYXrhwYQCAo6NjTppOZrCxsUGlSpVgY2Nj7aZQAcHMkdKYObIG5o6UxsxZzuLi2TfffIOQkBAMHjwY8+bNw5IlS+R1t2/fRrVq1bBixQpLT/PKaLVaODo6YsiQIWjYsKHRbS5fvowWLVqgXr16SEhIwLBhw9CvXz9s3bpV4dbmL/7+/rC3t7d2M3JFQZy55M0338SSJUtw5swZbN26FUIING7cmINQvkJCCIT4ekD1Gk23eezYMZQoUSLDFM+VK1cGgFwpJoeGhiIgIAAzZ87Er7/+iuvXr+PQoUMYOHAgQkJC0LVrV4vPQcYJIZCcnAwhhLWbQgUEM0dKY+bIGpg7UhozZzmLime//vorBg8ejLJly2Ljxo0YNGiQwfo33ngD5cqVy7Vb8+rWrYvBgwdj8ODBcHd3h4+PDyZMmCAHIDg4GFOnTkV0dDRcXFwQFBSEjRs34s6dO2jTpg1cXFxQrlw5HDlyRD6ms7Mz5s+fj/79+8Pf39/oeRcsWICQkBDMnDkTpUqVwuDBg9GxY0fMnj3brHb/8ssvKFu2LBwdHeHt7Y2GDRvi0aNHmDRpEpYuXYoNGzZAkiRIkoS4uDjExcVBkiQkJyfLx0hISDAYTDs2NhYeHh7YunUrSpUqBRcXFzRt2hRJSUnyPlqtFu+//z48PDzg7e2N0aNHZ3ix6HQ6TJs2DSEhIXB0dET58uXxyy+/yOv1bdmxYweioqLg5OSE6tWr49y5c/I2kyZNQmRkJBYvXoxixYrBxcUF77zzDrRaLb744gv4+/vD19cXn3zyicG5X7xtMyQkBABQoUIFSJKEunXrmrw+AHDt2jV07twZHh4e8PLyQps2bUwONL5p0yZ4eHjIRR39tRw7dqy8Tb9+/fDWW2/J/9+zZw9q1aoFR0dHBAYGYsiQIXj06JG8Pjg4GB9//DGio6Ph5uaGAQMGGH0Met999x1KlSoFBwcHlCxZEt98843Rtr7MnCwAwKJFixAYGAgnJye0a9cOs2bNgoeHh1nnyOnzN2DAANSuXRvBwcGoWLEipk6dimvXrpl8HshyWq0WXauXhs1r1OU6KSlJ7gH2Iv2yGzduWHwOW1tbrFmzBs7OzmjdujUCAwNRpUoVPHz4EPv27TM765R9Wq0WZ8+eZVGcFMPMkdKYObIG5o6UxsxZzqLi2fTp01GsWDHs3LkTLVu2hK+vb4ZtypYti9OnT1tyGgNLly6FjY0NDh06hDlz5mDWrFn47rvv5PWzZ89GjRo1cOzYMbRo0QI9e/ZEdHQ03nrrLRw9ehShoaGIjo7OVsV1//79GXqlNWnSBPv3789y36SkJHTr1g19+vTBmTNnEBcXh/bt20MIgZEjR6Jz585y0SspKQnVq1c3u12PHz/GjBkzsHz5cuzatQuJiYkYOXKkvH7mzJmIjY3F4sWLsWfPHty7dw/r1q0zOMa0adOwbNkyLFiwAKdOncLw4cPx1ltvIT4+3mC7Dz74ADNnzsSRI0dgY2ODPn36GKy/ePEifvvtN/z+++/48ccf8f3336NFixa4fv064uPj8fnnn+PDDz/EwYMHjT6WQ4cOAQC2b9+OpKQkrF271uT1efbsGZo0aQJXV1fs3r0be/fulYuHxnqA1apVCw8ePMCxY8cAAPHx8fDx8ZELcfpl+mLXxYsX0bRpU3To0AEnTpzATz/9hD179mDw4MEGx50xYwbKly+PY8eOYcKECUYfAwCsXLkSEydOxCeffIIzZ87g008/xYQJE7B06VKj1yK79u7di4EDB2Lo0KFISEhAo0aNMhS6smLp8/fo0SMsWbIEISEhCAwMNHme9PR0pKamGvxQ3paWlma0B6mDg4O8Pjd4enoiMjISY8eOxfr16zFjxgxcuXIFnTp1wpMnT3LlHERERERERMZYdMNrQkICevbsCWdnZ5PbFC1aFLdu3bLkNAYCAwMxe/ZsSJKEiIgInDx5ErNnz0b//v0BAM2bN8fbb78NAJg4cSLmz5+PSpUqoVOnTgCAMWPGoFq1arh165bJnmYvu3nzZoaxdvz8/JCamoq0tLRMx9tJSkqCRqNB+/btERQUBOB5QVHP0dER6enpZrflRc+ePcOCBQsQGhoKABg8eDCmTJkir//yyy8xbtw4tG/fHsDzHnQv3mqanp6OTz/9FNu3b0e1atUAAMWLF8eePXvw7bffok6dOvK2n3zyifz/sWPHokWLFnjy5In8BVmn02Hx4sVwdXVF6dKlUa9ePZw7dw5btmyBSqVCREQEPv/8c+zcuRNVqlTJ8FgKFSoEAPD29ja4Fsauz4oVK6DT6fDdd99Bkp7fv7ZkyRJ4eHggLi4OjRs3Nji2u7s7IiMjERcXh6ioKMTFxWH48OGYPHkyHj58iJSUFFy4cEF+fNOmTUOPHj0wbNgwAEB4eDjmzp2LOnXqYP78+fJjrl+/PkaMGCGfRz8A48uP4aOPPsLMmTPl5yEkJASnT5/Gt99+i169emV8YrPpq6++QrNmzeTCaYkSJbBv3z5s2rTJ7GPk9Pn75ptvMHr0aDx69AgRERHYtm0b7OzsTJ5n2rRpmDx5cs4fLL129K/Rl+kLWrkxHllKSgpq1aqFUaNGGbzmoqKiULduXSxZsiRDz2ciIiIiIqLcYlHPM51OB1tb20y3uX37dq6Oa1W1alW5YAIA1apVw/nz5+Xuh+XKlZPX6QteLxar9Mtu376da23KTPny5dGgQQOULVsWnTp1wqJFi3D//v1cObaTk5NcOAOe3yalf1wpKSlISkoyKHTY2NggKipK/v+FCxfw+PFjNGrUCC4uLvLPsmXLcPHiRYNzvXhd9bdjvXgNg4ODDQb+9vPzQ+nSpaFSqQyW5cZ1P378OC5cuABXV1e5zV5eXnjy5EmGduvVqVMHcXFxEEJg9+7daN++PUqVKoU9e/YgPj4eRYoUQXh4uHz82NhYg2vSpEkT6HQ6XL58WT7mi9fSlEePHuHixYvo27evwfGmTp1qsq3Zde7cOXl8Kb2X/5+VnD5/PXr0wLFjxxAfH48SJUqgc+fOmfYCGjduHFJSUuSfzAaTp4wkScLdB49fq7EKChcubHC7uJ5+WZEiRSw+x5o1a3Dr1i20bt3aYHmdOnXg5uaGvXv3WnwOMk6SJDg6Ohp87hK9SswcKY2ZI2tg7khpzJzlLOp5FhERgd27d5tcr9FosGvXLoPi1av2YjFPHwxjy3Q6ndnH9Pf3z9B77tatW3Bzc8uyV4Varca2bduwb98+/PHHH/jqq6/wwQcf4ODBg/IYWS/TFyxe/IL87NmzDNu9XLiUJClbX6ofPnwIANi8eTOKFi1qsO7lgmdW19BYW4wty851z6zdb775JlauXJlhnb4H28vq1q2LxYsX4/jx47C1tUXJkiVRt25dxMXF4f79+wa97B4+fIi3334bQ4YMyXCcYsWKyf/OrMfli8cCno9J9nKPO3OmCjY3C5bK6fPn7u4Od3d3hIeHo2rVqvD09MS6devQrVs3o+ext7fPN5NEWINarcaiHQnWboaByMhI7Ny5E6mpqQaTBuhv8Y2MjLT4HPr335fHaBBCQKvVQqPRWHwOMk6tVqN8+fLWbgYVIMwcKY2ZI2tg7khpzJzlLOp5pu91Yuw2LK1Wi5EjR+LSpUuIjo625DQGXh5z6cCBAwgPDzerEJFT1apVw44dOwyWbdu2Tb7VMSuSJKFGjRqYPHkyjh07Bjs7O3nsMTs7uwxfCPUFoBd7c2R3xjp3d3cULlzY4HppNBr89ddf8v9Lly4Ne3t7JCYmIiwszOAns3GrXgX9rX4vXwtj16dixYo4f/48fH19M7Tb3d3d6PH1457Nnj1bLpTpi2dxcXEGg/tXrFgRp0+fznDssLCwTG9JNPYY/Pz8UKRIEVy6dCnDsUwVT19kThYiIiJw+PBhg2Uv/18JQggIIYzewke5Q6fToXyQL1Sv0V+MOnbsCK1Wi4ULF8rL0tPTsWTJElSpUkV+L0lMTMTZs2dzdI4SJUoAAFatWmWwfOPGjXj06BEqVKiQw9ZTVnQ6HW7fvp0rf/ggMgczR0pj5sgamDtSGjNnOYt6nr333nv49ddfMWXKFKxcuVIeC6pz5844cuQIrly5gsaNG6Nv37650ljg+Rew999/H2+//TaOHj2Kr776CjNnzrTomKdPn8bTp09x7949PHjwQC5O6HtMDBw4EPPmzcPo0aPRp08f/Pnnn1i9ejU2b96c5bEPHjyIHTt2oHHjxvD19cXBgwdx584dlCpVCsDz2+W2bt2Kc+fOwdvbG+7u7nLxatKkSfjkk0/wzz//5OgxDh06FJ999hnCw8NRsmRJzJo1y2DWRldXV4wcORLDhw+HTqdDzZo1kZKSgr1798LNzS1XxuMyl6+vLxwdHfH7778jICAADg4OcHd3N3p9evTogenTp+P/2Lv3+J7r///j9/d7swPbzBy2OZ9POZ/mlBky8RElSgp9lE+hqEQqUYSSQ+qTTiKliD4kQtK2hBgZKeezjMlhc9jG3u/X7w+/vb+9mzHe83qZ3a6Xy/tS79fx8Xq7t/V+eL5ezy5duui1115T6dKldfDgQf3vf//TsGHDVLp06SzHL1KkiOrUqaM5c+bo3XfflSS1atVKPXr00KVLl9xGng0fPlxNmzbVoEGD9Nhjj6lQoUL6448/tHLlSte+13MNr776qp5++mkVLlxYHTp0UHp6ujZu3KjTp0/r2WefvernkpMsPPXUU2rVqpUmT56szp0768cff9SyZctu6pDcffv2ad68eWrfvr2KFy+uI0eOaMKECfL391fHjh1v2nnzO6fTqY71K2v7nyd1MePWmCknIiJC3bt314gRI5SUlKTKlSvr008/1YEDBzRjxgzXdr1791ZcXJzbKMrk5GS98847kuS69fLdd99VcHCwgoODXZN0dO7cWXfccYdee+01HTx4UE2bNtWePXv07rvvKjw8PFd/x8Cd0+nUvn37FBIS4nYbN3CzkDmYjczBCuQOZiNznvPoUytQoIBWrFihF154QSdPntS2bdtkGIYWLFigU6dOafjw4Vq8eHGufonv3bu3UlNT1aRJEw0cOFCDBw9W//79PTpmx44dVb9+fX377beKjY1V/fr13UYyVKhQQUuXLtXKlStVt25dTZo0SR9//LGio6OveeygoCD99NNP6tixo6pWraqXX35ZkyZN0t133y1Jevzxx1WtWjU1atRIxYsX15o1a1SgQAF9+eWX2rFjh+rUqaM33nhDY8eOve7reu655/TII4+oT58+atasmQIDA3Xvvfe6bTNmzBiNHDlS48ePV40aNdShQwctXbo0R6OicpO3t7emTZumDz74QCVLllSXLl0kXfnzKViwoH766SeVLVvW9eyyfv36KS0tze22sX+KjIyUw+FwjTILCQlRzZo1FRYWpmrVqrm2q1OnjuLi4rRr1y7deeedql+/vl555ZVrPrspu2t47LHH9PHHH2vmzJmqXbu2IiMjNWvWrBx9xjnJQosWLfT+++9r8uTJqlu3rpYvX65nnnnG1cy+Gfz8/LR69Wp17NhRlStX1gMPPKDAwECtXbv2irPu4vY2e/ZsDRkyRJ999pmefvppXbp0SUuWLFGrVq2uut/p06c1cuRIjRw5UsuXL5d0eZbgkSNH6q233nJt5+Pjo9WrV2vIkCFau3atBg8erFmzZqlr165as2aNihUrdlOvDwAAAED+ZjNy6cnThmFo586dOnXqlIKCglSjRo1cv5WydevWqlevnqZOnZqrxwVuN48//rh27Nhx1WcS3gpSUlJUuHBhJScnX7XxicsyMjL06rSPNWnJetNGnmX8NNuU8+DWlJGRoY0bN6pRo0by9vZosDqQI2QOZiNzsAK5g9nInLsb+R7q0adWsWJF3X333frvf/8rm82m6tWre3I4ADforbfe0l133aVChQpp2bJl+vTTT/Xee+9ZXRZymc1m0/6kM7fUbJu4vdlsNhUuXJiZmWAaMgezkTlYgdzBbGTOcx7dtvnXX3/l+9Eihw4dUkBAQLavQ4cOWV0ibmHjxo3LNjuZt/bmxIYNG3TXXXepdu3aev/99zVt2jQ99thjkqQ77rgj23NcacZS3Lq8vLw0d+0fuuTgQZ8wh5eX100ZSQ5kh8zBbGQOViB3MBuZ85xHt222bNlSoaGh+vrrr3OzpjwlIyNDBw4cyHZ9+fLlGRaJbJ06dUqnTp264jp/f3+VKlXK43McPHhQly5duuK60NBQBQYGenyOG8Vtm9fH6XSqzRMjtW7XETmc5ow+47bN/M3pdOro0aMqWbIkD5eFKcgczEbmYAVyB7OROXem37Y5fPhwdevWTTExMYqKivLkUHmWt7e3KleubHUZyKNCQkIUEhJyU89Rrly5m3p8mMfpdOrO6mW0Yc9ROZy3xmybuL05nU4dOXJEYWFh/I8WTEHmYDYyByuQO5iNzHnOo+bZ6dOn1b59e7Vv315du3ZV48aNFRoaesX7aHv37u3JqQAAAAAAAADTedQ869u3r2w2mwzD0Ndff+26ffPvzTPDMGSz2WieAQAAAAAAIM/xqHk2c+bM3KoDAHANdrtdWw4el9PJhAEwh91uV/HixRneD9OQOZiNzMEK5A5mI3Oe82jCAADwBBMGXD/vVuaO4mXCAAAAAAC3kxv5HkrbEQDyCKfTqY71K8nbnvW5ksDN4HQ6tXfvXkY7wjRkDmYjc7ACuYPZyJznPLpt89ChQznetmzZsp6cCgDyPafTqbrlQvXDbwckZtuECZxOp06cOKFy5coxzB+mIHMwG5mDFcgdzEbmPOdR86x8+fJXnFnzn2w2mzIyMjw5FQAAAAAAAGA6j5pnvXv3vmLzLDk5WVu2bNH+/fsVGRmp8uXLe3IaAAAAAAAAwBIeNc9mzZqV7TrDMDRp0iS9+eabmjFjhienAQDo8iw5q3ccloNnFcAkdrtdpUuXZng/TEPmYDYyByuQO5iNzHnups+2GRUVpZCQEH399dc38zQA8iBm27x+zLYJAAAAADfulpxts1GjRvrxxx9v9mkA4LbncDj0YPOaKuDF3xjBHA6HQ9u3b5fDwQQVMAeZg9nIHKxA7mA2Mue5m/4NbO/evUwWAAC5wDAMVSgRnKOJWoDcYBiGkpOTdZMHqQMuZA5mI3OwArmD2cic5zx65ll2nE6n/vzzT82aNUvffPON2rZtezNOAwD5Tqfm9TTq6cfk7X1TfnwDAAAAAP7Bo29fdrv9qiMgDMNQkSJFNGnSJE9OAwAAAAAAAFjCo+ZZq1atrtg8s9vtKlKkiBo3bqxHH31UJUqU8OQ0AABd/tlasWJFZsmBacgczEbmYDYyByuQO5iNzHnups+2CQDZYbZNAAAAAICZTJ9t89ChQ0pJSbnqNmfPntWhQ4c8OQ0AQJdnydmyZQuz5MA0ZA5mI3MwG5mDFcgdzEbmPOdR86xChQqaOnXqVbeZNm2aKlSo4MlpAAC6/BzJ1NRUZsmBacgczEbmYDYyByuQO5iNzHnOo+ZZTj54/nAAAAAAAACQV930p8UdOXJEgYGBN/s0AAAAAAAAQK677tk2X3vtNbf3sbGxV9zO4XDo8OHDmjt3rpo2bXpDxQEA/o+Xl5eqV68uLy8vq0tBPkHmYDYyB7OROViB3MFsZM5z1z3b5t+nNrXZbNe8LbNkyZJauHChGjdufGMVArhtMdsmAAAAAMBMN/I99LpHnsXExEi6/CyzNm3aqG/fvurTp0+W7by8vBQSEqLq1au7NdwAADcmIyNDmzdvVv369eXtfd0/voHrRuZgNjIHs5E5WIHcwWxkznPX/alFRka6/n3UqFGKiopSq1atcrUoAMgvvFv1zvG2Pt5e+nHcf25iNUBWTGkOs5E5mI3MwQrkDmYjc57xqOU4atSo3KoDAAAAAAAAuOXk2ni9w4cP6+jRo0pPT7/iekanAQAAAAAAIK/xuHn27bff6vnnn9fu3buvuh1DBAHAM5cyHKpTpw6z5MA0Xl5eZA6mInMwG5mDFcgdzEbmPOfRk/xjY2N177336ty5cxo0aJAMw1CrVq3Uv39/1axZU4ZhqFOnTnrllVdyq14AyLcMST4+PlaXgXyGzMFsZA5mI3OwArmD2cicZzxqnk2YMEEBAQHatGmT3n77bUlSVFSUpk+frt9++02vv/66Vq1apS5duuRKsQCQn/l4e2njxo2M5IVpHA4HmYOpyBzMRuZgBXIHs5E5z3nUPIuPj1fXrl0VGhrqWuZ0Ol3/PmLECNWvX5+RZwAAAAAAAMiTPGqeXbhwQaVKlXK99/X1VUpKits2TZs21Zo1azw5DQAAAAAAAGAJj5pnYWFhOnHihOt9qVKl9Pvvv7ttc/LkSYYGAgAAAAAAIE/yqHlWt25dbdu2zfU+KipKMTEx+vLLL3X+/HmtWLFCX331lerUqeNxoQCQ313McKhRo0bMkgPTeHl5kTmYiszBbGQOViB3MBuZ85xHzbN77rlHCQkJOnjwoCTpxRdfVEBAgB5++GEFBQWpY8eOysjI0NixY3OlWADIz2ySLl68aHUZyGfIHMxG5mA2MgcrkDuYjcx5xqPm2b///W9duHBB5cqVkyRVqFBB8fHxeuKJJ9S+fXs9/vjjWr9+vVq1apUrxQJAflbA20tbt27lVniYxuFwkDmYiszBbGQOViB3MBuZ85x3bh+wUqVK+u9//5vbhwUAAAAAAABM59HIs386deqUDh8+nJuHBAAAAAAAACzjcfMsOTlZgwcPVmhoqIoXL64KFSq41q1fv14dO3bUpk2bPD0NAEDiIZ8wHZmD2cgczEbmYAVyB7OROc/YDMMwbnTnU6dOqXnz5tq1a5caNGigtLQ0bd++3XUfbWpqqsLCwtSvXz9Nnjw514oGcHtISUlR4cKFlZycrKCgIKvLsYR3q97XtX3GT7NvUiUAAAAAcPu7ke+hHo08Gz16tHbt2qW5c+dq48aN6t69u9t6f39/RUZG6scff/TkNAAASXabdObMGWX+nceZM2fUv39/FS9eXIUKFVJUVJR+/fXXHB9v+/bt6tChgwICAhQSEqJHHnlEJ06cyLJdYmKi+vfvrwoVKsjf31+VKlXSs88+q5MnT+bateHWZBiGW+aAm43MwWxkDlYgdzAbmfOcR82zxYsX61//+pd69OiR7Tbly5fXkSNHPDkNAECSt5eXduzYIYfDIafTqU6dOumLL77QoEGD9OabbyopKUmtW7fW7t27r3msI0eOqFWrVtqzZ4/GjRunoUOHaunSpbrrrrvcprE+d+6cmjVrpoULF6p3795655131LFjR7377rtq166dnE7nzbxkWMzhcLgyB5iBzMFsZA5WIHcwG5nznEezbSYmJurBBx+86ja+vr46f/68J6cBAPzDggULtHbtWs2fP1/333+/JKlHjx6qWrWqRo0apS+++OKq+48bN07nz5/Xpk2bVLZsWUlSkyZNdNddd2nWrFnq37+/pMt/SXLw4EEtWbJEnTp1cu0fEhKi1157TVu2bFH9+vVv0lUCAAAAgPU8GnlWtGjRa86uuWPHDoWHh3tyGtyg8uXLa+rUqbl6zAMHDshmsykhIcGj44wePVr16tXLlZpy81i36rltNptsNpuCg4Nv+rluVPny5V11njlzxupybnsLFixQaGio7rvvPtey4sWLq0ePHvrmm2+Unp5+1f2//vpr/etf/3I1ziSpXbt2qlq1qr766ivXspSUFElSaGio2/6ZP9f9/f09vhYAAAAAuJV51Dxr1aqVvvnmm2xvy/zjjz+0fPlytWvXzpPT5Dl9+/Z1NRFsNpuKFi2qDh06aOvWrVaX5rJ8+XLZbDYdO3bMbXl4eLjKly/vtiyzYbZq1SqVKVNGiYmJqlWr1k2tL/Ocma/AwEDdcccdGjhwYJZb0oYOHapVq1bd1Hqkyw2sRYsWWXJuSZo5c6Z27drlep+YmKiHHnpIVatWld1u15AhQ6643/z581W9enX5+fmpdu3a+u67725KffHx8fr6669vyrFxmWEY8vf3l81m0+bNm9WgQQPZ7e4/xps0aaILFy64ZeWf/vzzTyUlJalRo0ZZ1jVp0kSbN292vW/VqpXsdrsGDx6sX375RUeOHNF3332n119/XV27dlX16tVz7wJxy7HZbK7MAWYgczAbmYMVyB3MRuY851Hz7KWXXpLD4VCLFi00Z84c/fXXX5IuP4R6xowZatOmjXx9ffX888/nSrF5SYcOHZSYmKjExEStWrVK3t7e+te//mV1WS4tW7aUt7e3YmNjXcu2b9+u1NRUnT59WgcOHHAtj4mJka+vr1q0aCEvLy+FhYXJ29ujO35z7IcfflBiYqK2bNmicePGafv27apbt65bwyogIEBFixbN9hh/f35TbrvWuXNTcHCwSpQo4Xqfnp6u4sWL6+WXX1bdunWvuM/atWvVs2dP9evXT5s3b1bXrl3VtWtXbdu2LdfrK168uEJCQnL9uPg/lxxO1a1bV15eXkpMTLziqN7MZUePHs32OImJiW7b/nP/U6dOuUau1axZUx9++KH++OMPNWvWTGXKlFGnTp3Utm1bzZ8/PzcuC7cwLy8vV+YAM5A5mI3MwQrkDmYjc57zqHlWu3ZtzZs3T2fOnFHv3r313nvvyTAM1apVS48//rhSU1P11VdfqUqVKrlVb57h6+ursLAwhYWFqV69enrhhRd0+PBht5nshg8frqpVq6pgwYKqWLGiRo4cqUuXLrnWb9myRVFRUQoMDFRQUJAaNmyojRs3utb//PPPuvPOO+Xv768yZcro6aefzvHz5QICAtS4cWO35llsbKxatmypFi1aZFnetGlT+fn5ZbltMzY21jUqrVGjRipYsKCaN2+unTt3up1vwoQJCg0NVWBgoPr166e0tLQc1Vm0aFGFhYWpYsWK6tKli3744QdFRESoX79+rocd/vPWyb59+6pr1656/fXXVbJkSVWrVk2SdPjwYfXo0UPBwcEKCQlRly5d3JqEkvTJJ5/ojjvukK+vr8LDwzVo0CBJco3Gu/fee2Wz2Vzv/3lup9Op1157TaVLl5avr6/q1aun5cuXu9Znfn7/+9//FBUVpYIFC6pu3bpat25djj6Pvytfvrzefvtt9e7dW4ULF77iNm+//bY6dOig559/XjVq1NCYMWPUoEEDvfvuu9keN/OaPvnkE5UtW1YBAQEaMGCAHA6H3nzzTYWFhalEiRJ6/fXXr7vm9PR0paSkuL2Qc3abTUlJSXI6nUpNTZWvr2+Wbfz8/CRJqamp2R4nc11O9y9VqpSaNGmiqVOnauHChXr22Wc1Z84cvfDCCx5dD259TqfTlTnADGQOZiNzsAK5g9nInOeuu3mWkpLiNpLnnnvu0f79+zVp0iR1795d7dq107333qs33nhDe/fuVceOHXO14Lzo3Llz+vzzz1W5cmW3UUqBgYGaNWuW/vjjD7399tv66KOPNGXKFNf6Xr16qXTp0oqPj9emTZv0wgsvqECBApKkvXv3qkOHDurWrZu2bt2qefPm6eeff3Y1e3IiKipKMTExrvcxMTFq3bq1IiMj3ZbHxsYqKirqqsd66aWXNGnSJG3cuFHe3t7697//7Vr31VdfafTo0Ro3bpw2btyo8PBwvffeezmu8+8ybx87ePCgNm3alO12q1at0s6dO7Vy5UotWbJEly5dUnR0tAIDA7V69WqtWbNGAQEB6tChgyvP06dP18CBA9W/f3/99ttvWrx4sSpXrizp8i2J0uVbJxMTE13v/+ntt9/WpEmT9NZbb2nr1q2Kjo7WPffck+VW05deeklDhw5VQkKCqlatqp49eyojI+OGPpOrWbduXZbbpqOjo6/ZrNu7d6+WLVum5cuX68svv9SMGTPUqVMnHTlyRHFxcXrjjTf08ssva/369ddVz/jx41W4cGHXq0yZMtd9TfmN4XTIuJgq42KqvDLSFB8fr0uXLsnf3/+KzzXLbExf7Vlkmetysv+aNWv0r3/9S6+//roGDx6srl27atKkSXr55Zc1efJk/fHHHx5fI25dTqdT+/bt43+0YBoyB7OROViB3MFsZM5z133vXZEiRTR69GiNHDnStWzPnj2y2+2aO3durhaXly1ZskQBAQGSpPPnzys8PFxLlixxez7Ryy+/7Pr38uXLa+jQoZo7d66GDRsmSTp06JCef/551zOF/j6Cb/z48erVq5frOVdVqlTRtGnTFBkZqenTp7tGj1xNVFSUxo0b57r9Ky4uTs8//7wyMjI0ffp0SdK+fft06NChazbPXn/9dUVGRkqSXnjhBXXq1ElpaWny8/PT1KlT1a9fP/Xr10+SNHbsWP3www85Hn32T5mfx4EDB9SkSZMrblOoUCF9/PHH8vHxkSR9/vnncjqd+vjjj133ec+cOVPBwcGKjY1V+/btNXbsWD333HMaPHiw6ziNGzeWdPmWROnyrZNhYWHZ1vbWW29p+PDhrllo33jjDcXExGjq1Kn673//69pu6NChrpkLX331Vd1xxx3as2dPrj8/6tixY1ke9B4aGprlWXf/5HQ69cknnygwMFA1a9ZUVFSUdu7cqe+++052u13VqlVzXVtERESO6xkxYoSeffZZ1/uUlBQaaNeSckLO31ZKktIk/etfX2n37t0KDw933X75d5nLSpYsme0hM2/XzG7/kJAQ16i0Dz74QKGhoVmej3bPPfdo9OjRWrt2rWrWrHlDlwYAAAAAecF1jzwzDEOGYbgtW7ZsmZ555plcK+p2EBUVpYSEBCUkJGjDhg2Kjo7W3XffrYMHD7q2mTdvnlq0aKGwsDAFBATo5Zdf1qFDh1zrn332WT322GNq166dJkyYoL1797rWbdmyRbNmzVJAQIDrFR0dLafTqf379+eoxubNm8vHx0exsbH6448/lJqaqgYNGqhRo0Y6ceKE9u/fr9jYWPn7+6tp06ZXPVadOnVc/575xTwpKUnS5Wep/bPB0qxZsxzVeCWZ+bvaww5r167tapxJlz+vPXv2KDAw0PV5hYSEKC0tTXv37lVSUpKOHj2qtm3b3nBdKSkpOnr0qFq0aOG2vEWLFtq+fbvbsqt9XreC8uXLKzAw0PU+NDRUNWvWdGv+hoaGXnfNvr6+CgoKcnvhGgoVkb1WO9lrtZNP3fZ6++23XbeD//rrr1n+9mj9+vUqWLCgqlatmu0hS5UqpeLFi7vdBp5pw4YNbrciHz9+3HWL9N9l3mJ+M0ZMAgAAAMCtxKNnniF7hQoVUuXKlVW5cmU1btxYH3/8sc6fP6+PPvpI0uXb6Xr16qWOHTtqyZIl2rx5s1566SW3W2JHjx6t33//XZ06ddKPP/6omjVrauHChZIu3wr6n//8x9WgS0hI0JYtW7R7925VqlQpRzUWLFhQTZo0UUxMjGJiYtSyZUt5eXmpQIECat68uWt5ixYt3BpRV5J5O6n0f02tmzUkNLMRVaFChWy3KVSokNv7c+fOqWHDhm6fV0JCgnbt2qWHHnroqre43QxmfV5hYWE6fvy427Ljx49fdfTcP+uTLtd4pWUM+735bAV8ZSsSLluRcNmLhOuuu+6Sv7+/7r//fh0/flz/+9//XNv+9ddfmj9/vjp37uz2PLO9e/e6Nd8lqVu3blqyZIkOHz7sWrZq1Srt2rVL3bt3dy2rWrWqjh8/7vYcREn68ssvJUn169fPzcvFLcZms6lw4cLMzATTkDmYjczBCuQOZiNznjNnykTIZrPJbre7HsK9du1alStXTi+99JJrm7+PSstUtWpVVa1aVc8884x69uypmTNn6t5771WDBg30xx9/uJ7JdaOioqI0d+5cnT59Wq1bt3Ytb9WqlWJjYxUXF6cnnnjCo3PUqFFD69evV+/evV3Lfvnllxs6ltPp1LRp01ShQoXr+tLeoEEDzZs3TyVKlMh2tFP58uW1atWqbG9RLVCgwBVH4GQKCgpSyZIltWbNGtctrNLlZ0Zld3vpzdasWTOtWrXKdXuvJK1cudKjkX+wziWHUzVq1JAk3X///WratKkeffRR/fHHHypWrJjee+89ORwOvfrqq277ZY6o/PsEGS+++KLmz5+vqKgoDR48WOfOndPEiRNVu3ZtPfroo67tBg0apJkzZ6pz58566qmnVK5cOcXFxenLL7/UXXfddV237SLv8fLycmUOMAOZg9nIHKxA7mA2Muc5Rp7dJOnp6Tp27JiOHTum7du366mnntK5c+fUuXNnSZefUXbo0CHNnTtXe/fu1bRp01yjyqTLM90NGjRIsbGxOnjwoNasWaP4+HhX4IcPH661a9dq0KBBSkhI0O7du/XNN99c14QB0uXm2e7du7VixQq3hk9kZKQWLVqkw4cPX/N5Z9cyePBgffLJJ5o5c6Z27dqlUaNG6ffff8/RvidPntSxY8e0b98+LV68WO3atdOGDRs0Y8aM65pmt1evXipWrJi6dOmi1atXu25Jffrpp3XkyBFJl0f6TZo0SdOmTdPu3bv166+/6p133nEdI7O5duzYMZ0+ffqK53n++ef1xhtvaN68edq5c6deeOEFJSQkuD1HLTdljqA7d+6cTpw4oYSEBLcHuA8ePFjLly/XpEmTtGPHDo0ePVobN2687pzg1uBlt+nIkSNyOp3y8vLSd999pwceeEDTpk3T888/r2LFiunHH390zTB7NWXKlFFcXJwqVaqkF154QW+++aY6duyolStXuo1aq1atmjZt2qQOHTro888/11NPPaW1a9dq6NChWrRo0U28WtwKnE6nK3OAGcgczEbmYAVyB7OROc8x8uwmWb58uetZVoGBgapevbrmz5/vGt11zz336JlnntGgQYOUnp6uTp06aeTIkRo9erSky53hkydPqnfv3jp+/LiKFSum++67zzWipE6dOoqLi9NLL72kO++8U4ZhqFKlSnrggQeuq85mzZrJ19dXhmGoYcOGruURERG6dOmSAgICXA/Nv1EPPPCA9u7dq2HDhiktLU3dunXTk08+qRUrVlxz38yZIgsWLKhy5copKipKH3744XWPuCtYsKB++uknDR8+XPfdd5/Onj2rUqVKqW3btq6RaH369FFaWpqmTJmioUOHqlixYrr//vtdx5g0aZKeffZZffTRRypVqpTbKJ5MTz/9tJKTk/Xcc88pKSlJNWvW1OLFi90me8hNfx99t2nTJn3xxRcqV66cq7bmzZvriy++0Msvv6wXX3xRVapU0aJFi1SrVq2bUg9uLi+7XUeOHFFYWJjsdruKFCmijz/+WB9//PFV97tSViXpjjvuyNF/h9WqVdP8+fNvpGTkcZn/o5WZOeBmI3MwG5mDFcgdzEbmPGcz/vn0/2uw2+2uZ3ll2rNnj/bu3avo6Ogrn8Rm09KlSz2rFMjHbDabFi5cqK5du1pdylXFxsYqKipKp0+fVnBw8DW3T0lJUeHChZWcnJxvJw/wbtX72hv9fz7eXvpx3H/UqFEjeXvzdx+4+TIyMrRx40YyB9OQOZiNzMEK5A5mI3PubuR76A19anv27NGePXuyLF++fPkVt+ehdIDnevbsqaJFi7puM73V3HHHHdq3b5/VZQAAAAAAkKuuu3m2f//+m1EHgKvYvXu3JF3Xc97M9t133+nSpUuSlG9Hkd1sTqdTxYsXZ6g1TGO328kcTEXmYDYyByuQO5iNzHnuum/bBIDcwm2b13fbpiRl/DT7JlUCAAAAALe/G/keStsRAPIIb7tNe/fuZZYcmMbpdJI5mIrMwWxkDlYgdzAbmfMczTMAyCPsdrtOnDjBLz2Yxul0kjmYiszBbGQOViB3MBuZ8xzNMwAAAAAAACAbNM8AAAAAAACAbNA8A4A8wuF0qnTp0sySA9PY7XYyB1OROZiNzMEK5A5mI3Oe87a6AABAzjichkqXLm11GchHMv9HCzALmYPZyBysQO5gNjLnOdqOAJBHFPCya/v27XI4HFaXgnzC4XCQOZiKzMFsZA5WIHcwG5nzHM0zAMgjbDabkpOTZRiG1aUgnzAMg8zBVGQOZiNzsAK5g9nInOdongEAAAAAAADZoHkGAAAAAAAAZIPmGQDkERkOpypWrMgsOTCN3W4nczAVmYPZyBysQO5gNjLnOWbbBIA8wmkYKlGihNVlIB+x2+1kDqYiczAbmYMVyB3MRuY8R/MMACyU8dPsHG/rcDi0ZcsW1apVS15eXjexKuAyh8Ohbdu2kTmYhszBbGQOViB3MBuZ8xxj9gAgjzAMQ6mpqcySA9OQOZiNzMFsZA5WIHcwG5nzHM0zAAAAAAAAIBs0zwAAAAAAAIBs0DwDgDzCy8tL1atX5zkFMA2Zg9nIHMxG5mAFcgezkTnPMWEAAOQRNptNwcHBVpeBfITMwWxkDmYjc7ACuYPZyJznGHkGAHlERkaG4uPjlZGRYXUpyCfIHMxG5mA2MgcrkDuYjcx5juYZAOQhDofD6hKQz5A5mI3MwWxkDlYgdzAbmfMMzTMAAAAAAAAgGzTPAAAAAAAAgGzYDMMwrC4CQP6UkpKiwoULKzk5WUFBQVaXc8szDEOpqany9/eXzWazuhzkA2QOZiNzMBuZgxXIHcxG5tzdyPdQRp4BQB7i4+NjdQnIZ8gczEbmYDYyByuQO5iNzHmG5hkA5BEOh0MbN27kYZ8wDZmD2cgczEbmYAVyB7OROc/RPAMAAAAAAACyQfMMAAAAAAAAyAbNMwAAAAAAACAbzLYJwDLMtnl9DMOQw+GQl5cXs+TAFGQOZiNzMBuZgxXIHcxG5twx2yYA3OYuXrxodQnIZ8gczEbmYDYyByuQO5iNzHmG5hkA5BEOh0Nbt25llhyYhszBbGQOZiNzsAK5g9nInOdongEAAAAAAADZoHkGAAAAAAAAZIPmGQDkIV5eXlaXgHyGzMFsZA5mI3OwArmD2cicZ5htE4BlmG0TAAAAAGAmZtsEgNuYYRg6c+aM+DsPmIXMwWxkDmYjc7ACuYPZyJznaJ4BQB7hcDi0Y8cOZsmBacgczEbmYDYyByuQO5iNzHmO5hkAAAAAAACQDZpnAAAAAAAAQDZongFAHmGz2eTv7y+bzXbNbdPT0zV8+HCVLFlS/v7+ioiI0MqVK3N0nj///FM9evRQcHCwgoKC1KVLF+3bt++K286YMUM1atSQn5+fqlSponfeeee6rgm3tuvJHJAbyBzMRuZgBXIHs5E5zzHbJgDLMNvmzdOzZ08tWLBAQ4YMUZUqVTRr1izFx8crJiZGLVu2zHa/c+fOqUGDBkpOTtZzzz2nAgUKaMqUKTIMQwkJCSpatKhr2w8++EBPPPGEunXrpujoaK1evVqfffaZJkyYoOHDh5txmQAAAABwXW7keyjNMwCWoXl2fZxOp/766y8VK1ZMdnv2A4c3bNigiIgITZw4UUOHDpUkpaWlqVatWipRooTWrl2b7b5vvvmmhg8frg0bNqhx48aSpB07dqhWrVoaNmyYxo0bJ0lKTU1VmTJl1LRpUy1ZssS1/8MPP6xFixbp8OHDKlKkSG5cNiyU08wBuYXMwWxkDlYgdzAbmXN3I99D+dQA5IrY2FjZbDadOXPG6lJuW06nU/v27ZPT6bzqdgsWLJCXl5f69+/vWubn56d+/fpp3bp1Onz48FX3bdy4satxJknVq1dX27Zt9dVXX7mWxcTE6OTJkxowYIDb/gMHDtT58+e1dOnS67083IJymjkgt5A5mI3MwQrkDmYjc56jeQZNmDBBNptNQ4YMcS1LS0vTwIEDVbRoUQUEBKhbt246fvy42342my3La+7cuSZXn7tat27t9jlIl29jK1CgQJZre/DBB2Wz2XTgwAG35eXLl9fIkSNvcqVA9jZv3qyqVatm+VuUJk2aSJISEhKuuJ/T6dTWrVvVqFGjLOuaNGmivXv36uzZs65zSMqybcOGDWW3213rAQAAACCvo3mWz8XHx+uDDz5QnTp13JY/88wz+vbbbzV//nzFxcXp6NGjuu+++7LsP3PmTCUmJrpeXbt2Naly8wQEBKhRo0aKjY11Wx4bG6syZcq4Ld+/f78OHjyoNm3amFtkLjEMQxkZGVaXAQ8lJiYqPDw8y/LMZUePHr3ifqdOnVJ6enqO9k1MTJSXl5dKlCjhtp2Pj4+KFi2a7TkAAAAAIK+heZaPnTt3Tr169dJHH33k9myi5ORkzZgxQ5MnT1abNm3UsGFDzZw5U2vXrtUvv/zidozg4GCFhYW5Xn5+fpKkXbt2yWazaceOHW7bT5kyRZUqVbpmbadPn1avXr1UvHhx+fv7q0qVKpo5c6akK98emJCQ4DYKbNasWQoODtaSJUtUrVo1FSxYUPfff78uXLigTz/9VOXLl1eRIkX09NNPy+FwXLOeqKgotybZ9u3blZaWpieffNJteWxsrHx9fdWsWbNrHjM2NlZNmjRRoUKFFBwcrBYtWujgwYOu9RMmTFBoaKgCAwPVr18/vfDCC6pXr55r/ZVGyXXt2lV9+/Z1vf/ss8/UqFEjBQYGKiwsTA899JCSkpLcarDZbFq2bJkaNmwoX19f/fzzz3I6nRo/frwqVKggf39/1a1bVwsWLHA713fffaeqVavK399fUVFRWUbgXUl6erpSUlLcXsg5m82mwoULX3OWnNTUVPn6+mZZnvnfZ2pqarb7ScrRvqmpqfLx8bnicfz8/LI9B/KWnGYOyC1kDmYjc7ACuYPZyJznaJ7lYwMHDlSnTp3Url07t+WbNm3SpUuX3JZXr15dZcuW1bp167Ico1ixYmrSpIk++eQTZc4/UbVqVTVq1Ehz5sxx237OnDl66KGHrlnbyJEj9ccff2jZsmXavn27pk+frmLFil3X9V24cEHTpk3T3LlztXz5csXGxuree+/Vd999p++++06fffaZPvjggyxNoSuJiorSzp07lZiYKEmuGQvbtGnj1jyLiYlRs2bNXI2G7GRkZKhr166KjIzU1q1btW7dOvXv39/1w+yrr77S6NGjNW7cOG3cuFHh4eF67733ruv6JenSpUsaM2aMtmzZokWLFunAgQNuzbVML7zwgiZMmKDt27erTp06Gj9+vGbPnq33339fv//+u5555hk9/PDDiouLkyQdPnxY9913nzp37qyEhAQ99thjeuGFF65Zz/jx41W4cGHXq0yZMtd9TfmZl5eXatSoIS8vr6tu5+/vr/T09CzL09LSXOuz209Sjvb19/fXxYsXr3ictLS0bM+BvCWnmQNyC5mD2cgcrEDuYDYy5zlvqwuANebOnatff/1V8fHxWdYdO3ZMPj4+Cg4OdlseGhqqY8eOud6/9tpratOmjQoWLKjvv/9eAwYM0Llz5/T0009Lknr16qV3331XY8aMkXR5NNqmTZv0+eefX7O+Q4cOqX79+q7nKZUvX/66r/HSpUuaPn26a6Tb/fffr88++0zHjx9XQECAatasqaioKMXExOiBBx646rFatGghHx8fxcbGqmfPnoqNjVVkZKQaNmyov/76S/v371eFChUUFxenfv36XbO2lJQUJScn61//+pervho1arjWT506Vf369XMda+zYsfrhhx9cDYyc+ve//+3694oVK2ratGlq3Lixzp07p4CAANe61157TXfddZeky42TcePG6YcffnCNoKtYsaJ+/vlnffDBB4qMjHR9rpMmTZIkVatWTb/99pveeOONq9YzYsQIPfvss26fAw20nHM6nTp69KhKlix51VlywsPD9eeff2ZZntn8LVmy5BX3CwkJka+vr2u7q+0bHh4uh8OhpKQkt1s3L168qJMnT2Z7DuQtOc0ckFvIHMxG5mAFcgezkTnP8anlQ4cPH9bgwYM1Z86ca46QupqRI0eqRYsWql+/voYPH65hw4Zp4sSJrvUPPvigDhw44LrVc86cOWrQoIGqV69+zWM/+eSTmjt3rurVq6dhw4Zp7dq1111fwYIF3W4RDQ0NVfny5d2aRqGhoW63MV7tWI0bN3aNMouLi1Pr1q3l7e2t5s2bKzY2Vvv27dOhQ4cUFRV1zeOFhISob9++io6OVufOnfX222+7NSy2b9+uiIgIt31ycivoP23atEmdO3dW2bJlFRgYqMjISEmXm5N/9/eHvu/Zs0cXLlzQXXfdpYCAANdr9uzZ2rt3r0f1+fr6KigoyO2FnHM6nTpy5Mg1Z8mpV6+edu3aleW22PXr17vWX4ndblft2rW1cePGLOvWr1+vihUrKjAw0O0Y/9x248aNcjqd2Z4DeUtOMwfkFjIHs5E5WIHcwWxkznM0z/KhTZs2KSkpSQ0aNJC3t7e8vb0VFxenadOmydvbW6Ghobp48aLbM8Uk6fjx4woLC8v2uBERETpy5Ijrlq+wsDC1adNGX3zxhSTpiy++UK9evXJU4913362DBw/qmWee0dGjR9W2bVsNHTpUklyd8sxbRKXLo8z+qUCBAm7vbTbbFZfl9AdI5ii133//XampqWrQoIEkKTIyUjExMYqJiVHBggWzNJWyM3PmTK1bt07NmzfXvHnzVLVq1SzPlLsau93u9hlI7p/D+fPnFR0draCgIM2ZM0fx8fFauHChJGW53a5QoUKufz937pwkaenSpUpISHC9/vjjjxzd4grr3X///XI4HPrwww9dy9LT0zVz5kxFRES4RvsdOnQoy3MJ77//fsXHx7s1xXbu3Kkff/xR3bt3dy1r06aNQkJCNH36dLf9p0+froIFC6pTp04349IAAAAAwHQ0z/Khtm3b6rfffnNrjDRq1Ei9evVy/XuBAgW0atUq1z47d+7UoUOHrjq6KCEhQUWKFHF72HivXr00b948rVu3Tvv27dODDz6Y4zqLFy+uPn366PPPP9fUqVNdjYDixYtLkttIrYSEhBwf90ZFRUVp9+7d+uKLL9SyZUvX/eKtWrVSXFycYmNjXbd35lT9+vU1YsQIrV27VrVq1XI1GmvUqOEaJZTpn4214sWLu30GDodD27Ztc73fsWOHTp48qQkTJujOO+9U9erVczTKrmbNmvL19dWhQ4dUuXJlt1dm06VGjRrasGHDVeuDdSIiItS9e3eNGDFCw4YN04cffqg2bdrowIEDevPNN13b9e7d2+12YUkaMGCAKlWqpE6dOmnixImaOnWq7rrrLoWGhuq5555zbefv768xY8ZoyZIl6t69uz7++GPXf68vvfSSQkJCTLteAAAAALiZeOZZPhQYGKhatWq5LStUqJCKFi3qWt6vXz89++yzCgkJUVBQkJ566ik1a9ZMTZs2lSR9++23On78uJo2bSo/Pz+tXLlS48aNc40Oy3TffffpySef1JNPPqmoqKgcPwfplVdeUcOGDXXHHXcoPT1dS5YscX3Jz2zijB49Wq+//rp27drlevZWbjhx4kSWZlx4eLiaN28uX19fvfPOO3rppZdc65o0aaKkpCR98803GjFiRI7OsX//fn344Ye65557VLJkSe3cuVO7d+9W7969JUmDBw9W37591ahRI7Vo0UJz5szR77//rooVK7qO0aZNGz377LNaunSpKlWqpMmTJ7uNFixbtqx8fHz0zjvv6IknntC2bdtcz5+7msDAQA0dOlTPPPOMnE6nWrZsqeTkZK1Zs0ZBQUHq06ePnnjiCU2aNEnPP/+8HnvsMW3atEmzZs3K0bXjxtntdhUvXjxHzymYPXu2Ro4cqc8++0ynT59WnTp1tGTJErVq1eqq+wUGBio2NlbPPPOMxo4dK6fTqdatW2vKlCmuxnWmAQMGqECBApo0aZIWL16sMmXKaMqUKRo8eLBH14lbx/VkDsgNZA5mI3OwArmD2chcLjAAwzAiIyONwYMHu96npqYaAwYMMIoUKWIULFjQuPfee43ExETX+mXLlhn16tUzAgICjEKFChl169Y13n//fcPhcGQ5do8ePQxJxieffJLjesaMGWPUqFHD8Pf3N0JCQowuXboY+/btc63/+eefjdq1axt+fn7GnXfeacyfP9+QZOzfv98wDMOYOXOmUbhwYbdjjho1yqhbt67bsj59+hhdunRx+xwkZXmNGTPGbf0vv/zidpzWrVsbkox169bl6PqOHTtmdO3a1QgPDzd8fHyMcuXKGa+88orb5/f6668bxYoVMwICAow+ffoYw4YNc6v/4sWLxpNPPmmEhIQYJUqUMMaPH2906dLF6NOnj2ubL774wihfvrzh6+trNGvWzFi8eLEhydi8ebNhGIYRExNjSDJOnz7tVp/T6TSmTp1qVKtWzShQoIBRvHhxIzo62oiLi3Nt8+233xqVK1c2fH19jTvvvNP45JNPrnisq0lOTjYkGcnJyTneBwAAAACAG3Uj30NthvGPhyYBuCWNHj1aixYtMuUWVbOkpKSocOHCSk5OZvKAHHA6na6ZXflbI5iBzMFsZA5mI3OwArmD2cicuxv5HsqnBgB5hNPp1IkTJ5glB6YhczAbmYPZyBysQO5gNjLnOZpnsMQTTzyhgICAK76eeOIJq8vLFdldX0BAgFavXm11eQAAAAAAIAeYMACWeO2117JMLpDpdrl972q3V5YqVeq6jzd69GiNHj36xgsCAAAAAADXjeYZLFGiRAmVKFHC6jJuqsqVK1tdAm4zdrtdpUuX5jkFMA2Zg9nIHMxG5mAFcgezkTnPMWEAAMswYQAAAAAAwExMGAAAtzGHw6Ht27fL4XBYXQryCTIHs5E5mI3MwQrkDmYjc56jeQYAeYRhGEpOThYDhmEWMgezkTmYjczBCuQOZiNznqN5BgAAAAAAAGSD5hkAAAAAAACQDZpnAJBH2O12VaxYkVlyYBoyB7OROZiNzMEK5A5mI3OeY7ZNAJZhtk0AAAAAgJmYbRMAbmMOh0NbtmxhlhyYhszBbGQOZiNzsAK5g9nInOdongFAHmEYhlJTU5klB6YhczAbmYPZyBysQO5gNjLnOZpnAAAAAAAAQDZongEAAAAAAADZoHkGAHmEl5eXqlevLi8vL6tLQT5B5mA2MgezkTlYgdzBbGTOc95WFwAAyBmbzabg4GCry0A+QuZgNjIHs5E5WIHcwWxkznOMPAOAPCIjI0Px8fHKyMiwuhTkE2QOZiNzMBuZgxXIHcxG5jxH8wwA8hCml4bZyBzMRuZgNjIHK5A7mI3MeYbmGQAAAAAAAJANmmcAAAAAAABANmyGYRhWFwEgf0pJSVHhwoWVnJysoKAgq8u55RmGodTUVPn7+8tms1ldDvIBMgezkTmYjczBCuQOZiNz7m7keygjzwAgD/Hx8bG6BOQzZA5mI3MwG5mDFcgdzEbmPEPzDADyCIfDoY0bN/KwT5iGzMFsZA5mI3OwArmD2cic52ieAQAAAAAAANmgeQYAAAAAAABkg+YZAAAAAAAAkA1m2wRgGWbbvD6GYcjhcMjLy4tZcmAKMgezkTmYjczBCuQOZiNz7phtEwBucxcvXrS6BOQzZA5mI3MwG5mDFcgdzEbmPEPzDADyCIfDoa1btzJLDkxD5mA2MgezkTlYgdzBbGTOczTPAAAAAAAAgGzQPAMAAAAAAACyQfMMAPIQLy8vq0tAPkPmYDYyB7OROViB3MFsZM4zzLYJwDLMtgkAAAAAMBOzbQLAbcwwDJ05c0b8nQfMQuZgNjIHs5E5WIHcwWxkznM0zwAgj3A4HNqxYwez5MA0ZA5mI3MwG5mDFcgdzEbmPEfzDAAAAAAAAMgGzTMAAAAAAAAgGzTPACCPsNls8vf3l81ms7oU5BNkDmYjczAbmYMVyB3MRuY8x2ybACzDbJsAAAAAADMx2yYA3MacTqeSkpLkdDqtLgX5BJmD2cgczEbmYAVyB7OROc/RPAOAPMLpdGrfvn380oNpyBzMRuZgNjIHK5A7mI3MeY7mGQDchtLT0zV8+HCVLFlS/v7+ioiI0MqVK3O0759//qkePXooODhYQUFB6tKli/bt23fFbWfMmKEaNWrIz89PVapU0TvvvJOblwEAAAAAlqN5BgC3ob59+2ry5Mnq1auX3n77bXl5ealjx476+eefr7rfuXPnFBUVpbi4OL344ot69dVXtXnzZkVGRurkyZNu237wwQd67LHHdMcdd+idd95Rs2bN9PTTT+uNN964mZcGAAAAAKbytroAAEDO2Gw2FS5c+Jqz5GzYsEFz587VxIkTNXToUElS7969VatWLQ0bNkxr167Ndt/33ntPu3fv1oYNG9S4cWNJ0t13361atWpp0qRJGjdunCQpNTVVL730kjp16qQFCxZIkh5//HE5nU6NGTNG/fv3V5EiRXLjsmGhnGYOyC1kDmYjc7ACuYPZyJznGHmGW9ro0aNVr149q8vIVvny5TV16lSry7hltG7dWkOGDLG6jNuWl5eXatSoIS8vr6tut2DBAnl5eal///6uZX5+furXr5/WrVunw4cPX3Xfxo0buxpnklS9enW1bdtWX331lWtZTEyMTp48qQEDBrjtP3DgQJ0/f15Lly693svDLSinmQNyC5mD2cgcrEDuYDYy5zmaZ3nAiRMn9OSTT6ps2bLy9fVVWFiYoqOjtWbNGkmXu8iLFi2ytkgAN53T6dSRI0eu+aDPzZs3q2rVqlmmXW7SpIkkKSEhIdvjb926VY0aNcqyrkmTJtq7d6/Onj3rOoekLNs2bNhQdrvdtR55W04zB+QWMgezkTlYgdzBbGTOczTP8oBu3bpp8+bN+vTTT7Vr1y4tXrxYrVu3zvL8oau5ePHiTaww9xmGoYyMjBvaN69d662Gz+/WldNfeomJiQoPD8+yPHPZ0aNHr7jfqVOnlJ6enqN9ExMT5eXlpRIlSrht5+Pjo6JFi2Z7DuQt/I8WzEbmYDYyByuQO5iNzHmO5tkt7syZM1q9erXeeOMNRUVFqVy5cmrSpIlGjBihe+65R+XLl5ck3XvvvbLZbK73mbc7fvzxx6pQoYL8/PwkSYcOHVKXLl0UEBCgoKAg9ejRQ8ePH3edL3O/Tz75RGXLllVAQIAGDBggh8OhN998U2FhYSpRooRef/111z4PPfSQHnjgAbe6L126pGLFimn27NmSLv/HOn78eFWoUEH+/v6qW7eu6zlJkhQbGyubzaZly5apYcOG8vX1dXuw+QcffKAyZcqoYMGC6tGjh5KTk13r+vbtq65du+r1119XyZIlVa1aNUnSZ599pkaNGikwMFBhYWF66KGHlJSU5Npv1qxZCg4Odqt70aJFWe4D//bbb9W4cWP5+fmpWLFiuvfee93WX7hwQf/+978VGBiosmXL6sMPP8z+D/RvLl68qEGDBik8PFx+fn4qV66cxo8f71q/e/dutWrVSn5+fqpZs6ZWrlzpNsow8zM7c+aMa5+EhATZbDYdOHBAknTy5En17NlTpUqVUsGCBVW7dm19+eWXbnW0bt1agwYN0pAhQ1SsWDFFR0dLkrZt26a7775bAQEBCg0N1SOPPKK//vrLtd/58+fVu3dvBQQEKDw8XJMmTcrRdePmS01Nla+vb5blmT8HUlNTs91PUo72TU1NlY+PzxWP4+fnl+05AAAAACCvoXl2iwsICFBAQIAWLVqk9PT0LOvj4+MlSTNnzlRiYqLrvSTt2bNHX3/9tf73v/8pISFBTqdTXbp00alTpxQXF6eVK1dq3759WRpfe/fu1bJly7R8+XJ9+eWXmjFjhjp16qQjR44oLi5Ob7zxhl5++WWtX79ektSrVy99++23OnfunOsYK1as0IULF1yNpvHjx2v27Nl6//339fvvv+uZZ57Rww8/rLi4OLdzv/DCC5owYYK2b9+uOnXquK7jq6++0rfffqvly5dr8+bNWZ6ztGrVKu3cuVMrV67UkiVLJF1u4I0ZM0ZbtmzRokWLdODAAfXt2/e6Pv+lS5fq3nvvVceOHbV582atWrXKdetbpkmTJqlRo0auup588knt3LnzmseeNm2aFi9erK+++ko7d+7UnDlzXM1Pp9Op++67Tz4+Plq/fr3ef/99DR8+/Lpql6S0tDQ1bNhQS5cu1bZt29S/f3898sgj2rBhg9t2n376qXx8fLRmzRq9//77OnPmjNq0aaP69etr48aNWr58uY4fP64ePXq49nn++ecVFxenb775Rt9//71iY2P166+/XrWe9PR0paSkuL2Q+/z9/a/48yItLc21Prv9JOVoX39//2xHKaalpWV7DgAAAADIcwzc8hYsWGAUKVLE8PPzM5o3b26MGDHC2LJli2u9JGPhwoVu+4waNcooUKCAkZSU5Fr2/fffG15eXsahQ4dcy37//XdDkrFhwwbXfgULFjRSUlJc20RHRxvly5c3HA6Ha1m1atWM8ePHG4ZhGJcuXTKKFStmzJ4927W+Z8+exgMPPGAYhmGkpaUZBQsWNNauXetWY79+/YyePXsahmEYMTExhiRj0aJFWa7Dy8vLOHLkiGvZsmXLDLvdbiQmJhqGYRh9+vQxQkNDjfT09Kt+jvHx8YYk4+zZs4ZhGMbMmTONwoULu22zcOFC4+//WTRr1szo1atXtscsV66c8fDDD7veO51Oo0SJEsb06dOvWothGMZTTz1ltGnTxnA6nVnWrVixwvD29jb+/PNP17Jly5a5/VlnfmanT592bbN582ZDkrF///5sz9upUyfjueeec72PjIw06tev77bNmDFjjPbt27stO3z4sCHJ2Llzp3H27FnDx8fH+Oqrr1zrT548afj7+xuDBw/O9tyjRo0yJGV5JScnZ7sP/o/D4TD27Nnj9t/ilbRr186oUaNGluU//PCDIclYvHhxtsf39fU1nnzyySzrXn75ZUOS62fD2LFjDUnG8ePH3bZLT0837Ha78eyzz+b0snALy2nmgNxC5mA2MgcrkDuYjcy5S05Ovu7voYw8ywO6deumo0ePavHixerQoYNiY2PVoEEDzZo166r7lStXTsWLF3e93759u8qUKaMyZcq4ltWsWVPBwcHavn27a1n58uUVGBjoeh8aGqqaNWvKbre7Lcu8BdLb21s9evTQnDlzJF2+ne+bb75Rr169JF0eOXbhwgXdddddrpF0AQEBmj17tvbu3etW85UeVF62bFmVKlXK9b5Zs2ZyOp1uo7tq166d5RayTZs2qXPnzipbtqwCAwMVGRkp6fKtqzmVkJCgtm3bXnWbzBFy0uXJG8LCwtxuD81O3759lZCQoGrVqunpp5/W999/71qX+WdVsmRJ17JmzZrluO5MDodDY8aMUe3atRUSEqKAgACtWLEiy2fQsGFDt/dbtmxRTEyM259X9erVJV0embh3715dvHhRERERrn1CQkJct8xmZ8SIEUpOTna9rjbrI7Ky2+2qVKmS23+LV1KvXj3t2rUry8i+zNGi2c1ga7fbVbt2bW3cuDHLuvXr16tixYqunw2Zx/jnths3bpTT6bylZ8lFzuU0c0BuIXMwG5mDFcgdzEbmPMcnl0f4+fnprrvu0siRI7V27Vr17dtXo0aNuuo+hQoVuqFzFShQwO29zWa74rK/P2ywV69eWrVqlZKSkrRo0SL5+/urQ4cOkuS6nXPp0qVKSEhwvf744w+35555UvM/9zt//ryio6MVFBSkOXPmKD4+XgsXLpT0fw/Et9vtMgzDbb9Lly65vc/JrWfX+myy06BBA+3fv19jxoxRamqqevToofvvv/+a+2XK/MH392v4Z/0TJ07U22+/reHDhysmJkYJCQmKjo7OcrvdPz+/c+fOqXPnzm5/XgkJCa7nsN0oX19fBQUFub2Qc06nU3v37r1mvu6//345HA635++lp6dr5syZioiIcDXQDx06pB07dmTZNz4+3q0ptnPnTv3444/q3r27a1mbNm0UEhKi6dOnu+0/ffp0FSxYUJ06dbrh68StI6eZA3ILmYPZyBysQO5gNjLnOZpneVTNmjV1/vx5SZebNw6H45r71KhRQ4cPH3Yb7fPHH3/ozJkzqlmzpkf1NG/eXGXKlNG8efM0Z84cde/e3dVUqlmzpnx9fXXo0CFVrlzZ7fX3UXDZOXTokNvMfb/88ovsdvtVRznt2LFDJ0+e1IQJE3TnnXeqevXqWUaDFS9eXGfPnnV9jtLlkWZ/V6dOHa1atSonH8ENCQoK0gMPPKCPPvpI8+bN09dff61Tp065/qwSExNd2/7yyy9Z6pfkts0/61+zZo26dOmihx9+WHXr1lXFihW1a9eua9bVoEED/f777ypfvnyWP7NChQqpUqVKKlCggGskkySdPn06R8fGjXM6nTpx4sQ1f+lFRESoe/fuGjFihIYNG6YPP/xQbdq00YEDB/Tmm2+6tuvdu7dq1Kjhtu+AAQNUqVIlderUSRMnTtTUqVN11113KTQ0VM8995xrO39/f40ZM0ZLlixR9+7d9fHHH6tPnz76/PPP9dJLLykkJCR3Lx6WyGnmgNxC5mA2MgcrkDuYjcx5ztvqAnB1J0+eVPfu3fXvf/9bderUUWBgoDZu3Kg333xTXbp0kXT5NstVq1apRYsW8vX1VZEiRa54rHbt2ql27drq1auXpk6dqoyMDA0YMECRkZFXvF3yej300EN6//33tWvXLsXExLiWBwYGaujQoXrmmWfkdDrVsmVLJScna82aNQoKClKfPn2uelw/Pz/16dNHb731llJSUvT000+rR48eCgsLy3afsmXLysfHR++8846eeOIJbdu2TWPGjHHbJiIiQgULFtSLL76op59+WuvXr89yK+yoUaPUtm1bVapUSQ8++KAyMjL03Xff3dDD+/9p8uTJCg8PV/369WW32zV//nyFhYUpODhY7dq1U9WqVdWnTx9NnDhRKSkpeumll9z2z2w+jh49Wq+//rp27dqVZcbLKlWqaMGCBVq7dq2KFCmiyZMn6/jx49dslg4cOFAfffSRevbsqWHDhikkJER79uzR3Llz9fHHHysgIED9+vXT888/r6JFi6pEiRJ66aWXGAZ8C5k9e7ZGjhypzz77TKdPn1adOnW0ZMmSa44cDAwMVGxsrJ555hmNHTtWTqdTrVu31pQpU9xuA5cuN9oKFCigSZMmafHixSpTpoymTJmiwYMH38xLAwAAAABT8U33FhcQEKCIiAhNmTJFrVq1Uq1atTRy5Eg9/vjjevfddyVdnu1x5cqVKlOmjOrXr5/tsWw2m7755hsVKVJErVq1Urt27VSxYkXNmzcvV2rt1auX/vjjD5UqVUotWrRwWzdmzBiNHDlS48ePV40aNdShQwctXbpUFSpUuOZxK1eurPvuu08dO3ZU+/btVadOHb333ntX3ad48eKaNWuW5s+fr5o1a2rChAl666233LYJCQnR559/ru+++061a9fWl19+qdGjR7tt07p1a82fP1+LFy9WvXr11KZNmywzVd6owMBAvfnmm2rUqJEaN26sAwcO6LvvvpPdbpfdbtfChQuVmpqqJk2a6LHHHtPrr7/utn+BAgX05ZdfaseOHapTp47eeOMNjR071m2bl19+WQ0aNFB0dLRat26tsLAwde3a9Zq1lSxZUmvWrJHD4VD79u1Vu3ZtDRkyRMHBwa4G2cSJE3XnnXeqc+fOateunVq2bJnl2Wmwjp+fnyZOnKjExESlpaVpw4YNio6OdtsmNjY2y63LklS6dGnNnz9fycnJOnv2rL799ltVrlz5iud5/PHHtWPHDqWnp2vPnj0aMmSIbDbbTbkmAAAAALCCzbjSNycAtySbzaaFCxfmqAGWF6SkpKhw4cJKTk7m+Wc54HQ6dfToUZUsWZJRfjAFmYPZyBzMRuZgBXIHs5E5dzfyPZTbNgEgj7Db7SpdurTVZSAfIXMwG5mD2cgcrEDuYDYy5zlajsBNMm7cOAUEBFzxdffdd1tdHvIgh8Oh7du352iCECA3kDmYjczBbGQOViB3MBuZ8xwjz4Cb5IknnlCPHj2uuM7f3/+Gjsld1vmbYRhKTk4mBzANmYPZyBzMRuZgBXIHs5E5z9E8A26SkJAQhYSEWF0GAAAAAADwALdtAgAAAAAAANmgeQYAeYTdblfFihWZIQemIXMwG5mD2cgcrEDuYDYy5zmbwU2vACxyI1MEAwAAAABwo27keyhtRwDIIxwOh7Zs2cIsOTANmYPZyBzMRuZgBXIHs5E5z9E8A4A8wjAMpaamMksOTEPmYDYyB7OROViB3MFsZM5zNM8AAAAAAACAbNA8AwAAAAAAALJB8wwA8ggvLy9Vr15dXl5eVpeCfILMwWxkDmYjc7ACuYPZyJznvK0uAACQMzabTcHBwVaXgXyEzMFsZA5mI3OwArmD2cic5xh5BgB5REZGhuLj45WRkWF1KcgnyBzMRuZgNjIHK5A7mI3MeY7mGQDkIUwvDbOROZiNzMFsZA5WIHcwG5nzDM0zAAAAAAAAIBs0zwAAAAAAAIBs2AzDMKwuAkD+lJKSosKFCys5OVlBQUFWl3PLMwxDqamp8vf3l81ms7oc5ANkDmYjczAbmYMVyB3MRubc3cj3UEaeAUAe4uPjY3UJyGfIHMxG5mA2MgcrkDuYjcx5huYZAOQRDodDGzdu5GGfMA2Zg9nIHMxG5mAFcgezkTnP0TwDAAAAAAAAskHzDAAAAAAAAMgGzTMAAAAAAAAgG8y2CcAyzLZ5fQzDkMPhkJeXF7PkwBRkDmYjczAbmYMVyB3MRubcMdsmANzmLl68aHUJyGfIHMxG5mA2MgcrkDuYjcx5huYZAOQRDodDW7duZZYcmIbMwWxkDmYjc7ACuYPZyJznaJ4BAAAAAAAA2aB5BgAAAAAAAGSD5hkA5CFeXl5Wl4B8hszBbGQOZiNzsAK5g9nInGeYbROAZZhtEwAAAABgJmbbBIDbmGEYOnPmjPg7D5iFzMFsZA5mI3OwArmD2cic52ieAUAe4XA4tGPHDmbJgWnIHMxG5mA2MgcrkDuYjcx5juYZAAAAAAAAkA2aZwAAAAAAAEA2aJ4BQB5hs9nk7+8vm81mdSnIJ8gczEbmYDYyByuQO5iNzHmO2TYBWIbZNgEAAAAAZmK2TQC4jTmdTiUlJcnpdFpdCvIJMgezkTmYjczBCuQOZiNznqN5BgB5hNPp1L59+/ilB9OQOZiNzMFsZA5WIHcwG5nzHM0zAAAAAAAAIBs0zwAAAAAAAIBs0DwDgDzCZrOpcOHCzJID05A5mI3MwWxkDlYgdzAbmfMcs20CsAyzbQIAAAAAzMRsmwBwG3M6nTpy5EiOHvSZnp6u4cOHq2TJkvL391dERIRWrlyZo/P8+eef6tGjh4KDgxUUFKQuXbpo3759V9x2xowZqlGjhvz8/FSlShW9884713VNuLVdT+aA3EDmYDYyByuQO5iNzHmO5hkA5BHX80uvb9++mjx5snr16qW3335bXl5e6tixo37++eer7nfu3DlFRUUpLi5OL774ol599VVt3rxZkZGROnnypNu2H3zwgR577DHdcccdeuedd9SsWTM9/fTTeuONNzy6Ttw6+B8tmI3MwWxkDlYgdzAbmfOct9UFAABy14YNGzR37lxNnDhRQ4cOlST17t1btWrV0rBhw7R27dps933vvfe0e/dubdiwQY0bN5Yk3X333apVq5YmTZqkcePGSZJSU1P10ksvqVOnTlqwYIEk6fHHH5fT6dSYMWPUv39/FSlS5CZfKQAAAADcfIw8A5BrWrdurSFDhlhdRr63YMECeXl5qX///q5lfn5+6tevn9atW6fDhw9fdd/GjRu7GmeSVL16dbVt21ZfffWVa1lMTIxOnjypAQMGuO0/cOBAnT9/XkuXLs3FKwIAAAAA69A8y8cmTJggm83m1uz48MMP1bp1awUFBclms+nMmTNu+8TGxspms13xFR8fb+4F5KIDBw64XUvRokXVvn17bd68WZL0wgsvqHr16m777NixQzabTX379nVbPmvWLPn6+io1NdWs8pFP2O12FS9eXHb71X90b968WVWrVs3y8MsmTZpIkhISEq64n9Pp1NatW9WoUaMs65o0aaK9e/fq7NmzrnNIyrJtw4YNZbfbXeuRt+U0c0BuIXMwG5mDFcgdzEbmPMcnl0/Fx8frgw8+UJ06ddyWX7hwQR06dNCLL754xf2aN2+uxMREt9djjz2mChUqXPELd17zww8/KDExUStWrNC5c+d0991368yZM4qKitLOnTt17Ngx17YxMTEqU6aMYmNj3Y4RExOjpk2byt/f3+Tqc8fFixetLgHZsNvtqlSp0jV/6SUmJio8PDzL8sxlR48eveJ+p06dUnp6eo72TUxMlJeXl0qUKOG2nY+Pj4oWLZrtOZC35DRzQG4hczAbmYMVyB3MRuY8xyeXD507d069evXSRx99lOWZREOGDNELL7ygpk2bXnFfHx8fhYWFuV5FixbVN998o0cffVQ2m03S5ZFXwcHBWrFihWrUqKGAgAB16NBBiYmJOaovNjZWTZo0UaFChRQcHKwWLVro4MGDki4/BL1r165Zam7durXrfevWrfXUU09pyJAhKlKkiEJDQ/XRRx/p/PnzevTRRxUYGKjKlStr2bJlWc5dtGhRhYWFqVGjRnrrrbd0/PhxrV+/Xi1btlSBAgXcGmWxsbEaOHCgTp06pQMHDrgtj4qKuuZ1Xrx4UYMGDVJ4eLj8/PxUrlw5jR8/3rV+9+7datWqlfz8/FSzZk2tXLlSNptNixYtcp3nn6MDExISZLPZXPWcPHlSPXv2VKlSpVSwYEHVrl1bX375pVsdrVu31qBBgzRkyBAVK1ZM0dHRkqRt27bp7rvvVkBAgEJDQ/XII4/or7/+cu13/vx59e7dWwEBAQoPD9ekSZOuec3wjNPp1N69e6/5oM/U1FT5+vpmWe7n5+dan91+knK0b2pqqnx8fK54HD8/P0Ze3iZymjkgt5A5mI3MwQrkDmYjc56jeZYPDRw4UJ06dVK7du08PtbixYt18uRJPfroo27LL1y4oLfeekufffaZfvrpJx06dMj14PKrycjIUNeuXRUZGamtW7dq3bp16t+/v6sxl1OffvqpihUrpg0bNuipp57Sk08+qe7du6t58+b69ddf1b59ez3yyCO6cOFCtsfIHDl28eJFFSpUSI0bN1ZMTIxrfWxsrNq2basWLVq4lu/bt0+HDh3KUfNs2rRpWrx4sb766ivt3LlTc+bMUfny5SVd/uF23333ycfHR+vXr9f777+v4cOHX9dnIElpaWlq2LChli5dqm3btql///565JFHtGHDBrftPv30U/n4+GjNmjV6//33debMGbVp00b169fXxo0btXz5ch0/flw9evRw7fP8888rLi5O33zzjb7//nvFxsbq119/vWo96enpSklJcXsh55xOp06cOHHNX3r+/v5KT0/PsjwtLc21Prv9JOVoX39//2xHKaalpeXZkZdwl9PMAbmFzMFsZA5WIHcwG5nzHLNt5jNz587Vr7/+mmvPJ5sxY4aio6NVunRpt+WXLl3S+++/r0qVKkmSBg0apNdee+2ax0tJSVFycrL+9a9/ufatUaPGdddVt25dvfzyy5KkESNGaMKECSpWrJgef/xxSdIrr7yi6dOna+vWrVccZXfmzBmNGTNGAQEBrudERUVFaf78+ZKkP/74Q2lpaapfv75atWql2NhYPfroo4qNjZWfn1+2I/f+7tChQ6pSpYpatmwpm82mcuXKudb98MMP2rFjh1asWKGSJUtKksaNG6e77777uj6HUqVKuTUtn3rqKa1YsUJfffWV67okqUqVKnrzzTdd78eOHav69eu7ZlaUpE8++URlypTRrl27VLJkSc2YMUOff/652rZtK+lyA+6fOfin8ePH69VXX72ua8D1Cw8P159//plleeboz8xM/VNISIh8fX2vOEr0n/uGh4fL4XAoKSnJ7dbNixcv6uTJk9meAwAAAADyGkae5SOHDx/W4MGDNWfOHNctWJ44cuSIVqxYoX79+mVZV7BgQVfzS7r8RTspKemaxwwJCVHfvn0VHR2tzp076+23387x7Z5/9/dnuXl5ealo0aKqXbu2a1loaKgkZampefPmCggIUJEiRbRlyxbNmzfPtW3r1q21a9cuJSYmKjY2Vi1btpSXl5ciIyNdt3PGxsaqefPmV7zt7Z/69u2rhIQEVatWTU8//bS+//5717rt27erTJkybg2IZs2aXffn4HA4NGbMGNWuXVshISEKCAjQihUrdOjQIbftGjZs6PZ+y5YtiomJUUBAgOuVOWHC3r17tXfvXl28eFERERGufUJCQlStWrWr1jNixAglJye7Xleb9RE3rl69etq1a1eWkX3r1693rb8Su92u2rVra+PGjVnWrV+/XhUrVlRgYKDbMf657caNG+V0OrM9BwAAAADkNTTP8pFNmzYpKSlJDRo0kLe3t7y9vRUXF6dp06bJ29tbDofjuo43c+ZMFS1aVPfcc0+WdQUKFHB7b7PZZBhGjo+7bt06NW/eXPPmzVPVqlX1yy+/SLr85f6fx7l06VKOzv/3ZZm3gf5z2Oq8efO0ZcsWnT59Wnv37lXHjh1d61q0aCEfHx/FxMQoJiZGkZGRkqTGjRvrr7/+0r59+xQbG6s2bdrk6DobNGig/fv3a8yYMUpNTVWPHj10//3352hfSa6HPf798/jnZzFx4kS9/fbbGj58uGJiYpSQkKDo6Ogst9sVKlTI7f25c+fUuXNnJSQkuL0yn8N2o3x9fRUUFOT2Qs7Z7XaVLl36mg/6vP/+++VwOPThhx+6lqWnp2vmzJmKiIhQmTJlJF0e/bhjx44s+8bHx7s1xXbu3Kkff/xR3bt3dy1r06aNQkJCNH36dLf9p0+froIFC6pTp043fJ24deQ0c0BuIXMwG5mDFcgdzEbmPMdtm/lI27Zt9dtvv7kte/TRR1W9enUNHz5cXl5eOT6WYRiaOXOmevfunaVRlRvq16+v+vXra8SIEWrWrJm++OILNW3aVMWLF9e2bdvctk1ISMi1GsqUKeM2Yu7v/P39FRERodjYWMXFxen555+XdLlR17RpU82YMUOHDx/O0fPOMgUFBemBBx7QAw88oPvvv18dOnTQqVOnVKNGDR0+fNht1sTMBmKm4sWLS7p8O13mxA8JCQlu26xZs0ZdunTRww8/LOlys3DXrl2qWbPmVetq0KCBvv76a5UvX17e3ll/TFSqVEkFChTQ+vXrVbZsWUnS6dOntWvXLldTEbkv85fetURERKh79+4aMWKEkpKSVLlyZX366ac6cOCAZsyY4dqud+/eiouLc2vADhgwQB999JE6deqkoUOHqkCBApo8ebJCQ0P13HPPubbz9/fXmDFjNHDgQHXv3l3R0dFavXq1Pv/8c73++usKCQnJ3YuHJXKaOSC3kDmYjczBCuQOZiNznqPtmI8EBgaqVq1abq9ChQqpaNGiqlWrliTp2LFjSkhI0J49eyRJv/32mxISEnTq1Cm3Y/3444/av3+/HnvssVytcf/+/RoxYoTWrVungwcP6vvvv9fu3btdzz1r06aNNm7cqNmzZ2v37t0aNWpUlmbazRQVFaW5c+cqLS1NDRo0cC2PjIzUO++845pYICcmT56sL7/8Ujt27NCuXbs0f/58hYWFKTg4WO3atVPVqlXVp08fbdmyRatXr9ZLL73ktn/lypVVpkwZjR49Wrt379bSpUuzzHhZpUoVrVy5UmvXrtX27dv1n//8R8ePH79mbZmziPbs2VPx8fHau3evVqxYoUcffVQOh0MBAQHq16+fnn/+ef3444/atm2b+vbty99k3GQOh0Pbt2/P0SjR2bNna8iQIfrss8/09NNP69KlS1qyZMk1Rw4GBgYqNjZWrVq10tixYzVy5EjVrVtXcXFxroZtpgEDBujDDz/Ub7/9poEDB2rNmjWaMmWKRowY4dF14tZxPZkDcgOZg9nIHKxA7mA2Muc5vunCzfvvv6/69eu7HqzfqlUr1a9fX4sXL3bbbsaMGWrevLnrOVi5pWDBgtqxY4e6deumqlWrqn///ho4cKD+85//SJKio6M1cuRIDRs2TI0bN9bZs2fVu3fvXK3haqKionT27Fm1aNHCbURWZGSkzp49q5YtW+Z4FFxgYKDefPNNNWrUSI0bN9aBAwf03XffyW63y263a+HChUpNTVWTJk302GOP6fXXX3fbv0CBAq7mW506dfTGG29o7Nixbtu8/PLLatCggaKjo9W6dWuFhYWpa9eu16ytZMmSWrNmjRwOh9q3b6/atWtryJAhCg4OdjXIJk6cqDvvvFOdO3dWu3bt1LJlyyzPTkPuMgxDycnJOboF2s/PTxMnTlRiYqLS0tK0YcMGRUdHu20TGxt7xWOVLl1a8+fPV3Jyss6ePatvv/1WlStXvuJ5Hn/8ce3YsUPp6enas2ePhgwZct2z4+LWdT2ZA3IDmYPZyBysQO5gNjLnOZvBpwfkGTabTQsXLsxRAywvSElJUeHChZWcnMzzz3IgIyNDGzduVKNGja54Oy2Q28gczEbmYDYyByuQO5iNzLm7ke+hjDwDAAAAAAAAskHzDKYLCAjI9rV69Wqry8s148aNy/Y67777bqvLQx5kt9tVsWJFni0H05A5mI3MwWxkDlYgdzAbmfMct23CdJmTEVxJqVKl5O/vb2I1N8+pU6eyTLSQyd/fX6VKlTK5olsPt20CAAAAAMx0I99DudkVpsvuweO3m5CQEIWEhFhdBm4jDodD27ZtU61ateTl5WV1OcgHyBzMRuZgNjIHK5A7mI3MeY4xewCQRxiGodTUVGbJgWnIHMxG5mA2MgcrkDuYjcx5juYZAAAAAAAAkA2aZwAAAAAAAEA2aJ4BQB7h5eWl6tWr85wCmIbMwWxkDmYjc7ACuYPZyJznmDAAAPIIm82m4OBgq8tAPkLmYDYyB7OROViB3MFsZM5zjDwDgDwiIyND8fHxysjIsLoU5BNkDmYjczAbmYMVyB3MRuY8R/MMAPIQh8NhdQnIZ8gczEbmYDYyByuQO5iNzHmG5hkAAAAAAACQDZpnAAAAAAAAQDZshmEYVhcBIH9KSUlR4cKFlZycrKCgIKvLueUZhqHU1FT5+/vLZrNZXQ7yATIHs5E5mI3MwQrkDmYjc+5u5HsoI88AIA/x8fGxugTkM2QOZiNzMBuZgxXIHcxG5jxD8wwA8giHw6GNGzfysE+YhszBbGQOZiNzsAK5g9nInOdongEAAAAAAADZoHkGAAAAAAAAZIPmGQAAAAAAAJANZtsEYBlm27w+hmHI4XDIy8uLWXJgCjIHs5E5mI3MwQrkDmYjc+6YbRMAbnMXL160ugTkM2QOZiNzMBuZgxXIHcxG5jxD8wwA8giHw6GtW7cySw5MQ+ZgNjIHs5E5WIHcwWxkznM0zwAAAAAAAIBs0DwDAAAAAAAAskHzDADyEC8vL6tLQD5D5mA2MgezkTlYgdzBbGTOM8y2CcAyzLYJAAAAADATs20CwG3MMAydOXNG/J0HzELmYDYyB7OROViB3MFsZM5zNM8AII9wOBzasWMHs+TANGQOZiNzMBuZgxXIHcxG5jxH8wwAAAAAAADIBs0zAAAAAAAAIBs0zwAgj7DZbPL395fNZrO6FOQTZA5mI3MwG5mDFcgdzEbmPMdsmwAsw2ybAAAAAAAzMdsmANzGnE6nkpKS5HQ6rS4F+QSZg9nIHMxG5mAFcgezkTnP0TwDgDzC6XRq3759/NKDacgczEbmYDYyByuQO5iNzHmO5hkAAAAAAACQDZpnAAAAAAAAQDZongFAHmGz2VS4cGFmyYFpyBzMRuZgNjIHK5A7mI3MeY7ZNgFYhtk2AQAAAABmYrZNALiNOZ1OHTlyhAd9wjRkDmYjczAbmYMVyB3MRuY8R/MMAPIIfunBbGQOZiNzMBuZgxXIHcxG5jxH8wwAAAAAAADIBs0zAAAAAAAAIBs0zwAgj7Db7SpevLjs9mv/6E5PT9fw4cNVsmRJ+fv7KyIiQitXrszRef7880/16NFDwcHBCgoKUpcuXbRv374rbjtjxgzVqFFDfn5+qlKlit55553ruibc2q4nc0BuIHMwG5mDFcgdzEbmPMdsmwAsw2ybN0/Pnj21YMECDRkyRFWqVNGsWbMUHx+vmJgYtWzZMtv9zp07pwYNGig5OVnPPfecChQooClTpsgwDCUkJKho0aKubT/44AM98cQT6tatm6Kjo7V69Wp99tlnmjBhgoYPH27GZQIAAADAdbmR76E0zwBYhubZ9XE6ndq/f78qVKhw1b812rBhgyIiIjRx4kQNHTpUkpSWlqZatWqpRIkSWrt2bbb7vvnmmxo+fLg2bNigxo0bS5J27NihWrVqadiwYRo3bpwkKTU1VWXKlFHTpk21ZMkS1/4PP/ywFi1apMOHD6tIkSK5cdmwUE4zB+QWMgezkTlYgdzBbGTO3Y18D+VTA5Br+vbtq65du1pdxm3L6XTqxIkT15wlZ8GCBfLy8lL//v1dy/z8/NSvXz+tW7dOhw8fvuq+jRs3djXOJKl69epq27atvvrqK9eymJgYnTx5UgMGDHDbf+DAgTp//ryWLl16vZeHW1BOMwfkFjIHs5E5WIHcwWxkznM0z6AJEybIZrNpyJAhrmVpaWkaOHCgihYtqoCAAHXr1k3Hjx932y8+Pl5t27ZVcHCwihQpoujoaG3ZssXk6nPXgQMHZLPZlJCQ4Lb8/fffV2BgoDIyMlzLzp07pwIFCqh169Zu28bGxspms2nv3r0mVAxktXnzZlWtWjXL36I0adJEkrLkO5PT6dTWrVvVqFGjLOuaNGmivXv36uzZs65zSMqybcOGDWW3213rAQAAACCvo3mWz8XHx+uDDz5QnTp13JY/88wz+vbbbzV//nzFxcXp6NGjuu+++1zrz507pw4dOqhs2bJav369fv75ZwUGBio6OlqXLl0y+zJuuqioKJ07d04bN250LVu9erXCwsK0fv16paWluZbHxMSobNmyqlSpkhWleuzixYtWlwAPJSYmKjw8PMvyzGVHjx694n6nTp1Senp6jvZNTEyUl5eXSpQo4badj4+PihYtmu05AAAAACCvoXmWj507d069evXSRx995PZsouTkZM2YMUOTJ09WmzZt1LBhQ82cOVNr167VL7/8IunyM5BOnTql1157TdWqVdMdd9yhUaNG6fjx4zp48KAkadasWQoODtaKFStUo0YNBQQEqEOHDkpMTMxRfbGxsWrSpIkKFSqk4OBgtWjRwnXsK90eOGTIELdRYK1bt9ZTTz2lIUOGqEiRIgoNDdVHH32k8+fP69FHH1VgYKAqV66sZcuWXbOWatWqKTw8XLGxsW71denSRRUqVHB9LpnLo6KirnlMwzA0evRolS1bVr6+vipZsqSefvpp1/qkpCR17txZ/v7+qlChgubMmaPy5ctr6tSpkq48Su7MmTOy2WyuOh0Oh/r166cKFSrI399f1apV09tvv+1WR+Zn+frrr6tkyZKqVq2aJOnw4cOuGRdDQkLUpUsXHThwwLWfw+HQs88+q+DgYBUtWlTDhg0Tj1C8uex2u0qXLn3N5xSkpqbK19c3y3I/Pz/X+uz2k5SjfVNTU+Xj43PF4/j5+WV7DuQtOc0ckFvIHMxG5mAFcgezkTnP8cnlYwMHDlSnTp3Url07t+WbNm3SpUuX3JZXr15dZcuW1bp16yRdbiYVLVpUM2bM0MWLF5WamqoZM2aoRo0aKl++vGu/Cxcu6K233tJnn32mn376SYcOHXI9wPxqMjIy1LVrV0VGRmrr1q1at26d+vfvL5vNdl3X+Omnn6pYsWLasGGDnnrqKT355JPq3r27mjdvrl9//VXt27fXI488ogsXLlzzWFFRUYqJiXG9j4mJUevWrRUZGelanpqaqvXr1+eoefb1119rypQp+uCDD7R7924tWrRItWvXdq3v27evDh8+rJiYGC1YsEDvvfeekpKSruv6nU6nSpcurfnz5+uPP/7QK6+8ohdffNHt2VWStGrVKu3cuVMrV67UkiVLdOnSJUVHRyswMFCrV6/WmjVrXM3PzJFpkyZN0qxZs/TJJ5/o559/1qlTp7Rw4cKr1pOenq6UlBS3F3Iup7/0/P39lZ6enmV55ghJf3//bPeTlKN9/f39sx2lmJaWlu05kLfwP1owG5mD2cgcrEDuYDYy5zk+uXxq7ty5+vXXXzV+/Pgs644dOyYfHx8FBwe7LQ8NDdWxY8ckSYGBgYqNjdXnn38uf39/BQQEaPny5Vq2bJm8vb1d+1y6dEnvv/++GjVqpAYNGmjQoEFatWrVNetLSUlRcnKy/vWvf6lSpUqqUaOG+vTpo7Jly17XddatW1cvv/yyqlSpohEjRsjPz0/FihXT448/ripVquiVV17RyZMntXXr1mseKyoqSmvWrFFGRobOnj2rzZs3KzIyUq1atXKN9Fq3bp3S09Nz1Dw7dOiQwsLC1K5dO5UtW1ZNmjTR448/LknatWuXli1bpo8++khNmzZVw4YNNWPGjOsezVOgQAG9+uqratSokSpUqKBevXrp0UcfzdI8K1SokD7++GPdcccduuOOOzRv3jw5nU59/PHHql27tmrUqKGZM2fq0KFDrmudOnWqRowYofvuu081atTQ+++/r8KFC1+1nvHjx6tw4cKuV5kyZa7revI7h8Oh7du3y+FwXHW78PDwK47wzFxWsmTJK+4XEhIiX1/fHO0bHh4uh8ORpaF78eJFnTx5MttzIG/JaeaA3ELmYDYyByuQO5iNzHmO5lk+dPjwYQ0ePFhz5sxx3Yp1vVJTU9WvXz+1aNFCv/zyi9asWaNatWqpU6dObg2eggULuj37Kzw8PEejp0JCQtS3b19FR0erc+fOevvtt3N8u+ff/f1Zbl5eXipatKjb6K7Q0FBJylFNrVu31vnz5xUfH6/Vq1eratWqKl68uCIjI13PPYuNjVXFihVz1OTr3r27UlNTVbFiRT3++ONauHCha0KC7du3y9vbWw0bNnRtX7169SwNzZz473//q4YNG6p48eIKCAjQhx9+qEOHDrltU7t2bbdb8LZs2aI9e/YoMDBQAQEBCggIUEhIiNLS0rR3714lJycrMTFRERERrn28vb2v+KD5vxsxYoSSk5Ndr6vN+oisDMNQcnLyNW+PrVevnnbt2pVlZN/69etd66/Ebrerdu3abs/2+/u+FStWVGBgoNsx/rntxo0b5XQ6sz0H8pacZg7ILWQOZiNzsAK5g9nInOdonuVDmzZtUlJSkho0aCBvb295e3srLi5O06ZNk7e3t0JDQ3Xx4kWdOXPGbb/jx48rLCxMkvTFF1/owIEDmjlzpho3bqymTZvqiy++0P79+/XNN9+49ilQoIDbMWw2W47/g505c6bWrVun5s2ba968eapatarr2WJ2uz3Lca40UcGVzv/3ZZm3geZkyt7KlSurdOnSiomJUUxMjCIjIyVdHolTpkwZrV27VjExMWrTpk2Orq9MmTLauXOn3nvvPfn7+2vAgAFq1apVjidcyBxy+/fP4Z/7zp07V0OHDlW/fv30/fffKyEhQY8++miW2+0KFSrk9v7cuXNq2LChEhIS3F67du3SQw89lKP6rsTX11dBQUFuL+S++++/Xw6HQx9++KFrWXp6umbOnKmIiAjXiL9Dhw5px44dWfaNj493a4rt3LlTP/74o7p37+5a1qZNG4WEhGj69Olu+0+fPl0FCxZUp06dbsalAQAAAIDpvK+9CW43bdu21W+//ea27NFHH1X16tU1fPhwlSlTRgUKFNCqVavUrVs3SZe/PB86dEjNmjWTdPlZZna73e0ZZJnvc9KIyqn69eurfv36GjFihJo1a6YvvvhCTZs2VfHixbVt2za3bRMSErI0y3JbVFSUYmNjdfr0aT3//POu5a1atdKyZcu0YcMGPfnkkzk+nr+/vzp37qzOnTtr4MCBql69un777TdVr15dGRkZ2rRpkxo3bizp8p/B3xuaxYsXl3T5drr69etLktvkAZK0Zs0aNW/eXAMGDHAt27t37zXratCggebNm6cSJUpk2+AKDw/X+vXr1apVK0ly1dugQYMcXz9ujoiICHXv3l0jRoxQUlKSKleurE8//VQHDhzQjBkzXNv17t1bcXFxbg3YAQMG6KOPPlKnTp00dOhQFShQQJMnT1ZoaKiee+4513b+/v4aM2aMBg4cqO7duys6OlqrV6/W559/rtdff10hISGmXjMAAAAA3Cw0z/KhwMBA1apVy21ZoUKFVLRoUdfyfv366dlnn1VISIiCgoL01FNPqVmzZmratKkk6a677tLzzz+vgQMH6qmnnpLT6dSECRPk7e2do+d9Xcv+/fv14Ycf6p577lHJkiW1c+dO7d69W71795Z0edTLxIkTNXv2bDVr1kyff/65tm3b5moieWrnzp1Zlt1xxx2KiorSwIEDdenSJdfIM0mKjIzUoEGDdPHixRxf/6xZs+RwOBQREaGCBQu6nh9Xrlw5FS1aVB06dNB//vMfTZ8+Xd7e3hoyZIjbQ9j9/f3VtGlTTZgwQRUqVFBSUpJefvllt3NUqVJFs2fP1ooVK1ShQgV99tlnio+PV4UKFa5aW69evTRx4kR16dJFr732mkqXLq2DBw/qf//7n4YNG6bSpUtr8ODBmjBhgqpUqaLq1atr8uTJWUYrInfZ7XZVrFgxRw/6nD17tkaOHKnPPvtMp0+fVp06dbRkyRJXszM7mc8zfOaZZzR27Fg5nU61bt1aU6ZMcTVsMw0YMEAFChTQpEmTtHjxYpUpU0ZTpkzR4MGDPbpO3DquJ3NAbiBzMBuZgxXIHcxG5jxH8wxXNGXKFNntdnXr1k3p6emKjo7We++951pfvXp1ffvtt3r11VfVrFkz2e121a9fX8uXL1d4eLjH5y9YsKB27NihTz/9VCdPnlR4eLgGDhyo//znP5Kk6OhojRw5UsOGDVNaWpr+/e9/q3fv3llG1N2oBx98MMuyw4cPKyoqSqmpqapevbrreWnS5ebZ2bNnVa1atRxff3BwsCZMmKBnn31WDodDtWvX1rfffquiRYtKunzb6mOPPabIyEiFhoZq7NixGjlypNsxPvnkE/Xr108NGzZUtWrV9Oabb6p9+/au9f/5z3+0efNmPfDAA7LZbOrZs6cGDBigZcuWXbW2ggUL6qefftLw4cN133336ezZsypVqpTatm3rGon23HPPKTExUX369JHdbte///1v3XvvvUpOTs7R9eP62e12lShRIkfb+vn5aeLEiZo4cWK222RO/vBPmTO05sTjjz/umugCt5/ryRyQG8gczEbmYAVyB7OROc/ZDJ4YB+QZ5cuX15AhQzRkyBCrS8kVKSkpKly4sJKTk3n+WQ44HA5t27ZNtWrVkpeXl9XlIB8gczAbmYPZyBysQO5gNjLn7ka+hzJmDwDyCMMwlJqayiw5MA2Zg9nIHMxG5mAFcgezkTnP0TyDZQICArJ9rV692uryPDZnzpxsr++OO+6wujwAAAAAAJADPPMMlvnnzJB/V6pUKfMKuUnuueceRUREXHHdjc4KeuDAAQ8qAgAAAAAA14tnngGwDM88uz6GYSg5OVmFCxeWzWazuhzkA2QOZiNzMBuZgxXIHcxG5tzdyPdQRp4BQB5hs9kUHBxsdRnIR8gczEbmYDYyByuQO5iNzHmOZ54BQB6RkZGh+Ph4ZWRkWF0K8gkyB7OROZiNzMEK5A5mI3Oeo3kGAHmIw+GwugTkM2QOZiNzMBuZgxXIHcxG5jxD8wwAAAAAAADIBs0zAAAAAAAAIBvMtgnAMsy2eX0Mw1Bqaqr8/f2ZJQemIHMwG5mD2cgcrEDuYDYy5+5Gvocy8gwA8hAfHx+rS0A+Q+ZgNjIHs5E5WIHcwWxkzjM0zwAgj3A4HNq4cSMP+4RpyBzMRuZgNjIHK5A7mI3MeY7mGQAAAAAAAJANmmcAAAAAAABANmieAQAAAAAAANlgtk0AlmG2zetjGIYcDoe8vLyYJQemIHMwG5mD2cgcrEDuYDYy547ZNgHgNnfx4kWrS0A+Q+ZgNjIHs5E5WIHcwWxkzjM0zwAgj3A4HNq6dSuz5MA0ZA5mI3MwG5mDFcgdzEbmPEfzDAAAAAAAAMgGzTMAAAAAAAAgGzTPACAP8fLysroE5DNkDmYjczAbmYMVyB3MRuY8w2ybACzDbJsAAAAAADMx2yYA3MYMw9CZM2fE33nALGQOZiNzMBuZgxXIHcxG5jxH8wwA8giHw6EdO3YwSw5MQ+ZgNjIHs5E5WIHcwWxkznM0zwAAAAAAAIBs0DwDAAAAAAAAskHzDADyCJvNJn9/f9lsNqtLQT5B5mA2MgezkTlYgdzBbGTOc8y2CcAyzLYJAAAAADATs20CwG3M6XQqKSlJTqfT6lKQT5A5mI3MwWxkDlYgdzAbmfMczTMAyCOcTqf27dvHLz2YhszBbGQOZiNzsAK5g9nInOdongEAAAAAAADZoHkGAAAAAAAAZIPmGQDkETabTYULF2aWHJiGzMFsZA5mI3OwArmD2cic55htE4BlmG0TAAAAAGAmZtsEgNuY0+nUkSNHeNAnTEPmYDYyB7OROViB3MFsZM5zNM8AII/glx7MRuZgNjIHs5E5WIHcwWxkznM0zwAAAAAAAIBs0DwDAAAAAAAAskHzDADyCLvdruLFi8tu50c3zEHmYDYyB7OROViB3MFsZM5zzLYJwDLMtgkAAAAAMBOzbQLAbczpdGrv3r086BOmIXMwG5mD2cgcrEDuYDYy5zmaZwCQRzidTp04cYJfejANmYPZyBzMRuZgBXIHs5E5z9E8AwAAAAAAALLhbXUBAPKvzEcupqSkWFxJ3pCRkaHz588rJSVF3t78+MbNR+ZgNjIHs5E5WIHcwWxkzl3m98/rmQKATw2AZc6ePStJKlOmjMWVAAAAAADyk7Nnz6pw4cI52pbZNgFYxul06ujRowoMDJTNZrO6nFteSkqKypQpo8OHDzM7KUxB5mA2MgezkTlYgdzBbGTOnWEYOnv2rEqWLCm7PWdPM2PkGQDL2O12lS5d2uoy8pygoCB+6cFUZA5mI3MwG5mDFcgdzEbm/k9OR5xlYsIAAAAAAAAAIBs0zwAAAAAAAIBs0DwDgDzC19dXo0aNkq+vr9WlIJ8gczAbmYPZyBysQO5gNjLnOSYMAAAAAAAAALLByDMAAAAAAAAgGzTPAAAAAAAA6ZL8ZgAADrNJREFUgGzQPAMAAAAAAACyQfMMAAAAAAAAyAbNMwDIA/773/+qfPny8vPzU0REhDZs2GB1SbiN/fTTT+rcubNKliwpm82mRYsWWV0SbnPjx49X48aNFRgYqBIlSqhr167auXOn1WXhNjZ9+nTVqVNHQUFBCgoKUrNmzbRs2TKry0I+MmHCBNlsNg0ZMsTqUnCbGj16tGw2m9urevXqVpeVZ9E8A4Bb3Lx58/Tss89q1KhR+vXXX1W3bl1FR0crKSnJ6tJwmzp//rzq1q2r//73v1aXgnwiLi5OAwcO1C+//KKVK1fq0qVLat++vc6fP291abhNlS5dWhMmTNCmTZu0ceNGtWnTRl26dNHvv/9udWnIB+Lj4/XBBx+oTp06VpeC29wdd9yhxMRE1+vnn3+2uqQ8y2YYhmF1EQCA7EVERKhx48Z69913JUlOp1NlypTRU089pRdeeMHi6nC7s9lsWrhwobp27Wp1KchHTpw4oRIlSiguLk6tWrWyuhzkEyEhIZo4caL69etndSm4jZ07d04NGjTQe++9p7Fjx6pevXqaOnWq1WXhNjR69GgtWrRICQkJVpdyW2DkGQDcwi5evKhNmzapXbt2rmV2u13t2rXTunXrLKwMAG6e5ORkSZebGcDN5nA4NHfuXJ0/f17NmjWzuhzc5gYOHKhOnTq5/b8dcLPs3r1bJUuWVMWKFdWrVy8dOnTI6pLyLG+rCwAAZO+vv/6Sw+FQaGio2/LQ0FDt2LHDoqoA4OZxOp0aMmSIWrRooVq1alldDm5jv/32m5o1a6a0tDQFBARo4cKFqlmzptVl4TY2d+5c/frrr4qPj7e6FOQDERERmjVrlqpVq6bExES9+uqruvPOO7Vt2zYFBgZaXV6eQ/MMAAAAt4yBAwdq27ZtPJcFN121atWUkJCg5ORkLViwQH369FFcXBwNNNwUhw8f1uDBg7Vy5Ur5+flZXQ7ygbvvvtv173Xq1FFERITKlSunr776itvTbwDNMwC4hRUrVkxeXl46fvy42/Ljx48rLCzMoqoA4OYYNGiQlixZop9++kmlS5e2uhzc5nx8fFS5cmVJUsOGDRUfH6+3335bH3zwgcWV4Xa0adMmJSUlqUGDBq5lDodDP/30k959912lp6fLy8vLwgpxuwsODlbVqlW1Z88eq0vJk3jmGQDcwnx8fNSwYUOtWrXKtczpdGrVqlU8lwXAbcMwDA0aNEgLFy7Ujz/+qAoVKlhdEvIhp9Op9PR0q8vAbapt27b67bfflJCQ4Ho1atRIvXr1UkJCAo0z3HTnzp3T3r17FR4ebnUpeRIjzwDgFvfss8+qT58+atSokZo0aaKpU6fq/PnzevTRR60uDbepc+fOuf2t5P79+5WQkKCQkBCVLVvWwspwuxo4cKC++OILffPNNwoMDNSxY8ckSYULF5a/v7/F1eF2NGLECN19990qW7aszp49qy+++EKxsbFasWKF1aXhNhUYGJjlOY6FChVS0aJFeb4jboqhQ4eqc+fOKleunI4ePapRo0bJy8tLPXv2tLq0PInmGQDc4h544AGdOHFCr7zyio4dO6Z69epp+fLlWSYRAHLLxo0bFRUV5Xr/7LPPSpL69OmjWbNmWVQVbmfTp0+XJLVu3dpt+cyZM9W3b1/zC8JtLykpSb1791ZiYqIKFy6sOnXqaMWKFbrrrrusLg0AcsWRI0fUs2dPnTx5UsWLF1fLli31yy+/qHjx4laXlifZDMMwrC4CAAAAAAAAuBXxzDMAAAAAAAAgGzTPAAAAAAAAgGzQPAMAAAAAAACyQfMMAAAAAAAAyAbNMwAAAAAAACAbNM8AAAAAAACAbNA8AwAAAAAAALJB8wwAAAAAAADIBs0zAAAAwEK7d+/Wvffeq/DwcNntdgUHB19z3ejRo2Wz2RQbG3tD54yNjZXNZtPo0aM9rh8AgNudt9UFAAAAAGbbtGmT3nvvPf300086evSonE6nSpYsqebNm6t379666667TKnD4XCoa9eu2rNnjx555BGVLl1afn5+11yXV/Xt21effvqp9u/fr/Lly1tdDgAAOWIzDMOwuggAAADADE6nU0OHDtWUKVPk7e2tNm3aqFatWipQoID27dunH374QadPn9Zrr72mkSNH3vR69uzZoypVqujxxx/Xhx9+mON1f/31l/766y+VLVtWBQsWvO7zXrhwQYcOHVKxYsVUrFgxj67hetA8AwDkRYw8AwAAQL7x8ssva8qUKapXr54WLFigSpUqua1PTU3Vu+++q5MnT5pSz9GjRyVJJUuWvK51nja9ChYsqOrVq9/w/gAA5Cc88wwAAAD5wp49e/Tmm2+qaNGiWr58eZbGmST5+/vr+eef16uvvupa9tdff2nIkCGqUKGCfH19VaJECfXo0UPbtm274nkuXryoyZMnq0GDBipUqJACAwN15513avHixW7blS9fXpGRkZKkV199VTabzfUcsqutk67+zLMtW7aoV69eKl26tHx9fRUeHq4OHTro22+/dW1ztWeeJSUl6ZlnnlHlypXl6+urYsWKqVu3ble83vLly6t8+fI6d+6cBg8erJIlS8rX11d16tTRggULsmz76aefSpIqVKjguqbWrVtf8XMEAOBWwcgzAAAA5AuzZs2Sw+HQf/7zH4WGhl51W19fX0nSiRMn1KxZM+3du1etW7fWgw8+qP3792vBggVaunSpVqxYoZYtW7r2S09PV4cOHRQbG6t69eqpX79+unTpkpYuXaouXbronXfe0aBBgyRJQ4YMUUJCgj799FNFRka6mkitW7dWcHBwtuuu5uuvv9ZDDz0kwzDUuXNnVatWTUlJSVq/fr1mzJihzp07X3X/zOs8cuSI2rdvr65duyopKUlff/21VqxYoVWrVikiIsJtn0uXLql9+/Y6ffq0unXrpgsXLmju3Lnq0aOHli9frvbt27uud9asWdqyZYsGDx7smvyA2zcBALc8AwAAAMgHWrdubUgyfvjhhxzv8+ijjxqSjBEjRrgtX7p0qSHJqFy5suFwOFzLX3zxRUOSMXLkSMPpdLqWp6SkGI0aNTJ8fHyMP//807U8JibGkGSMGjUqy7mvtm7UqFGGJCMmJsa17NixY0ahQoWMQoUKGb/++muWfQ4fPnzNYzdv3tzw8vIyli9f7rZ8586dRmBgoFG7dm235eXKlTMkGV26dDHS09Ndy3/44QdDkhEdHe22fZ8+fQxJxv79+7PUBwDArYrbNgEAAJAvHDt2TJJUunTpHG1/8eJFffnllypatKhefvllt3UdO3bUXXfdpT179mjNmjWSLk9GMH36dFWqVMl1q2WmwMBAvfLKK7p48aL+97//5dIVufv00091/vx5Pffcc6pfv36W9de67s2bN2vt2rXq06ePoqOj3dZVrVpVjz/+uH777bcr3r45ZcoU+fj4uN63bdtW5cqVU3x8/A1eDQAAtw5u2wQAAACuYMeOHUpLS1NUVNQVZ7SMiorSypUrlZCQoDvvvFM7d+7U6dOnVbJkSbdnpmU6ceKE67g3w4YNGyTJdZvk9frll18kScePH7/is9Ay696xY4dq1arlWh4cHKwKFSpk2b506dJat27dDdUCAMCthOYZAAAA8oWwsDDt2LFDf/75p6pVq3bN7VNSUiQp2+ejhYeHu2136tQpSdLvv/+u33//Pdvjnj9//rrqzqnk5GRJUqlSpW5o/8z6ly5dqqVLl2a73T/rL1y48BW38/b2ltPpvKFaAAC4lXDbJgAAAPKFFi1aSJJWrVqVo+2DgoIkXR6JdSWZt4Fmbpf5z27duskwjGxfM2fO9Og6spP5AP4///zzhvbPrP+dd965av19+vTJrZIBAMgTaJ4BAAAgX+jbt6+8vLz04Ycfum6hzE56erqqV68uPz8/xcfH68KFC1m2iY2NlSTVq1dPklSjRg0FBQVp48aNunTpUm6Xf01NmjSRJH3//fc3tH/mLJo381ZLLy8vSZLD4bhp5wAAILfRPAMAAEC+ULlyZQ0bNkx//fWX7r77bu3fvz/LNmlpaZo8ebJGjx4tHx8f9ezZU3/99ZfGjx/vtt3y5cu1YsUKVa5c2TWizdvbW08++aQOHjyooUOHXrGBtm3bNiUlJd2U6+vTp48CAgI0adIkJSQkZFl/rRFpTZo0UUREhL788kvNmzcvy3qn06m4uDiPagwJCZEkHT582KPjAABgJp55BgAAgHxj7NixSktL05QpU1StWjW1adNGtWrVUoECBbR//3798MMPOnnypMaOHStJeuONNxQXF6exY8dq7dq1ioiI0IEDBzR//nwVLFhQM2fOlN3+f38f/eqrr+rXX3/VtGnTtHTpUrVq1UolSpTQn3/+qd9++01btmzRunXrVKJEiVy/thIlSmj27Nl68MEH1aRJE91zzz2qVq2a/vrrL61fv17ly5fXokWLrnqML7/8UlFRUXrwwQc1depUNWjQQP7+/jp06JDWrVunEydOKC0t7YZrbNOmjd566y31799f3bp1U6FChVSuXDk98sgjN3xMAABuNppnAAAAyDfs/6+9O2RNNQzjOPzfDMaVDXnjqiDsE8wiiE1haW1h4LcwLAwGe6tBZGAX1GL2NZn2eUTQk094OeFwOAvX1R+4n/oL9319nbIs8/z8nOl0mqqqUlVVzudziqJIv9/Py8tLer1ekuTu7i6HwyFvb29Zr9fZ7/e5ubnJcDjMZDL57epkkjSbzWy328zn8ywWiyyXyxyPx7RarbTb7YzH43Q6nX/2v9FolMPhkPf39+x2u2w2m9ze3ubh4SGvr69/fH9/f5/v7++UZZnVapWvr680Go0URZHHx8c8PT391XyDwSAfHx+ZzWb5/PzM6XRKt9sVzwD40a4ul8vlfw8BAAAAAD+RnWcAAAAAUEM8AwAAAIAa4hkAAAAA1BDPAAAAAKCGeAYAAAAANcQzAAAAAKghngEAAABADfEMAAAAAGqIZwAAAABQQzwDAAAAgBriGQAAAADUEM8AAAAAoMYvLybC1FINuHAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the best estimator\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = features.columns\n",
    "\n",
    "# Get the coefficients (betas)\n",
    "coefficients = best_estimator.coef_\n",
    "\n",
    "# Create a DataFrame to hold feature names and their corresponding coefficients\n",
    "coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Sort the DataFrame by the absolute value of the coefficients\n",
    "coef_df['AbsCoefficient'] = coef_df['Coefficient'].abs()\n",
    "coef_df = coef_df.sort_values(by='AbsCoefficient', ascending=False)\n",
    "\n",
    "# Plot the top 10 most important features\n",
    "top_features = coef_df.head(10)\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(top_features['Feature'], top_features['Coefficient'], color='#033D5D')\n",
    "plt.xlabel('Coefficient', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.title('Top 10 Most Important Features in best-performing SGD: Squared', fontsize=16)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add data labels with some space\n",
    "for bar in bars:\n",
    "    plt.text(bar.get_width() + 0.05, bar.get_y() + bar.get_height()/2, f'{bar.get_width():.2f}', \n",
    "             va='center', ha='left', fontsize=12, color='black')\n",
    "\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(test_target, predictions, alpha=0.5, color='#033D5D')\n",
    "plt.plot([test_target.min(), test_target.max()], [test_target.min(), test_target.max()], 'r--', lw=2, color='black')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('SGD Regressor with squared features: Predicted vs Actual Test Values')\n",
    "plt.legend(['Predicted vs Actual', 'Perfect Fit'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
