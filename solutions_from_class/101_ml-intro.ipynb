{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "#### üéØ Learning Goals\n",
    "\n",
    "1. Understand the behavior of different **Loss Functions**.\n",
    "2. Understand the concept of **Empirical Risk Minimization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use a nicer style for plots\n",
    "plt.style.use(\"seaborn-v0_8-muted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## üìù Notation and Terminology\n",
    "Let us briefly refresh notation before we dive into the concepts of interest.\n",
    "\n",
    "+ We have a dataset $\\mathcal{D} = \\{(\\mathbf{x}^{(i)}, y^{(i)})\\}_{i=1}^n$ consisting of $n$ observations.\n",
    "+ Each observation $(\\mathbf{x}^{(i)}, y^{(i)})$ consists of \n",
    "    1. A vector $\\mathbf{x}^{(i)} \\in \\mathcal{X}$, also called *feature*, *covariate*, or *independent variable*, \n",
    "    2. A scalar $y^{(i)} \\in \\mathcal{Y}$, also called *label*, *target*, *response*, *outcomes*, or *dependent variable*. (Note that we will later also encounter vector-valued labels, i.e., $\\mathbf{y}^{(i)}$ but for now we focus to the simpler scalar case.)\n",
    "+ We assume that there exists some relationship between $y^{(i)}$ and $\\mathbf{x}^{(i)}$, which can be written in the general form $$y^{(i)} = f^*(\\mathbf{x}^{(i)}) + \\epsilon^{(i)},$$ where $f^*:\\mathcal{X}\\to\\mathcal{Y}$ is the *true* function and $\\epsilon^{(i)}$ is the *error* term or *noise* term.\n",
    "+ We want to find $\\hat{f}: \\mathcal{X} \\to \\mathcal{Y}$, an *estimate* of the true function $f^*:\\mathcal{X}\\to\\mathcal{Y}$. We also call this $\\hat{f}$ the *predictor*.\n",
    "\n",
    "<!-- + We assume that the data is generated by an unknown distribution $p_\\text{data}$, i.e., $(\\mathbf{x}^{(i)}, y^{(i)}) \\sim p_\\text{data}$. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Loss Functions\n",
    "\n",
    "Loss functions, also known as *cost* functions or *objective* functions, play a fundamental role in machine learning algorithms. They allow us to measure the quality of our predictions by penalizing incorrect predictions. In this section, we will look at some common loss functions and discuss their properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared Loss\n",
    "The squared loss is defined as the squared difference between the true label $y^{(i)}$ and the predicted label $\\hat{y}^{(i)}$:\n",
    "\n",
    "$$\n",
    "\\ell_\\text{sq}(y^{(i)}, \\hat{y}^{(i)}) = (y^{(i)} - \\hat{y}^{(i)})^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y, y_hat):\n",
    "    return (y - y_hat) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-1 Loss\n",
    "The 0-1 loss penalizes any incorrect prediction with a fixed penalty of 1, independent of the magnitude of the error:\n",
    "\n",
    "$$\n",
    "\\ell_\\text{0-1}(y^{(i)}, \\hat{y}^{(i)}) = \\begin{cases}\n",
    "0 & \\text{if } y^{(i)} = \\hat{y}^{(i)} \\\\\n",
    "1 & \\text{otherwise}\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_one_loss(y, y_hat):\n",
    "    return 1 * (y != y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è ‚úèÔ∏è Task 1\n",
    "\n",
    "Following the examples above, implement the following loss functions below:\n",
    "1. Absolute loss\n",
    "2. Huber loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute Loss\n",
    "The absolute loss is defined as the absolute difference between the true label $y^{(i)}$ and the predicted label $\\hat{y}^{(i)}$:\n",
    "\n",
    "$$\n",
    "\\ell_\\text{abs}(y^{(i)}, \\hat{y}^{(i)}) = |y^{(i)} - \\hat{y}^{(i)}|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_loss(y, y_hat):\n",
    "    # ‚û°Ô∏è ‚úèÔ∏è your code here\n",
    "    return y * np.nan # remove this line when you're done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss\n",
    "The Huber loss is a combination of the squared loss and the absolute loss. It is quadratic for small errors and linear for large errors. This makes it less sensitive to outliers than the squared loss and less sensitive to small errors than the absolute loss.\n",
    "\n",
    "$$\n",
    "\\ell_\\text{huber}(y, \\hat{y}) = \\begin{cases}\n",
    "\\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "\\delta(|y - \\hat{y}| - \\frac{1}{2}\\delta) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "*Hint:* Huber loss is difficult. In particular, we would like to code it in a way that `y_hat` and `y` can be vectors. Here is a bit of help to get you started:\n",
    "\n",
    "```python\n",
    "def huber_loss(y, y_hat, delta=1):\n",
    "    # 1.) Start by computing the absolute loss\n",
    "    abs_loss = ...\n",
    "    # 2.) Then make a mask for the indices where the absolute loss is smaller than delta\n",
    "    mask = abs_loss <= delta\n",
    "    # 3.) Then compute the huber loss\n",
    "    loss = np.zeros_like(abs_loss) # Initialize the loss vector with zeros\n",
    "    loss[mask] = ... # Fill in the huber loss where the mask is True\n",
    "    loss[~mask] = ... # Fill in the huber loss where the mask is False\n",
    "    return loss\n",
    "```\n",
    "\n",
    "alternatively, you can also use the [`np.where()`](https://numpy.org/doc/stable/reference/generated/numpy.where.html) command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y, y_hat, delta=1):\n",
    "    # ‚û°Ô∏è ‚úèÔ∏è your code here\n",
    "    return y * np.nan # remove this line when you're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REMOVE SOLUTION\n",
    "def absolute_loss(y, y_hat):\n",
    "    return np.abs(y_hat - y)\n",
    "\n",
    "def huber_loss(y, y_hat, delta=1):\n",
    "    abs_loss = absolute_loss(y_hat, y)\n",
    "    return np.where(\n",
    "        abs_loss <= delta, \n",
    "        0.5 * abs_loss ** 2, \n",
    "        delta * (abs_loss - 0.5 * delta)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Define our x-axis (linearly spaced values between -2 and 2)\n",
    "xs = np.linspace(-2, 2, 101)\n",
    "ys = np.zeros_like(xs)\n",
    "\n",
    "\n",
    "# Plot our loss functions\n",
    "ax.plot(xs, squared_loss(ys, xs), label='Squared loss')\n",
    "ax.plot(xs, absolute_loss(ys, xs), label='Absolute loss')\n",
    "ax.plot(xs, huber_loss(ys, xs), label='Huber loss')\n",
    "ax.plot(xs, zero_one_loss(ys, xs), label='0-1 loss')\n",
    "\n",
    "# Add some plot aesthetics\n",
    "ax.set_xlabel(\"Prediction error: $y - \\hat{y}$\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Loss functions\")\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è ‚úèÔ∏è Task 2\n",
    "Looking at the above plot, discuss the following questions with your classmates:\n",
    "1. If your predictor predicts perfectly, i.e., $\\hat{y} = y$, does the loss function matter?\n",
    "2. If your prediction error $y - \\hat{y} = 0.5$, which loss is the most forgiving: the absolute loss or the squared loss?\n",
    "3. What if instead the prediction error is $y - \\hat{y} = 1.5$?\n",
    "4. Why would one prefer the squared loss over the absolute loss or vice versa? (*Hint*: Read the description of the Huber loss again.)\n",
    "5. Have you noticed that all of the above loss functions are symmetric around $y - \\hat{y} = 0$? Why is that? What would happen if they were not symmetric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è ‚úèÔ∏è Task 3\n",
    "Repeat the above plot, but do the following:\n",
    "1. Change the values for `xs` to be linearly spaced between `-5` and `5`.\n",
    "2. Remove the 0-1 loss.\n",
    "3. Add Huber loss with $\\delta=\\frac{1}{2}$, $\\delta=2$, and $\\delta=5$.\n",
    "\n",
    "What do you notice? Discuss with your classmates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏è ‚úèÔ∏è your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REMOVE SOLUTION\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Define our x-axis (linearly spaced values between -2 and 2)\n",
    "xs = np.linspace(-5, 5, 101)\n",
    "ys = np.zeros_like(xs)\n",
    "\n",
    "\n",
    "# Plot our loss functions\n",
    "ax.plot(xs, squared_loss(ys, xs), label='Squared loss')\n",
    "ax.plot(xs, absolute_loss(ys, xs), label='Absolute loss')\n",
    "ax.plot(xs, huber_loss(ys, xs), label='Huber loss $(\\delta = 1)$')\n",
    "ax.plot(xs, huber_loss(ys, xs, .5), label='Huber loss $(\\delta = 0.5)$')\n",
    "ax.plot(xs, huber_loss(ys, xs, 2), label='Huber loss $(\\delta = 2)$')\n",
    "ax.plot(xs, huber_loss(ys, xs, 5), label='Huber loss $(\\delta = 5)$')\n",
    "\n",
    "# Add some plot aesthetics\n",
    "ax.set_xlabel(\"Prediction error: $y - \\hat{y}$\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Loss functions\")\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Empirical Risk Minimization\n",
    "\n",
    "Now that we understand how different loss functions act as a measure of the quality of our predictions, we can focus on using them to find a good predictor $\\hat{f}$.\n",
    "\n",
    "Recall that we would like to pick a predictor $\\hat{f}$ out of a larger set of possible predictors: our *hypothesis class* $\\mathcal{H}$. We will discuss the concept of hypothesis classes in more detail later on. For now, let us assume that we receive a finite set of possible predictors $\\mathcal{H} = \\{f_1, f_2, \\dots f_n\\}$ and we would like to pick the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Crop Yields\n",
    "We begin by considering a scenario where we are interested in predicting the crop yield of a field. We have a dataset $\\mathcal{D} = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^n$ consisting of $n=100$ observations of crops in the US, with average temperature $x^{(i)}$ and crop yield $y^{(i)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data about US crop yields based on temperature\n",
    "crops = pd.read_csv(\"data/us_crops.csv\")\n",
    "crops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we given the following hypothesis class $\\mathcal{H}$ of possible predictors:\n",
    "$$\\begin{align*}\n",
    "\\mathcal{H} &= \\{f_1, f_2, \\dots f_6\\}, \\text{ where} \\\\\n",
    "f_1(x) &= 639 &&\\text{(constant predictor)} \\\\\n",
    "f_2(x) &= 1000 - 25x\\\\\n",
    "f_3(x) &= 1010 - 30x\\\\\n",
    "f_4(x) &= 1020 - 9.5x - 1.5x^2\\\\\n",
    "f_5(x) &= 800 + 40x - 3.5x^2\\\\\n",
    "f_6(x) &= 400 + 30x + 2.9x^2 - 0.25x^3,\n",
    "\\end{align*}$$\n",
    "\n",
    "and we would like to pick the best predictor out of this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypotheses are functions that take our input data and return a prediction\n",
    "hypotheses = [\n",
    "    lambda x: 639 * np.ones_like(x),\n",
    "    lambda x: 1200 - 45 * x,\n",
    "    lambda x: 1010 - 30 * x,\n",
    "    lambda x: 1020 - 9.5 * x - 1.5 * x ** 2,\n",
    "    lambda x: 800 + 40 * x - 3.5 * x ** 2,\n",
    "    lambda x: 300 + 30 * x + 2.9 * x ** 2 - 0.25 * x ** 3,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot the individual observations\n",
    "ax.scatter(crops[\"temp\"], crops[\"yield\"], alpha=.5, color=\"gray\")\n",
    "\n",
    "# Create a grid of x values to make predictions for\n",
    "xs = np.linspace(crops[\"temp\"].min(), crops[\"temp\"].max(), 100)\n",
    "\n",
    "# Plot the hypotheses\n",
    "for i, f in enumerate(hypotheses):\n",
    "    ax.plot(xs, f(xs), label=f\"$f_{i+1}$\")\n",
    "\n",
    "ax.set_title(\"Crop Yield vs. Average Temperature\")\n",
    "ax.set_xlabel(\"Average Temperature\")\n",
    "ax.set_ylabel(\"Crop Yield\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function we want to use\n",
    "loss = squared_loss\n",
    "\n",
    "# Choose a predictor to evaluate\n",
    "predictor = hypotheses[0]\n",
    "\n",
    "# Compute predictions for the chosen predictor\n",
    "y_hat = predictor(crops[\"temp\"])\n",
    "\n",
    "# Compute the losses across the dataset\n",
    "losses = loss(crops[\"yield\"], y_hat)\n",
    "\n",
    "print(f\"The empirical risk of the constant predictor is: {losses.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è ‚úèÔ∏è Task 4\n",
    "First, make sure you thoroughly understand the code cell above. Once you do, implement the following:\n",
    "1. Write a loop that iterates over the hypotheses and computes the squared loss for each hypothesis.\n",
    "2. Find the hypothesis with the smallest squared loss and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = squared_loss # Define the loss function we want to use\n",
    "\n",
    "all_losses = [] # Use this list to store the losses for each predictor\n",
    "\n",
    "# Extend the following code\n",
    "for i, f in enumerate(hypotheses):\n",
    "    # Compute the predictions\n",
    "    y_hat = np.nan # ‚û°Ô∏è ‚úèÔ∏è your code here\n",
    "\n",
    "    # Compute the losses\n",
    "    losses = np.zeros(5) # ‚û°Ô∏è ‚úèÔ∏è your code here\n",
    "\n",
    "    # Store the empirical risk\n",
    "    all_losses.append(losses.mean())\n",
    "\n",
    "# Print out the predictor with the lowest empirical risk\n",
    "print(f\"Predictor with lowest empirical risk: f{np.argmin(all_losses) + 1}\",\n",
    "      f\"with empirical risk: {np.min(all_losses):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REMOVE SOLUTION\n",
    "\n",
    "loss = squared_loss # Define the loss function we want to use\n",
    "\n",
    "all_losses = [] # Use this list to store the losses for each predictor\n",
    "\n",
    "for i, f in enumerate(hypotheses):\n",
    "    # Compute the predictions\n",
    "    y_hat = f(crops[\"temp\"])\n",
    "\n",
    "    # Compute the losses\n",
    "    losses = loss(crops[\"yield\"], y_hat)\n",
    "    \n",
    "    # Store the empirical risk\n",
    "    all_losses.append(losses.mean())\n",
    "\n",
    "# Print out the predictor with the lowest empirical risk\n",
    "print(f\"Predictor with lowest empirical risk: f{np.argmin(all_losses) + 1}\",\n",
    "      f\"with empirical risk: {np.min(all_losses):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è ‚úèÔ∏è Task 5\n",
    "Repeat task 4, but this time compute the squared loss, the absolute loss and different Huber losses ($\\delta = 0.5, 1, 2, 5$) for each hypothesis, do you notice anything different? Discuss with your classmates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REMOVE SOLUTION\n",
    "\n",
    "# We use a DataFrame to store the losses\n",
    "all_losses = pd.DataFrame({\"predictor\": [f\"f{i+1}\" for i,_ in enumerate(hypotheses)]}) \n",
    "\n",
    "squared_losses = []\n",
    "absolute_losses = []\n",
    "huber_losses_delta_1 = []\n",
    "huber_losses_delta_05 = []\n",
    "huber_losses_delta_2 = []\n",
    "huber_losses_delta_5 = []\n",
    "\n",
    "for i, f in enumerate(hypotheses):\n",
    "    # Compute the predictions\n",
    "    y_hat = f(crops[\"temp\"])\n",
    "\n",
    "    # Compute the empirical risks\n",
    "    squared_losses.append(squared_loss(crops[\"yield\"], y_hat).mean())\n",
    "    absolute_losses.append(absolute_loss(crops[\"yield\"], y_hat).mean())\n",
    "    huber_losses_delta_1.append(huber_loss(crops[\"yield\"], y_hat, delta=1).mean())\n",
    "    huber_losses_delta_05.append(huber_loss(crops[\"yield\"], y_hat, delta=.5).mean())\n",
    "    huber_losses_delta_2.append(huber_loss(crops[\"yield\"], y_hat, delta=2).mean())\n",
    "    huber_losses_delta_5.append(huber_loss(crops[\"yield\"], y_hat, delta=5).mean())\n",
    "\n",
    "# Add the losses to the DataFrame\n",
    "all_losses[\"squared\"] = squared_losses\n",
    "all_losses[\"absolute\"] = absolute_losses\n",
    "all_losses[\"huber_delta_05\"] = huber_losses_delta_05\n",
    "all_losses[\"huber_delta_1\"] = huber_losses_delta_1\n",
    "all_losses[\"huber_delta_2\"] = huber_losses_delta_2\n",
    "all_losses[\"huber_delta_5\"] = huber_losses_delta_5\n",
    "\n",
    "# Display the DataFrame\n",
    "display(all_losses)\n",
    "\n",
    "for c in all_losses.columns[1:]:\n",
    "    print(\"Predictor with lowest empirical risk for {}: f{}\".format(c, np.argmin(all_losses[c]) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### ü§î Pause and ponder\n",
    "\n",
    "In this example, we have received a finite set of possible predictors $\\mathcal{H} = \\{f_1, f_2, \\dots f_6\\}$ and we have identified the best one out of this set, given a specific loss function. However, this list of predictors was predefined and limited in scope. How can we find a good predictor in a more general setting, i.e., when we do not receive a finite set of possible predictors?\n",
    "\n",
    "Can you think of a way to find a good predictor in a more general setting? Discuss with your classmates.\n",
    "\n",
    "*Hint:* Consider the mathematical minimization problem: $$\\min_{f \\in \\mathcal{H}} \\frac{1}{n} \\sum_{i=1}^n \\ell(y^{(i)}, f(\\mathbf{x}^{(i)})),$$\n",
    "\n",
    "can you think of a way to solve this problem for an infinite hypothesis class? \n",
    "\n",
    "Or perhaps a way to make this problem easier to solve? Perhaps the picture below can help you think this through. How can we mathematically represent the blue line? Does this provide any insight into how we can set up an infinite hypothesis class and still find its best predictor?\n",
    "\n",
    "... in any case, this is the topic of the next lecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://jldc.ch/slides/img/dsf/gd_linreg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
