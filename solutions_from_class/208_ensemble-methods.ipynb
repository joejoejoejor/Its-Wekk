{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagged Trees and Random Forests\n",
    "\n",
    "#### 🎯 Learning Goals\n",
    "\n",
    "1. Understand the concept of **Ensemble Methods**, why they are useful, and how they work.\n",
    "2. Learn how to implement **Random Forests** in `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use a nicer style for plots\n",
    "plt.style.use(\"seaborn-v0_8-muted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Ensemble Methods\n",
    "\n",
    "> *Sometimes, the whole is greater than the sum of its parts.*\n",
    "\n",
    "This quote is a good description of the key idea behind **Ensemble Methods**. Instead of having a single predictor do the job, we can take an ensemble of relatively *weak* predictors and have them work together to make better predictions than any single predictor could.\n",
    "\n",
    "We have learned about cross-validation to make our predictors more robust and drastically improve their generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "Bagging, short for \"bootstrap aggregating,\" is a technique designed to improve the stability and accuracy of machine learning algorithms by reducing their variance. This approach is particularly beneficial for decision trees, which are known to be high variance, low bias estimators. Decision trees are prone to varying greatly with small changes in the training data; resampling the data often leads to quite different trees and predictions.\n",
    "\n",
    "Consider $B$ random variables $Z_1, Z_2, \\dots, Z_B$, each independently drawn from a distribution with variance $\\sigma^2$. The variance of the average $\\bar{Z}$ of these variables is derived as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Var}(\\bar{Z}) &= \\text{Var}\\left(\\frac{1}{B}\\sum_{i=1}^B Z_i\\right) \\\\\n",
    "&= \\frac{1}{B^2}\\text{Var}\\left(\\sum_{i=1}^B Z_i\\right) \\\\\n",
    "&= \\frac{1}{B^2}\\sum_{i=1}^B \\text{Var}(Z_i) \\text{ since the } Z_i \\text{'s are independent} \\\\\n",
    "&= \\frac{1}{B^2} \\cdot B\\sigma^2 \\text{ because each } Z_i \\text{ has variance } \\sigma^2 \\\\\n",
    "&= \\frac{\\sigma^2}{B}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This mathematical result sheds light on the bagging process. If we create $B$ different training datasets via bootstrapping and train $B$ individual models, denoted as $\\hat{f}^{*b}(x)$ for the $b^\\text{th}$ model using the $b^\\text{th}$ dataset, then averaging the output of these models will yield a combined predictor with reduced variance. The intuition behind this is that different models will likely make different errors on various segments of the data, and by averaging their outputs, we can cancel out some of these errors, hence lowering the overall prediction error.\n",
    "\n",
    "When bagging, we typically don't have access to multiple training datasets. Instead, we create $B$ different datasets via [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)), and train $B$ individual models using these datasets. The final prediction is then made by averaging the predictions of the $B$ individual models. This procedure is known as **bagging**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ms_subclass</th>\n",
       "      <th>ms_zoning</th>\n",
       "      <th>lot_frontage</th>\n",
       "      <th>lot_area</th>\n",
       "      <th>street</th>\n",
       "      <th>alley</th>\n",
       "      <th>lot_shape</th>\n",
       "      <th>land_contour</th>\n",
       "      <th>utilities</th>\n",
       "      <th>lot_config</th>\n",
       "      <th>...</th>\n",
       "      <th>fence</th>\n",
       "      <th>misc_feature</th>\n",
       "      <th>misc_val</th>\n",
       "      <th>mo_sold</th>\n",
       "      <th>year_sold</th>\n",
       "      <th>sale_type</th>\n",
       "      <th>sale_condition</th>\n",
       "      <th>sale_price</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One_Story_1946_and_Newer_All_Styles</td>\n",
       "      <td>Residential_Low_Density</td>\n",
       "      <td>141</td>\n",
       "      <td>31770</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Slightly_Irregular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>No_Fence</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>215000</td>\n",
       "      <td>-93.619754</td>\n",
       "      <td>42.054035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One_Story_1946_and_Newer_All_Styles</td>\n",
       "      <td>Residential_High_Density</td>\n",
       "      <td>80</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>Minimum_Privacy</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>105000</td>\n",
       "      <td>-93.619756</td>\n",
       "      <td>42.053014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One_Story_1946_and_Newer_All_Styles</td>\n",
       "      <td>Residential_Low_Density</td>\n",
       "      <td>81</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Slightly_Irregular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>No_Fence</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172000</td>\n",
       "      <td>-93.619387</td>\n",
       "      <td>42.052659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One_Story_1946_and_Newer_All_Styles</td>\n",
       "      <td>Residential_Low_Density</td>\n",
       "      <td>93</td>\n",
       "      <td>11160</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>No_Fence</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>244000</td>\n",
       "      <td>-93.617320</td>\n",
       "      <td>42.051245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two_Story_1946_and_Newer</td>\n",
       "      <td>Residential_Low_Density</td>\n",
       "      <td>74</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Slightly_Irregular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>Minimum_Privacy</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>189900</td>\n",
       "      <td>-93.638933</td>\n",
       "      <td>42.060899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ms_subclass                 ms_zoning  \\\n",
       "0  One_Story_1946_and_Newer_All_Styles   Residential_Low_Density   \n",
       "1  One_Story_1946_and_Newer_All_Styles  Residential_High_Density   \n",
       "2  One_Story_1946_and_Newer_All_Styles   Residential_Low_Density   \n",
       "3  One_Story_1946_and_Newer_All_Styles   Residential_Low_Density   \n",
       "4             Two_Story_1946_and_Newer   Residential_Low_Density   \n",
       "\n",
       "   lot_frontage  lot_area street            alley           lot_shape  \\\n",
       "0           141     31770   Pave  No_Alley_Access  Slightly_Irregular   \n",
       "1            80     11622   Pave  No_Alley_Access             Regular   \n",
       "2            81     14267   Pave  No_Alley_Access  Slightly_Irregular   \n",
       "3            93     11160   Pave  No_Alley_Access             Regular   \n",
       "4            74     13830   Pave  No_Alley_Access  Slightly_Irregular   \n",
       "\n",
       "  land_contour utilities lot_config  ...            fence misc_feature  \\\n",
       "0          Lvl    AllPub     Corner  ...         No_Fence         None   \n",
       "1          Lvl    AllPub     Inside  ...  Minimum_Privacy         None   \n",
       "2          Lvl    AllPub     Corner  ...         No_Fence         Gar2   \n",
       "3          Lvl    AllPub     Corner  ...         No_Fence         None   \n",
       "4          Lvl    AllPub     Inside  ...  Minimum_Privacy         None   \n",
       "\n",
       "  misc_val mo_sold year_sold sale_type sale_condition sale_price  longitude  \\\n",
       "0        0       5      2010       WD          Normal     215000 -93.619754   \n",
       "1        0       6      2010       WD          Normal     105000 -93.619756   \n",
       "2    12500       6      2010       WD          Normal     172000 -93.619387   \n",
       "3        0       4      2010       WD          Normal     244000 -93.617320   \n",
       "4        0       3      2010       WD          Normal     189900 -93.638933   \n",
       "\n",
       "    latitude  \n",
       "0  42.054035  \n",
       "1  42.053014  \n",
       "2  42.052659  \n",
       "3  42.051245  \n",
       "4  42.060899  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load our data\n",
    "housing = pd.read_csv(\"data/ames_housing.csv\")\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regression tree from scikit-learn and a plotting helper\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# Import our train_test_split helper\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import the mean_squared_error function under the alias mse\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "# Import the resampling helper\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data intro features and targets\n",
    "y = np.log(housing[\"sale_price\"]) # Use the logarithm of the sale price\n",
    "features = [\"lot_frontage\", \"lot_area\", \"year_built\", \"pool_area\"]\n",
    "X = housing[features]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a single tree to our test\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE:  0.001216739449040038\n",
      "Test MSE :  0.1105799918505865\n"
     ]
    }
   ],
   "source": [
    "# Compute the training and test mse\n",
    "y_pred_train = tree.predict(X_train)\n",
    "y_pred_test = tree.predict(X_test)\n",
    "\n",
    "print(\"Train MSE: \", mse(y_train, y_pred_train))\n",
    "print(\"Test MSE : \", mse(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE:  0.010115499129406413\n",
      "Test MSE :  0.07430649558887976\n"
     ]
    }
   ],
   "source": [
    "# Let us now proceed with bagging, using B = 100 trees\n",
    "B = 100 # Number of estimators to bag\n",
    "\n",
    "# Create matrices to hold the predictions (n x B), each row is an observation,\n",
    "# each column is a tree, i.e., in row 1, we have the predictions of the B=100 \n",
    "# trees for the first observation\n",
    "y_pred_train_bag = np.zeros((X_train.shape[0], B))\n",
    "y_pred_test_bag = np.zeros((X_test.shape[0], B))\n",
    "\n",
    "# Iterate over the B estimators\n",
    "for b in range(B):\n",
    "    tree_b = DecisionTreeRegressor()\n",
    "\n",
    "    # Sample with replacement from X_train / y_train\n",
    "    X_train_b, y_train_b = resample(X_train, y_train)\n",
    "    \n",
    "    # Train the tree on the bootstrapped sample\n",
    "    tree_b.fit(X_train_b, y_train_b)\n",
    "\n",
    "    # Predict on X_train and X_test\n",
    "    y_pred_train_bag[:, b] = tree_b.predict(X_train)\n",
    "    y_pred_test_bag[:, b] = tree_b.predict(X_test)\n",
    "\n",
    "# Take the average of the predictions over all trees\n",
    "y_pred_train_bag = y_pred_train_bag.mean(axis=1)\n",
    "y_pred_test_bag = y_pred_test_bag.mean(axis=1)\n",
    "\n",
    "print(\"Train MSE: \", mse(y_train, y_pred_train_bag))\n",
    "print(\"Test MSE : \", mse(y_test, y_pred_test_bag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, implementing bagging from scratch is simple and a nice didactic example, but in practice, `scikit-learn` has a class to implement either `BaggingRegressor` or `BaggingClassifier` for us. Depending on whether we want to tacke a regression or classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sklearn implementation of bagging\n",
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE:  0.010052876040873814\n",
      "Test MSE :  0.07372084154112407\n"
     ]
    }
   ],
   "source": [
    "# Create a bagged tree estimator with B=100 trees\n",
    "bagged_trees = BaggingRegressor(DecisionTreeRegressor(), n_estimators=B)\n",
    "\n",
    "# Fit the bagged estimator and compute the MSE on the training set\n",
    "bagged_trees.fit(X_train, y_train)\n",
    "\n",
    "# Compute the predictions on the training and test sets\n",
    "y_pred_train_bag = bagged_trees.predict(X_train)\n",
    "y_pred_test_bag = bagged_trees.predict(X_test)\n",
    "\n",
    "print(\"Train MSE: \", mse(y_train, y_pred_train_bag))\n",
    "print(\"Test MSE : \", mse(y_test, y_pred_test_bag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the same as we obtained before (albeit some small differences due to the random nature of the algorithm). Bagging is a powerful technique that can be applied to any kind of predictor, but it is especially useful for decision trees.\n",
    "\n",
    "#### ➡️ ✏️ Task 1\n",
    "\n",
    "1. Train a `LinearRegression` model on the train data and evaluate its performance (both on train and test data).\n",
    "2. Fllowing the example above, train a `BaggingRegressor` model that uses `LinearRegression` as its base estimator. Evaluate its performance (both on train and test data).\n",
    "\n",
    "What do you observe? Did the performance improve? Why? Why not? Compare the results with the `DecisionTreeRegressor` and its bagged version. Discuss with your classmates why you think there are differences between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the linear regression model\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ➡️ ✏️ Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE:  0.09125266962955848\n",
      "Test MSE :  0.09157895171291991\n"
     ]
    }
   ],
   "source": [
    "# TODO: REMOVE SOLUTION\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# Compute the predictions on the training and test sets\n",
    "y_pred_train_lin = linreg.predict(X_train)\n",
    "y_pred_test_lin = linreg.predict(X_test)\n",
    "\n",
    "print(\"Train MSE: \", mse(y_train, y_pred_train_lin))\n",
    "print(\"Test MSE : \", mse(y_test, y_pred_test_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE:  0.09136241615123851\n",
      "Test MSE :  0.09226023822450422\n"
     ]
    }
   ],
   "source": [
    "# TODO: REMOVE SOLUTION\n",
    "\n",
    "bagged_regression = BaggingRegressor(LinearRegression(), n_estimators=B)\n",
    "\n",
    "# Fit the bagged estimator and compute the MSE on the training set\n",
    "bagged_regression.fit(X_train, y_train)\n",
    "\n",
    "# Compute the predictions on the training and test sets\n",
    "y_pred_train_bag = bagged_regression.predict(X_train)\n",
    "y_pred_test_bag = bagged_regression.predict(X_test)\n",
    "\n",
    "print(\"Train MSE: \", mse(y_train, y_pred_train_bag))\n",
    "print(\"Test MSE : \", mse(y_test, y_pred_test_bag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "Random forests provide an extension of bagged trees with an additional layer of randomness compared to generic bagging. Like bagging, random forests involve building multiple decision trees on bootstrapped training samples. The key difference lies in **how the trees are constructed**. In bagging, each tree is built on a bootstrapped sample of the training data. In random forests, each tree is built on a bootstrapped sample of the training data **with a random subset of features**. \n",
    "\n",
    "In essence, the trees in a random forest do not have the same information available to them when they are being built, many of them being trained on a different subset of features. This introduces further randomization into the model, which tends to reduce variance even further and improve performance.\n",
    "\n",
    "More formally, suppose we have a feature space $\\mathcal{X} \\in \\mathbb{R}^p$. Each tree in the bagged trees is fitted using the $p$ features, while, in a random forest, each tree is only given a random subset of $m$ features, where $m < p$. The value of $m$ is specified by the user and is held constant throughout the fitting process. A common choice is $m \\approx \\sqrt{p}$. Notice that if we set $m = p$, the random forest and bagged trees are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE:  0.010355853107657518\n",
      "Test MSE :  0.07403159275256627\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=B)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Compute the predictions on the training and test sets\n",
    "y_pred_train_rf = rf.predict(X_train)\n",
    "y_pred_test_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Train MSE: \", mse(y_train, y_pred_train_rf))\n",
    "print(\"Test MSE : \", mse(y_test, y_pred_test_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ➡️ ✏️ Task 2\n",
    "\n",
    "Do you have an intuition as to why the random forest is not working better than the bagged trees in this case? What would you do to improve the performance of the random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ➡️ ✏️ Task 3\n",
    "\n",
    "+ Choose more features from the initial dataset (at least 30). If need be, use dummies where appropriate.\n",
    "+ Construct a `RandomForestRegressor` and a `BaggingRegressor` using `DecisionTreeRegressor` as the base estimator.\n",
    "+ Fit the two estimators to your new dataset, compare their performance on train and test sets. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ➡️ ✏️ Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REMOVE SOLUTION\n",
    "\n",
    "features = housing.columns.drop(\"sale_price\")\n",
    "# Take all the features that are not categorical (strings, i.e., 'object' dtype)\n",
    "categorical_features = [f for f in features if housing[f].dtype == \"object\"]\n",
    "\n",
    "X = housing[features]\n",
    "X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "y = np.log(housing[\"sale_price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE (RF):  0.0028244672953999204\n",
      "Test MSE (RF) :  0.023991012542825717\n",
      "Train MSE (Bagged Trees):  0.0028032839173379506\n",
      "Test MSE (Bagged Trees) :  0.024238494811091475\n"
     ]
    }
   ],
   "source": [
    "# TODO: REMOVE SOLUTION\n",
    "\n",
    "# Train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=72)\n",
    "\n",
    "# Fit the random forest and bagged trees\n",
    "rf = RandomForestRegressor(n_estimators=B)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "bagged_trees = BaggingRegressor(DecisionTreeRegressor(), n_estimators=B)\n",
    "bagged_trees.fit(X_train, y_train)\n",
    "\n",
    "# Compute the predictions on the training and test sets\n",
    "y_pred_train_rf = rf.predict(X_train)\n",
    "y_pred_test_rf = rf.predict(X_test)\n",
    "\n",
    "y_pred_train_bag = bagged_trees.predict(X_train)\n",
    "y_pred_test_bag = bagged_trees.predict(X_test)\n",
    "\n",
    "print(\"Train MSE (RF): \", mse(y_train, y_pred_train_rf))\n",
    "print(\"Test MSE (RF) : \", mse(y_test, y_pred_test_rf))\n",
    "\n",
    "print(\"Train MSE (Bagged Trees): \", mse(y_train, y_pred_train_bag))\n",
    "print(\"Test MSE (Bagged Trees) : \", mse(y_test, y_pred_test_bag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
